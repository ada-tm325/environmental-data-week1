{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1-qx9rWmDms"
      },
      "source": [
        "---\n",
        "\n",
        "# <center> Environmental Data </center>\n",
        "\n",
        "---\n",
        "\n",
        "## Course objectives and philosophy\n",
        "Let's start by recalling what the learning objective of the Environmental Data modules are:\n",
        "\n",
        "1. Understand common data format and database structures specific to representative fields of environmental science\n",
        "2. Demonstrate technical competency in handling common data types routinely encountered in the environmental sciences and identify relevant open-source data repositories\n",
        "3. Identify and design suitable data analysis strategies that consider data types, data distribution constraints, strength, benefits and limitations of statistical and modelling tools and environmental dynamics.\n",
        "4. Understand the limitation of available data and data analysis products. Understand sources of errors and demonstrate ability to comprehensively characterize uncertainties and interpret results in the context of these uncertainties, including measurement errors, environmental uncertainties as well as errors stemming from the analytical procedure itself (e.g. calibration of analysis using synthetic data/models).\n",
        "\n",
        "You'll note that these objectives address a progression of skills. The first focus on technical competency (understanding common data format and an ability to manipulate these data). The last two go deeper than technical competency - these aim to develop a data analysis workflow. In real-life situations, some of that workflow will be informed by intuition, and the best workflow will depend on context (e.g. time, infrastructure available) and on the objectives of a study.\n",
        "\n",
        "Intuition will come with experience, and therefore, also from failure! This course is an opportunity to try, sometimes fail ... and therefore learn! (That is the luxury of being a student!) With that in mind, some of the coursework presented here will be purposefully \"vague\". This is not meant to be annoying, it is designed to push you to seek and explore possible solutions (solutions are rarely unique) and express your creativity as an analyst.\n",
        "\n",
        "**Important**: Data analysis is a key component of the scientific method and therefore cannot exist without `hypothesis testing`. Approach every part of the data analysis workflow as a test, an experiment. Each experiment should be motivated by a hypothesis (a goal), will involve applying a methodology, will produce results pertinent to the hypothesis/goal, and will require a critique of these results with regards to their ability to address the hypothesis/goal before drawing a conclusion (it is ok for conclusions to come with caveats and uncertainties if the analysis warrants limits of interpretation).  \n",
        "\n",
        "**Warning**: Data analysis can also be a bit of  \"rabbit hole\"! There will always be nicer, cooler, better things to do. Sometimes, good is good enough though. Whatever you do, never lose sight of the overall objectives of a study. This needs to be delivered in priority.\n",
        "\n",
        "## This segment of the module\n",
        "In this module, content will be delivered using a mixture of\n",
        "powerpoint lectures, jupyter notebooks and practical exercises.\n",
        "\n",
        "Warning: other directories, containing figures, images, animations or data are also provided.\n",
        "However, cliamte data can be very large, and github has a low limit for these.\n",
        "A dropbox link with data also exist: https://www.dropbox.com/sh/fxcmtbz4o3tacz1/AABjQbeyg27zDh1chZxRDFcpa?dl=0\n",
        "\n",
        "### Github repository: `environmental_data_week1`\n",
        "\n",
        "\n",
        "## We will focus and use climate data.\n",
        "This segment of the module will focus on 2D, 3D and 4D data. To illustrate techniques and learn some useful skills, we will use data around the theme of climate and climate change. As part of this, we will learn how to access and use climate data and output from global climate models.\n",
        "\n",
        "The science of climate is beyond the scope of this course, but although we will focus on data analysis, it is my hope that you will develop an appreciation and better understanding of climate science in the process.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1：翻译**\n",
        "\n",
        "# **<center> 环境数据 </center>**\n",
        "\n",
        "---\n",
        "\n",
        "**课程目标与理念**\n",
        "\n",
        "让我们先回顾一下环境数据模块的学习目标是什么：\n",
        "\n",
        "1. **理解环境科学代表性领域特有的常见数据格式和数据库结构。**\n",
        "2. **展示处理环境科学中经常遇到的常见数据类型的技术能力，并识别相关的开源数据存储库。**\n",
        "3. **识别并设计合适的数据分析策略**，这些策略要考虑数据类型、数据分布约束、统计和建模工具的优势、益处与局限性，以及环境动力学。\n",
        "4. **理解现有数据和数据分析产品的局限性。** 理解误差来源，并展示在这些不确定性的背景下全面描述不确定性并解释结果的能力，包括测量误差、环境不确定性以及源于分析过程本身的误差（例如使用合成数据/模型对分析进行校准）。\n",
        "\n",
        "你会注意到这些目标解决了一系列技能的进阶。第一个侧重于技术能力（理解常见数据格式和操作这些数据的能力）。后两个则比技术能力更深入——它们旨在开发数据分析工作流。在现实生活中，部分工作流将由直觉告知，而最佳工作流将取决于具体情境（例如时间、可用基础设施）和研究目标。\n",
        "\n",
        "直觉源于经验，因此也源于失败！这门课程是一个尝试、有时失败……并因此学习的机会！（这是作为学生的奢侈！）考虑到这一点，这里展示的一些课程作业将是故意“模糊”的。这并不是为了让人恼火，而是旨在推动你寻找和探索可能的解决方案（解决方案很少是唯一的），并表达你作为分析师的创造力。\n",
        "\n",
        "**重要**：数据分析是科学方法的关键组成部分，因此离不开`假设检验`。将数据分析工作流的每一部分都视为一次测试、一次实验。每个实验都应由假设（目标）驱动，涉及应用方法论，产生与假设/目标相关的结果，并需要在得出结论之前针对结果解决假设/目标的能力进行批判（如果分析证明了解释的局限性，结论带有保留条件和不确定性是可以的）。\n",
        "\n",
        "**警告**：数据分析也可能有点像个“无底洞”！总会有更好看、更酷、更好的事情可做。不过有时候，差不多好就足够了。无论你做什么，永远不要忽视研究的总体目标。这需要优先交付。\n",
        "\n",
        "**本模块的这部分环节**\n",
        "\n",
        "在本模块中，内容将混合使用 PPT 讲座、jupyter notebooks 和实践练习来交付。\n",
        "\n",
        "> **警告**：还提供了其他目录，包含图表、图像、动画或数据。但是，气候数据可能非常大，github 对此限制较低。也存在一个带有数据的 dropbox 链接：[Dropbox Link](https://www.dropbox.com/sh/fxcmtbz4o3tacz1/AABjQbeyg27zDh1chZxRDFcpa?dl=0)\n",
        "\n",
        "**Github 仓库：`environmental_data_week1`**\n",
        "\n",
        "**我们将聚焦并使用气候数据。**\n",
        "\n",
        "本模块的这一部分将侧重于 2D、3D 和 4D 数据。为了说明技术并学习一些实用技能，我们将围绕气候和气候变化这一主题使用数据。作为其中的一部分，我们将学习如何访问和使用气候数据以及全球气候模型的输出。\n",
        "\n",
        "气候科学超出了本课程的范围，但虽然我们将侧重于数据分析，我也希望你们在这个过程中能对气候科学产生欣赏并有更好的理解。\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（划重点）**\n",
        "\n",
        "这一段其实是教授在给你打“预防针”和定调子，核心意思就这几点：\n",
        "\n",
        "1. **这课学啥？**\n",
        "    * **先学技术：** 搞懂数据格式（比如 .nc 文件），学会怎么下载和读取数据。\n",
        "    * **再学思路：** 拿到数据怎么分析？怎么设计流程？\n",
        "    * **最后学严谨：** 要明白数据是有误差的，结论不能下太死，要学会分析“不确定性”。\n",
        "\n",
        "2. **作业为什么有时候看不懂？**\n",
        "    * 教授明确说了，有些作业要求是**故意写得很模糊（Purposefully \"vague\"）**。\n",
        "    * **目的：** 逼你自己去想办法，而不是照着说明书无脑操作。因为现实中的工作往往就是没有明确说明书的。\n",
        "    * **心态：** 试错没关系，学生有犯错的特权。\n",
        "\n",
        "3. **避坑指南（重要）：**\n",
        "    * **别钻牛角尖（Rabbit Hole）：** 做数据分析很容易陷进去，想把图画得更漂亮、代码写得更炫酷。教授警告说：**差不多行了（Good is good enough）**，先把作业交了，把主要问题解决了，别为了完美主义耽误正事。\n",
        "    * **要有假设：** 别瞎跑数据，做每一步分析前先想好“我想验证什么假设”。\n",
        "\n",
        "4. **关于数据：**\n",
        "    * 这次主要用**气候数据**（2D/3D/4D）来练手。\n",
        "    * 数据文件很大，Github 放不下，如果这里找不到，记得去那个 **Dropbox 链接**下载。\n",
        "\n",
        "> **总结一句话：** 这门课不仅仅教你写代码，更想教你像个分析师一样思考。作业如果不清楚是故意的，让你自己发挥。做的时候别太纠结完美，先把结果搞出来。"
      ],
      "metadata": {
        "id": "zzEI5spcoAFu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgpte7ezmDms"
      },
      "source": [
        "---\n",
        "\n",
        "# 01- Introduction to climate data and climate models\n",
        "\n",
        "---\n",
        "\n",
        "<a id='Contents'></a>\n",
        "## Contents\n",
        "- [Lectures](#lectures)\n",
        "- [Question](#question)\n",
        "- [ESMValTool](#ESMValTool)\n",
        "- [Sources of climate data](#climatedata)\n",
        "- [CEDA](#CEDA)\n",
        "- [File formats](#fileformat)\n",
        "- [Metadata](#metadata)\n",
        "- [NetCDF](#NetCDF)\n",
        "- [NCO](#NCO)\n",
        "- [Reanalysis products](#reanalysis)\n",
        "- [Climate services UKCP18](#ukcp18)\n",
        "  \n",
        "## Learning outcomes\n",
        "1. Understand the basics of climate modelling\n",
        "2. Know where to find and how to access climate data (ESGF, CEDA)\n",
        "3. Understand common file formats\n",
        "4. Understand the importance of metadata and understand CF conventions\n",
        "5. Understand the NetCDF file format and know how to read, create and maniputate NetCDF data\n",
        "6. Become familiar with other key tools used to manipulate netCDF data (NCO, CDO)\n",
        "7. Know how to access ERA5 reanalysis products\n",
        "8. Climate Services and UKCP18 and ability to access that data\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "# 01- 气候数据与气候模型导论\n",
        "\n",
        "---\n",
        "\n",
        "<a id='Contents'></a>\n",
        "## 目录\n",
        "- [讲座](#lectures)\n",
        "- [问题](#question)\n",
        "- [ESMValTool](#ESMValTool)\n",
        "- [气候数据来源](#climatedata)\n",
        "- [CEDA](#CEDA)\n",
        "- [文件格式](#fileformat)\n",
        "- [元数据](#metadata)\n",
        "- [NetCDF](#NetCDF)\n",
        "- [NCO](#NCO)\n",
        "- [再分析产品](#reanalysis)\n",
        "- [气候服务 UKCP18](#ukcp18)\n",
        "  \n",
        "## 学习成果\n",
        "1. 理解气候建模的基础知识\n",
        "2. 知道在哪里寻找以及如何访问气候数据（ESGF, CEDA）\n",
        "3. 理解常见的文件格式\n",
        "4. 理解元数据的重要性并理解 CF 公约（CF conventions）\n",
        "5. 理解 NetCDF 文件格式，并知道如何读取、创建和操作 NetCDF 数据\n",
        "6. 熟悉其他用于操作 netCDF 数据的关键工具（NCO, CDO）\n",
        "7. 知道如何获取 ERA5 再分析产品\n",
        "8. 了解气候服务和 UKCP18，并具备获取该数据的能力\n",
        "\n",
        "---\n",
        "**Part 2：大白话总结（划重点）**\n",
        "\n",
        "这一页 PPT/Notebook 就是告诉你 **“这节课我们要搞定哪些硬骨头”**，其实就是一个工具包列表：\n",
        "\n",
        "1.  **你要去哪找数据？**\n",
        "    * **CEDA** 和 **ESGF**：这俩是“数据仓库”，你需要知道大门朝哪开（也就是怎么注册、怎么搜）。\n",
        "    * **ERA5** 和 **UKCP18**：这俩是具体的“货物”。一个是过去的复盘数据，一个是未来的预测数据。\n",
        "\n",
        "2.  **拿到数据怎么拆包？**\n",
        "    * **NetCDF**：这是气候界的“压缩包”格式，你得学会怎么打开它。\n",
        "    * **元数据 (Metadata)** 和 **CF公约**：这是“说明书”，教你读懂文件里的变量名（比如 `temp` 到底是摄氏度还是开尔文）。\n",
        "\n",
        "3.  **用什么工具修修补补？**\n",
        "    * **NCO** 和 **CDO**：这俩是气候数据的“瑞士军刀”，专门用来快速处理这些特殊格式文件的工具。\n",
        "\n",
        "**总结一句话：** 这节课的目标就是让你学会：**去哪找数据、下载下来是什么格式、以及怎么用工具把这些格式的文件打开并处理它。**"
      ],
      "metadata": {
        "id": "sNHV-zlupgDq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNUrxIGdmDmt"
      },
      "source": [
        "<a id='lectures'></a>\n",
        "# Lecture on relevance of climate science, climate models and climate modelling\n",
        "\n",
        "\n",
        "(see Powerpoint presentations)\n",
        "\n",
        "1. 01v2_Why_climate_data.pptx/pdf\n",
        "2. 02v2_Building_climate_models.pptx/pdf\n",
        "3. 03v2_Common_issues.pptx/pdf\n",
        "   \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwcD3-5DmDmt"
      },
      "source": [
        "<a id='question'></a>\n",
        "\n",
        "# Driving question to answer this week:\n",
        "It is ok to learn techniques and go through practicals, but the overal objective of this course is to equip you with the skills needed to pursue your own analyses. With this in mind, we are going to work towards a central question, to motivate learning though the next few lessons.  \n",
        "\n",
        "\n",
        "* ## Should French Champagne makers invest in wineries in Hampshire (UK)?\n",
        "\n",
        "Recently, [french champagne producer Tattinger have decided to invest in a winery in Kent, called Domaine Evremond](https://www.domaineevremond.com).  Tattinger is aware that other English wineries in [Hampshire](https://en.wikipedia.org/wiki/Hampshire) are also [producing excellent sparkling wine](https://www.visit-hampshire.co.uk/food-and-drink/vineyards).\n",
        "\n",
        "Tattinger's business wing is of course concerned about this. Are these new UK wineries going to be strong competitors for them, or should Tattinger see these as investment opportunities and try to buy them while they can to establish a stronger presence in the UK market?\n",
        "\n",
        "Like everyone else, Tattinger is also aware about climate change and they realize that, from now on, they need to systematically incorporate a climate analysis as part of their business decisions.  \n",
        "\n",
        "From the growing success of English winemakers, Tattinger can see that the current climate in the Hampshire region is good enough at the moment for growing good grapes, but they want to know if and how climate change will affect the current climate in Hampshire. Ultimately, they want to know if climate change could affect the wine buisness in Hampshire. Will climate change negatively affect viticulture in the region, should we expect little change, or are conditions actually going to become even better for growing grapes in the next few decades?\n",
        "\n",
        "Imagine that your new start-up, as an up-and-coming climate intelligence company, is being contacted by representatives from Tattinger HQ to advise them on how climate will change in Hampshire. They want to know how climate change may affect wineries in Hampshire in the coming 50 years or so and they are looking for someone to deliver answers.  \n",
        "\n",
        "**Your task is to i) develop a research strategy that will answer their questions on the future of climate in Hampshire, ii) present initial results on the issue, and iii) issue a preliminary recommendation as to wether Tattinger should or should not invest in UK wineries going forward.**\n",
        "\n",
        "Sadly, time is of the essence! Tattinger has heard that [Moet-et-Chandon](https://en.wikipedia.org/wiki/Moët_&_Chandon), another major champagne producer, is also eyeing on these buisness opportunities and they must make a decision quickly. **You will be expected to produce a jupyter notebook by Thursday afternoon that summarizes your analysis and recommendations to Tattinger. While you will not be required to submit this code, you will be able to refer back to it during the final assessed quiz, so doing a thorough job will help. The final quiz will certainly ask questions related to this experience, such as file formats, limitations of data, etc.**\n",
        "\n",
        "In your analysis, consider which data you want to use, how to get them (can you access and analyze them already?), what could be learned from them. Consider resolution, data use policy, data storage and analytical requirements (would you need to use a cloud service or supercomputer do perform the analysis? if so, could you use this for free as a commercial entity, if not, how much could that cost?). You may also want to use output from reanalysis products (e.g. ERA5) or climate models (UKCP18, CMIP, etc.).\n",
        "\n",
        "Use any and all resources at your disposal (including your instructor and your classmates) but keep in mind that **this is a commercial project**, so beware of any data use or resource access policies in place. In the real world, it could be that addressing this question would imply some costs that could be passed on to the client. However, the rules for this exercise are to only use open-acess resources.\n",
        "\n",
        "You don't have much time, but this could be a big and important contract for your new company so you want to develop a good solution, ideally with some preliminary results to showcase your skills, and prove to Tattinger that you are the right people for the job. Your company could earn future business from them!\n",
        "\n",
        "**IMPORTANT**: Although you are encouraged to exchange with your classmates and exchange ideas, source of data, and work in groups, you will only be able to rely on your own code during the quiz.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "<a id='question'></a>\n",
        "\n",
        "# 本周需要回答的驱动性问题\n",
        "光学习技术和做练习是可以的，但这门课程的总体目标是让你具备进行独立分析所需的技能。考虑到这一点，我们将致力于解决一个核心问题，以此来激发接下来几节课的学习。\n",
        "\n",
        "\n",
        "* ## 法国香槟制造商应该投资汉普郡（英国）的酒庄吗？\n",
        "\n",
        "最近，[法国香槟生产商 Tattinger 决定在肯特郡投资一家名为 Domaine Evremond 的酒庄](https://www.domaineevremond.com)。Tattinger 意识到，[汉普郡](https://en.wikipedia.org/wiki/Hampshire)的其他英国酒庄也正在[生产优质的起泡酒](https://www.visit-hampshire.co.uk/food-and-drink/vineyards)。\n",
        "\n",
        "Tattinger 的商业部门对此当然感到担忧。这些新的英国酒庄会成为他们的强力竞争对手吗？还是说 Tattinger 应该将这些视为投资机会，趁现在买下它们，以在英国市场建立更强大的影响力？\n",
        "\n",
        "像其他人一样，Tattinger 也意识到了气候变化，他们意识到，从现在开始，他们需要系统地将气候分析纳入其商业决策的一部分。\n",
        "\n",
        "从英国酿酒商日益增长的成功中，Tattinger 可以看到，汉普郡目前的气候对于种植优质葡萄来说已经足够好了，但他们想知道气候变化是否以及将如何影响汉普郡当前的气候。归根结底，他们想知道气候变化是否会影响汉普郡的葡萄酒业务。气候变化会对该地区的葡萄种植产生负面影响吗？我们应该预期变化不大吗？还是说在接下来的几十年里，条件实际上会变得更适合种植葡萄？\n",
        "\n",
        "想象一下，你的新初创公司，作为一家崭露头角的气候情报公司，正被 Tattinger 总部的代表联系，请你就气候将如何改变汉普郡这一问题提供建议。他们想知道气候变化在未来 50 年左右可能如何影响汉普郡的酒庄，他们正在寻找能提供答案的人。\n",
        "\n",
        "**你的任务是：**\n",
        "\n",
        "i) 制定一项研究策略，以回答关于汉普郡气候未来的问题；\n",
        "\n",
        "ii) 展示关于该问题的初步结果；\n",
        "\n",
        "iii) 发布一份关于 Tattinger 接下来是否应该投资英国酒庄的初步建议。\n",
        "\n",
        "遗憾的是，时间紧迫！Tattinger 听说另一家主要香槟生产商 [Moet-et-Chandon](https://en.wikipedia.org/wiki/Moët_&_Chandon) 也看中了这些商业机会，他们必须迅速做出决定。\n",
        "\n",
        "**你需要在这个星期四下午之前制作一个 Jupyter Notebook，总结你的分析和给 Tattinger 的建议。虽然不需要提交此代码，但你可以在最终的评估测验（Quiz）中参考它，因此做得彻底会有所帮助。期末测验肯定会问与这次经历相关的问题，例如文件格式、数据的局限性等。**\n",
        "\n",
        "在你的分析中，考虑你想使用哪些数据，如何获取它们（你现在能访问和分析它们吗？），以及能从中通过什么。考虑分辨率、数据使用政策、数据存储和分析要求（你需要使用云服务或超级计算机来执行分析吗？如果是，作为商业实体你可以免费使用吗？如果不能，成本是多少？）。你可能还想使用再分析产品（例如 ERA5）或气候模型（UKCP18, CMIP 等）的输出。\n",
        "\n",
        "利用你可以支配的任何和所有资源（包括你的讲师和同学），但请记住**这是一个商业项目**，所以要注意任何现有的数据使用或资源访问政策。在现实世界中，解决这个问题可能意味着一些成本，这些成本可能会转嫁给客户。但是，本练习的规则是仅使用开放获取资源。\n",
        "\n",
        "你的时间不多，但这可能是你们新公司的一份重要大合同，所以你想开发一个好的解决方案，最好有一些初步结果来展示你的技能，并向 Tattinger 证明你们是这份工作的合适人选。你的公司可能会从他们那里赢得未来的业务！\n",
        "\n",
        "**重要提示**：虽然鼓励你与同学交流、交换想法和数据来源，并进行小组合作，但在测验期间，你只能依赖你自己的代码。\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这就是你的作业！）**\n",
        "\n",
        "这一段非常重要，它就是你这周做 Practical 和 Lecture 的**终极应用场景**。\n",
        "\n",
        "1.  **你是谁？**\n",
        "    * 你现在假装是一家 **“气候情报咨询公司”** 的老板/分析师。\n",
        "\n",
        "2.  **客户是谁？**\n",
        "    * **Tattinger**（那个很有名的法国香槟牌子）。\n",
        "\n",
        "3.  **他们想干嘛？**\n",
        "    * 他们看英国的 **Hampshire（汉普郡）** 现在的葡萄种得不错，想去那里买地建酒庄。\n",
        "    * 但是他们怕**气候变化**。万一以后变太热或雨水太多，葡萄烂了怎么办？或者万一以后气候变得更好了呢？\n",
        "\n",
        "4.  **你的任务（Task）：**\n",
        "    * 分析 Hampshire **未来 50 年**的气候趋势。\n",
        "    * 给出建议：**“买”还是“不买”？**\n",
        "\n",
        "5.  **死线（Deadline）：**\n",
        "    * **本周四下午**。\n",
        "    * 你需要产出一个 Jupyter Notebook。\n",
        "\n",
        "6.  **为什么要认真做？（考试考这个！）**\n",
        "    * 虽然这个 Notebook **不用上交**，但是！期末有个 **Quiz（测验）**。\n",
        "    * 考试的时候，允许你打开这个 Notebook 来看。\n",
        "    * 考试题目会直接问你这里面的细节（比如：你用的 ERA5 数据是什么格式？有什么优缺点？）。如果你没做，考试时对着空白屏幕就完了。\n",
        "\n",
        "7.  **操作限制：**\n",
        "    * 这是一个“商业模拟”，所以要注意数据版权（虽然这次我们只用免费开源的）。\n",
        "    * 可以用 **ERA5**（过去的数据）和 **UKCP18**（未来的预测）。\n",
        "\n",
        "**一句话攻略：**\n",
        "你要去下载 Hampshire 过去和未来的温度/降水数据，画个图对比一下，如果发现未来 50 年温度适度上升且降水稳定，就建议 Tattinger “买入”；如果发现未来会变成沙漠或者天天暴雨，就建议“快跑”。"
      ],
      "metadata": {
        "id": "K7UBj-J1qV3A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veaTUE6ImDmt"
      },
      "source": [
        "<a id='ESMValTool'></a>\n",
        "\n",
        "# **考虑用于处理气候数据的 Python 环境(Python environments to consider for working with climate data)**\n",
        "\n",
        "* ## **地球系统模型评估工具 (ESMValTool)(Earth System Model Evaluation Tool (ESMValTool))**\n",
        "![esmvaltool](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/ESMValTool-logo-2.png?raw=1)\n",
        "\n",
        "This week, it should be possible to simply install packages as needed...\n",
        "\n",
        "However, if you anticipate possible future needs, you may want to invest a bit of effort to setup a specific python environment to work with climate data. If so, a good place to start is the `ESMValTool` project.\n",
        "\n",
        "\n",
        "\n",
        "[ESMValTool](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/index.html) is  a community-led, open source, set of software tools (build on python 3) developed to improve diagnosing and understanding climate models. While the focus has been on climate models, not observations, one cannot quantify model biases without also comparing the model results with observations. [ESMValTool](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/index.html) therefore also has some capability to analyze and manipulate large datasets.\n",
        "\n",
        "[ESMValTool](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/index.html) can work offline, but as the amount of climate data is so large, it is mostly designed to work with data centers that provide local access to the vast amount of data. One such initiative is the Earth System Grid Federation ([ESGF](https://esgf.llnl.gov)). ESMValTool provides functionality to work on HPC systems.\n",
        "\n",
        "We will not use ESMValTool here specifically, but it will install other convenient tools and you may be curious to investigate ESMValTool capabilities on your own anyways. It is now used routinely for climate modelling research.\n",
        "\n",
        "Additional information and help can be found at https://www.esmvaltoolorg, and on the [ESMValGroup](https://github.com/ESMValGroup) github page. The [ESMValTool tutorial](https://esmvalgroup.github.io/ESMValTool_Tutorial/) is a great resource to get started.\n",
        "\n",
        "\n",
        "### OPTIONAL: Installing ESMValTool (NB: this often fails for students - installing this is not required for the course)\n",
        "[Installation instructions](https://docs.esmvaltool.org/en/latest/quickstart/installation.html) differ by operating system and it is very much an experimental, in development, tool - keep this in mind!\n",
        "\n",
        "[Notes:ESMValTool 2.0 requires a Unix(-like) operating system and Python 3.7+. (Python 3.7, 3.8 and 3.9 are supported). On my laptop (Macbook pro 13'' M1 2020 with OS Monterey 12.0.1), I installed it in Nov 2021 with Python 3.9.7. ...but I installed it when 'conda' was still an installation option. ESMValTool developer have  changed installation methods to `mamba` recently. Do refer to online/official instructions). You will now be required to also [install `mamba`](https://docs.esmvaltool.org/en/latest/quickstart/installation.html#mamba-installation) first (...yet another package/dependency manager).]\n",
        "\n",
        "ESMValTool can also work with Julia, R, NCL. Do check installation instructions for these extensions.\n",
        "\n",
        "[back to contents](##Contents)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "<a id='ESMValTool'></a>\n",
        "\n",
        "**考虑用于处理气候数据的 Python 环境**\n",
        "\n",
        "* **地球系统模型评估工具 (ESMValTool)**\n",
        "![esmvaltool](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/ESMValTool-logo-2.png?raw=1)\n",
        "\n",
        "本周，根据需要简单地安装软件包应该是可以的……\n",
        "\n",
        "然而，如果你预见到未来可能的需求，你可能想投入一点精力来设置一个专门的 Python 环境来处理气候数据。如果是这样，一个很好的起点是 `ESMValTool` 项目。\n",
        "\n",
        "[ESMValTool](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/index.html) 是一个社区主导的、开源的软件工具集（基于 Python 3 构建），旨在改进气候模型的诊断和理解。虽然重点一直是在气候模型而非观测上，但不将模型结果与观测进行比较，就无法量化模型偏差。因此，[ESMValTool](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/index.html) 也具有一些分析和操作大型数据集的能力。\n",
        "\n",
        "[ESMValTool](https://docs.esmvaltool.org/projects/esmvalcore/en/latest/index.html) 可以离线工作，但由于气候数据量非常大，它主要是设计用于提供海量数据本地访问的数据中心。地球系统网格联合会 ([ESGF](https://esgf.llnl.gov)) 就是这样一个倡议。ESMValTool 提供了在 HPC（高性能计算）系统上工作的功能。\n",
        "\n",
        "我们这里不会专门使用 ESMValTool，但它会安装其他方便的工具，而且你也可能会好奇想要自己研究一下 ESMValTool 的功能。它现在已常规用于气候建模研究。\n",
        "\n",
        "更多信息和帮助可以在 https://www.esmvaltool.org 找到，也可以在 [ESMValGroup](https://github.com/ESMValGroup) github 页面上找到。[ESMValTool 教程](https://esmvalgroup.github.io/ESMValTool_Tutorial/) 是入门的绝佳资源。\n",
        "\n",
        "\n",
        "**可选：安装 ESMValTool（注意：学生安装经常失败 - 课程不要求安装此工具）**\n",
        "\n",
        "[安装说明](https://docs.esmvaltool.org/en/latest/quickstart/installation.html)因操作系统而异，并且它在很大程度上是一个实验性的、正在开发中的工具——请记住这一点！\n",
        "\n",
        "[注：ESMValTool 2.0 需要 Unix（或类 Unix）操作系统和 Python 3.7+。（支持 Python 3.7, 3.8 和 3.9）。在我的笔记本电脑（Macbook pro 13'' M1 2020, OS Monterey 12.0.1）上，我在 2021 年 11 月使用 Python 3.9.7 安装了它……但我是在 'conda' 还是安装选项时安装的。ESMValTool 开发者最近已将安装方法更改为 `mamba`。请参考在线/官方说明）。你现在还需要先[安装 `mamba`](https://docs.esmvaltool.org/en/latest/quickstart/installation.html#mamba-installation)（……这是另一个包/依赖管理器）。]\n",
        "\n",
        "ESMValTool 也可以与 Julia, R, NCL 一起工作。请检查这些扩展的安装说明。\n",
        "\n",
        "[back to contents](##Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（避坑指南）**\n",
        "\n",
        "这一块其实是教授在介绍**专业人士用的重型武器**，但对你这周的作业来说，这是一个**坑**，请小心绕开：\n",
        "\n",
        "1.  **ESMValTool 是个啥？**\n",
        "    * 这是一个专门用来**挑气候模型毛病**（诊断和评估）的工具包。\n",
        "    * 它是给长期做气候研究的人准备的，可以处理海量数据。\n",
        "\n",
        "2.  **你需要用它吗？**\n",
        "    * **不需要！** 教授明确说了“我们不会专门使用 ESMValTool”。\n",
        "    * 这周只需要安装一些基础的包就能搞定作业。\n",
        "\n",
        "3.  **为什么教授要提它？**\n",
        "    * 主要是为了扩展知识面，告诉你如果以后还要在这个领域深造，有个好东西叫 ESMValTool。\n",
        "    * 它会自动安装很多依赖包，如果你把环境配好了，以后用别的工具也方便。\n",
        "\n",
        "4.  **千万别踩的坑（重要）：**\n",
        "    * **不要尝试安装它！** 除非你时间多得没处花。\n",
        "    * 教授警告了：**学生安装这个经常失败**（NB: this often fails for students）。\n",
        "    * 它的安装很麻烦，对操作系统有要求（Unix），还要装 `mamba` 这种复杂的包管理器。你今天生病还要补作业，**直接跳过安装这一步，千万别在这浪费时间。**"
      ],
      "metadata": {
        "id": "l2b-fm32rBMy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is99bp4CmDmt"
      },
      "source": [
        "<a id='climatedata'></a>\n",
        "\n",
        "# **处理气候数据和气候模型(Working with climate data and climate models)**\n",
        "\n",
        "### **地球系统网格联合会 (ESGF)(Earth System Grid Federation (ESGF))**\n",
        "\n",
        "![esgf](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/ESGF_logo.png?raw=1)\n",
        "\n",
        "\n",
        "[ESGF](https://esgf.llnl.gov) is an open-source platform that provides distributed access to peta/exa-scale scientific data, **globally**.\n",
        "ESGF is an interagency and international effort led by the US Department of Energy (DOE), co-funded by National Aeronautics and Space Administration (NASA), National Oceanic and Atmospheric Administration (NOAA), the US National Science Foundation (NSF), and other international partners, such as the Max Planck Institute for Meteorology (MPI-M) German Climate Computing Centre (DKRZ), the Australian National University (ANU) National Computational Infrastructure (NCI), Institut Pierre-Simon Laplace (IPSL), and the **Centre for Environmental Data Analysis ([CEDA](https://www.ceda.ac.uk))** in the UK. For information about **CMIP6** experiments, go [here](https://wcrp-cmip.org/cmip-phases/cmip6/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**这里是关于 ESGF（地球系统网格联合会）的部分。这一块主要是在介绍全球气候数据的“总仓库”。**"
      ],
      "metadata": {
        "id": "u5imeeT6r72M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "<a id='climatedata'></a>\n",
        "\n",
        "**处理气候数据和气候模型**\n",
        "\n",
        "**地球系统网格联合会 (ESGF)**\n",
        "\n",
        "![esgf](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/ESGF_logo.png?raw=1)\n",
        "\n",
        "\n",
        "[ESGF](https://esgf.llnl.gov) 是一个开源平台，**在全球范围内**提供对 PB/Exa 级（海量）科学数据的分布式访问。\n",
        "ESGF 是一个由美国能源部 (DOE) 领导的跨机构和国际性努力，由美国国家航空航天局 (NASA)、美国国家海洋和大气管理局 (NOAA)、美国国家科学基金会 (NSF) 以及其他国际合作伙伴共同资助，例如马克斯·普朗克气象研究所 (MPI-M)、德国气候计算中心 (DKRZ)、澳大利亚国立大学 (ANU) 国家计算基础设施 (NCI)、皮埃尔-西蒙·拉普拉斯研究所 (IPSL)，以及英国的 **环境数据分析中心 ([CEDA](https://www.ceda.ac.uk))**。关于 **CMIP6** 实验的信息，请点击[这里](https://wcrp-cmip.org/cmip-phases/cmip6/)。\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这就是个“全球图书馆”）**\n",
        "\n",
        "这一段可能看着名字挺多挺吓人，其实就讲了一件事：**全世界的气候数据都存在哪？**\n",
        "\n",
        "1.  **ESGF 是什么？**\n",
        "    * 它是全球气候数据的**总网络/总联盟**。\n",
        "    * 把它想象成一个 **“全球图书馆联盟”** ，各个国家（美国、德国、英国等）都有分馆，但大家都用这一套系统来共享数据。\n",
        "\n",
        "2.  **它为什么重要？**\n",
        "    * 因为你后面要用的 **CMIP6** 数据（目前最新的全球气候模型实验数据）就是通过这个平台发布的。\n",
        "    * 只要你搞气候研究，这就是你绕不开的“数据源头”。\n",
        "\n",
        "3.  **和英国有什么关系？**\n",
        "    * 英国的分馆叫 **CEDA**。\n",
        "    * 也就是你之前注册账号的那个地方。你在 CEDA 下载数据，其实就是在访问 ESGF 全球网络的一部分。\n",
        "\n",
        "**一句话理解：** ESGF 是全球科学家共享气候数据的大平台，我们要用的 CMIP6 数据就在这里面，而我们在英国主要通过 CEDA 这个入口进去拿数据。"
      ],
      "metadata": {
        "id": "75KVPm79ruNy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7lH-9JUmDmt"
      },
      "source": [
        "<a id='CEDA'></a>\n",
        "\n",
        "### **对于英国：环境数据分析中心 (CEDA)(For the UK: Center for Environmental Data Analysis (CEDA))**\n",
        "\n",
        "![ceda](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/ceda_archive_logo_transp_white_3_h80.png?raw=1)\n",
        "\n",
        "[CEDA](https://www.ceda.ac.uk) serves the UK environmental science community. It is a component of ESGF.\n",
        "\n",
        "CEDA has two main branches:\n",
        "1. [CEDA-Archive: https://archive.ceda.ac.uk](https://archive.ceda.ac.uk)\n",
        "2. [JASMIN: https://jasmin.ac.uk](https://jasmin.ac.uk)\n",
        "\n",
        "[CEDA-Archive](https://archive.ceda.ac.uk) serves as the national (UK) data centre for atmospheric and earth observation research. It currently holds >18 Petabytes of atmospheric and earth observation data from a variet of sources, such as aircraft campaigns, satellites imagery, automatic weather stations, climate models, etc.\n",
        "\n",
        "CEDA is one of 5 UK data centers, comprising the Environmental Data Service ([EDS](https://nerc.ukri.org/research/sites/environmental-data-service-eds/)), supported by the UK Research and Innovation's National Environmental Research Council (NERC). The 5 centers are\n",
        "1. British Oceanographic Data Centre (Marine),\n",
        "2. CEDA (Atmospheric, Earth Observation, and Solar and space physics),\n",
        "3. Environmental Information Data Centre (Terrestrial and freshwater),\n",
        "4. National Geoscience Data Centre (Geoscience), and\n",
        "5. Polar Data Centre (Polar and cryosphere).\n",
        "   \n",
        "As the focus is on climate here, CEDA is the most relevant, but it is useful to know of these resources if you end up working in other fields.\n",
        "\n",
        "CEDA also serves as the Data Distribution Center ([DDC](https://www.ipcc-data.org)) for the Intergovernmental Panel on Climate Change ([IPCC](https://www.ipcc.ch)).\n",
        "\n",
        "[JASMIN](https://jasmin.ac.uk) is a data intensive supercomputer for environmental science. It currently supports over 160 projects ranging from climate science and oceanography to air pollution, earthquake deformation or biodiversity studies. JASMIN consists of multi-Petabyte fast storage, co-located with data analysis computing facilities. This would be the \"go-to\" tool for processing large amounts of climate data, which are directly linked to the system (through CEDA). We will here work offline (using local storage), but JASMIN and CEDA are useful resources for anyone working in environmental science in the UK (most nations don't have these facilities - they are rare and valuable!).\n",
        "\n",
        "Since CEDA and JASMIN are linked, the **compute power is linked to the data**, making for an efficient environment for data analysis.\n",
        "\n",
        "#### Getting access\n",
        "\n",
        "Obviously, maintaining these facilities has a cost and therefore not all users will have the same access priviledges. Members of educational, academic and research institutions in the UK can often access these infrastructures for free (pending one can demonstrate need and subject to a fair-use policy). Use and access will likely not be free for commercial applications.  \n",
        "\n",
        "**Use your Imperial College London email adresses (or from another UK institution) to register!**\n",
        "\n",
        "In order to use the CEDA-Archive, it is necessary to first [register for an account](https://services.ceda.ac.uk/cedasite/register/info/). This is generally quick, easy and free.\n",
        "\n",
        "Similarly, access to JASMIN requires an account, but this involves a more complex multi-step process. First, one must ask for a [JASMIN portal account](https://help.jasmin.ac.uk/article/4435-get-a-jasmin-account). If granted, one can then register for a [jasmin-login account] (https://help.jasmin.ac.uk/article/161-get-login-account). The type of account given will depend on need and priviledges requested. Users granted jasmin-login access get a HOME directory of 100GB of storage and can access the shared JASMIN servers (Scientific servers, data transfer servers, and the LOTUS HPC cluster).\n",
        "   \n",
        "[back to contents](#Contents)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**这里主要是在介绍英国本土的“数据大本营”——CEDA。这一块对你今天的 Practical 非常重要，因为你必须要有它的账号才能下载 UKCP18 数据。**\n",
        "\n",
        "----\n",
        "\n",
        "**Part 1：翻译**\n",
        "\n",
        "<a id='CEDA'></a>\n",
        "\n",
        "**对于英国：环境数据分析中心 (CEDA)**\n",
        "\n",
        "![ceda](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/ceda_archive_logo_transp_white_3_h80.png?raw=1)\n",
        "\n",
        "[CEDA](https://www.ceda.ac.uk) 服务于英国环境科学界。它是 ESGF 的一个组成部分。\n",
        "\n",
        "CEDA 有两个主要分支：\n",
        "1. [CEDA-Archive: https://archive.ceda.ac.uk](https://archive.ceda.ac.uk)\n",
        "2. [JASMIN: https://jasmin.ac.uk](https://jasmin.ac.uk)\n",
        "\n",
        "[CEDA-Archive](https://archive.ceda.ac.uk) 作为大气和地球观测研究的国家（英国）数据中心。它目前持有超过 18 PB 的大气和地球观测数据，这些数据来自各种来源，如飞机活动、卫星图像、自动气象站、气候模型等。\n",
        "\n",
        "CEDA 是组成环境数据服务 ([EDS](https://nerc.ukri.org/research/sites/environmental-data-service-eds/)) 的 5 个英国数据中心之一，由英国研究与创新署的国家环境研究委员会 (NERC) 支持。这 5 个中心是：\n",
        "1. 英国海洋数据中心（海洋），\n",
        "2. CEDA（大气、地球观测以及太阳和空间物理），\n",
        "3. 环境信息数据中心（陆地和淡水），\n",
        "4. 国家地学数据中心（地学），以及\n",
        "5. 极地数据中心（极地和冰冻圈）。\n",
        "   \n",
        "由于这里的重点是气候，CEDA 是最相关的，但如果你最终在其他领域工作，了解这些资源是有用的。\n",
        "\n",
        "CEDA 还作为政府间气候变化专门委员会 ([IPCC](https://www.ipcc.ch)) 的数据分发中心 ([DDC](https://www.ipcc-data.org))。\n",
        "\n",
        "[JASMIN](https://jasmin.ac.uk) 是一台用于环境科学的数据密集型超级计算机。它目前支持超过 160 个项目，范围从气候科学和海洋学到空气污染、地震形变或生物多样性研究。JASMIN 由数 PB 的快速存储组成，并与数据分析计算设施共处一地。这将是处理大量气候数据的“首选”工具，这些数据直接链接到系统（通过 CEDA）。我们这里将离线工作（使用本地存储），但对于任何在英国从事环境科学工作的人来说，JASMIN 和 CEDA 都是有用的资源（大多数国家没有这些设施——它们稀有且宝贵！）。\n",
        "\n",
        "由于 CEDA 和 JASMIN 是链接在一起的，**计算能力与数据相连**，从而为数据分析提供了一个高效的环境。\n",
        "\n",
        "#### 获取访问权限\n",
        "\n",
        "显然，维护这些设施是有成本的，因此并非所有用户都拥有相同的访问权限。英国教育、学术和研究机构的成员通常可以免费访问这些基础设施（前提是可以证明需求并遵守公平使用政策）。商业应用的使用和访问可能不是免费的。\n",
        "\n",
        "**使用你的帝国理工学院邮箱地址（或来自其他英国机构的邮箱）进行注册！**\n",
        "\n",
        "为了使用 CEDA-Archive，必须先[注册一个账户](https://services.ceda.ac.uk/cedasite/register/info/)。这通常很快、很简单且免费。\n",
        "\n",
        "同样，访问 JASMIN 也需要一个账户，但这涉及更复杂的多步骤流程。首先，必须申请一个 [JASMIN 门户账户](https://help.jasmin.ac.uk/article/4435-get-a-jasmin-account)。如果获批，然后可以注册一个 [jasmin-login 账户](https://help.jasmin.ac.uk/article/161-get-login-account)。给予的账户类型将取决于需求和请求的权限。获得 jasmin-login 访问权限的用户将获得 100GB 的 HOME 目录存储空间，并可以访问共享的 JASMIN 服务器（科学服务器、数据传输服务器和 LOTUS HPC 集群）。\n",
        "   \n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这是你的“身份证”和“机房”）**\n",
        "\n",
        "这一段介绍了英国环境科学的两大支柱，你需要搞清楚它们的关系，因为你要用其中一个：\n",
        "\n",
        "1.  **CEDA 是什么？**\n",
        "    * 它是英国环境数据的**总管**。\n",
        "    * 它是 ESGF（全球网络）在英国的分部。\n",
        "    * **关键点：** 你今天的 Practical 必须要去 CEDA 注册账号，否则你没法下载 UKCP18 数据。记得用学校邮箱注册！\n",
        "\n",
        "2.  **CEDA 分为哪两部分？**\n",
        "    * **CEDA-Archive（档案库）：** 这是一个**巨大的硬盘**，里面存了 18PB 的数据（卫星图、气候模型等）。你需要访问这里来**下载数据**。\n",
        "    * **JASMIN（超级计算机）：** 这是一台**超级电脑**，专门用来跑大数据的。\n",
        "\n",
        "3.  **你需要用 JASMIN 吗？**\n",
        "    * **这周不需要！** 教授说了我们这次是“离线工作”（work offline），也就是把数据下载到你自己的电脑或 Colab 上跑，不占用这台超级计算机。\n",
        "    * JASMIN 的账号申请很麻烦，这周先别管它，只要搞定 CEDA-Archive 的账号就行。\n",
        "\n",
        "**一句话总结：** CEDA 是你下载英国数据的地方，马上去用学校邮箱注册一个 **CEDA-Archive** 的账号，至于那个叫 JASMIN 的超级计算机，这周先不用管。"
      ],
      "metadata": {
        "id": "caFGt5rysJ4O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxRHuDICmDmt"
      },
      "source": [
        "<a id='fileformat'></a>\n",
        "\n",
        "## **文件格式(File formats)**\n",
        "\n",
        "![unidata](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/unidatalogo.png?raw=1)\n",
        "\n",
        "It is customary for different scientific communities to use different data formats ... it would be too easy otherwise!\n",
        "\n",
        "More seriously, these file formats ususally evolve out of necessity. Not all data are equal in quantity, precision, type, etc. Each community therefore tends to evolve data formats specific to their needs and to cater for certain applications.\n",
        "\n",
        "* **TEXT**: Files in **text formats**, typically encoded as [ASCII](https://en.wikipedia.org/wiki/ASCII) or [Unicode](https://en.wikipedia.org/wiki/Unicode) (often UTF-8, or UTF-16), simply have bytes that can be interpreted as text characters. ASCII represents the first 128 charcters of UTF-8, so ASCII is a subset of Unicode. ASCII was developed as 7-bit code, but since UTF-8 kept the ASCII definitions when moving to 8-bits, UTF-8 (and subsequent) are backwards compatible with ASCII. Text formats are very attractive and useful as the file content is directly human-readable but this only works well for small files because data storage of such data is inefficient.\n",
        "\n",
        "* **BINARY**: More commonly, we will encouter files in **binary formats**. Sadly, binary files are not human-readable. They need to be interpreted, **using sets of rules set by whoever prescribed the format**. These rules have to be known to decode the data effectively (this can be a source of problem...and cause for frustration!).  \n",
        "  \n",
        "Some common data formats found in environmental sciences are summarized in [this table](https://help.ceda.ac.uk/article/104-file-formats).\n",
        "\n",
        "`pandas` I/O tool can handle most of them (read and write capability), as summarized in this  [table](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).\n",
        "\n",
        "`gdal`, the Geospatial Data Abstraction Library ([GDAL](https://gdal.org)), is also able to deal with these (and many more).\n",
        "\n",
        "Some of the most imporant, or common, file formats in climate studies are:\n",
        "\n",
        "1. `.csv`: These are comma-separated-values,i.e text separated by commas. This format (or the related tab-delimited text format) is typically used for small datasets, such as when saving an Excel spreadsheet into a format useful for other programmes. Certain data-loggers can also produce text-based datasets. Data in this format will tend to be in the form of small tables or as 1-D data, such as time-series data.  \n",
        "   \n",
        "2. `.hdf`: The so-called [Hierarchichal Data Format](https://www.hdfgroup.org) is the format commonly associated with satellite imagery, where they typically hold raster data. HDF files tend to be very large and can have quite complex structure. The current version is `HDF5`, which differs from legacy `HDF4`. Common HDF file suffixes are: .hdf, .h4, .hdf4, .he2, .h5, .hdf5, .he5. `pandas` is able to read HDF data into `python` using [`pandas.read_hdf`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html). The more specific `h5py` python package ([HD5 For Python](http://www.h5py.org)) could provide more control, if needed.  \n",
        "   \n",
        "3. `.nc` : NetCDF files are a typical format for gridded datasets with more than one dimension (i.e 2D to ND data), for example data with a latitude, longitude, depth/altitude and certain values. It is the data currency format used to exchange climate model outputs. Note, this does not mean that raw output of climate models comes as `.nc` files. Generally, models use their own binary formats during computation, to improve speed and storage. Results are only transformed into convenient `.nc` format at the end, where the goal is data communication and interpretation. The [`xarray`](http://xarray.pydata.org/en/stable/) python library is tailored to work with `netCDF` data formats.  \n",
        "   \n",
        "4. `grb`: [GRIB](https://en.wikipedia.org/wiki/GRIB) files, which stands for 'GRIdded Binary' or 'General Regularly-distributed Information in Binary form', are often used in meteorology. They are the types of files used by ECMWF. There have been 3 versions: i) version 0 is legacy, no longer in use, version 2 is slowly replacing version 1, but version 1 is used by most meteorology center today. The [`cfgrib`](https://github.com/ecmwf/cfgrib) python library from ECMWF will help translate `.grb` into `xarray`, `netcdf` or `hdf`. Alternatively, the [`pygrib`](https://github.com/jswhit/pygrib) python package will provide a way to read .grb files, and modify existing ones, but not create them.\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**这一块非常重要。**\n",
        "\n",
        "**你在 Practical 课上下载下来的文件全都是这些奇怪的后缀（.nc, .grib），如果不理解它们是什么，你连怎么用 Python 读取它们都不知道。**\n",
        "\n",
        "**期末 Quiz 也非常喜欢考这几种格式的区别。**\n",
        "\n",
        "----\n",
        "\n",
        "**Part 1：翻译**\n",
        "\n",
        "<a id='fileformat'></a>\n",
        "\n",
        "**文件格式**\n",
        "\n",
        "![unidata](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/unidatalogo.png?raw=1)\n",
        "\n",
        "不同的科学社区习惯使用不同的数据格式……要是能统一就太容易了！\n",
        "\n",
        "严肃地说，这些文件格式通常是出于必要而演变出来的。并非所有数据在数量、精度、类型等方面都是平等的。因此，每个社区往往会演变出特定于其需求并迎合特定应用的数据格式。\n",
        "\n",
        "* **文本 (TEXT)**：**文本格式**的文件，通常编码为 [ASCII](https://en.wikipedia.org/wiki/ASCII) 或 [Unicode](https://en.wikipedia.org/wiki/Unicode)（通常是 UTF-8 或 UTF-16），仅包含可以解释为文本字符的字节。ASCII 代表 UTF-8 的前 128 个字符，因此 ASCII 是 Unicode 的子集。ASCII 最初是作为 7 位代码开发的，但由于 UTF-8 在转换为 8 位时保留了 ASCII 定义，因此 UTF-8（及后续版本）向后兼容 ASCII。文本格式非常有吸引力且有用，因为文件内容是直接**人类可读的**，但这仅适用于小文件，因为这种数据的存储效率很低。\n",
        "\n",
        "* **二进制 (BINARY)**：更常见的是，我们会遇到**二进制格式**的文件。遗憾的是，二进制文件不是人类可读的。它们需要被解释，**使用由制定该格式的人设定的一套规则**。必须了解这些规则才能有效地解码数据（这可能是问题的来源……也是挫折的根源！）。\n",
        "  \n",
        "  [这个表格](https://help.ceda.ac.uk/article/104-file-formats)总结了环境科学中发现的一些常见数据格式。\n",
        "\n",
        "  `pandas` I/O 工具可以处理其中大多数（读取和写入能力），如[此表](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)所示。\n",
        "\n",
        "  `gdal`，即地理空间数据抽象库 ([GDAL](https://gdal.org))，也能够处理这些（以及更多）格式。\n",
        "\n",
        "\n",
        "\n",
        "**气候研究中一些最重要或最常见的文件格式包括：**\n",
        "\n",
        "1.  `.csv`：这是逗号分隔值，即用逗号分隔的文本。这种格式（或相关的制表符分隔文本格式）通常用于**小型数据集**，例如将 Excel 电子表格保存为对其他程序有用的格式时。某些数据记录器也可以生成基于文本的数据集。这种格式的数据往往是小表格的形式或一维数据，如时间序列数据。\n",
        "    \n",
        "2.  `.hdf`：所谓的 [分层数据格式 (Hierarchichal Data Format)](https://www.hdfgroup.org) 是通常与**卫星图像**相关的格式，它们通常包含栅格数据。HDF 文件往往非常大，并且可能具有相当复杂的结构。当前版本是 `HDF5`，它不同于旧版 `HDF4`。常见的 HDF 文件后缀有：.hdf, .h4, .hdf4, .he2, .h5, .hdf5, .he5。`pandas` 可以使用 [`pandas.read_hdf`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_hdf.html) 将 HDF 数据读取到 `python` 中。如果需要，更具体的 `h5py` python 包 ([HD5 For Python](http://www.h5py.org)) 可以提供更多控制。\n",
        "    \n",
        "3.  `.nc`：**NetCDF** 文件是具有多个维度的**网格化数据集**（即 2D 到 ND 数据）的典型格式，例如具有经度、纬度、深度/高度和特定值的数据。它是用于交换气候模型输出的**数据通用货币**格式。注意，这并不意味着气候模型的原始输出就是 `.nc` 文件。通常，模型在计算过程中使用自己的二进制格式以提高速度和存储。结果仅在最后转化为方便的 `.nc` 格式，此时的目标是数据交流和解释。[`xarray`](http://xarray.pydata.org/en/stable/) python 库是专门为处理 `netCDF` 数据格式而量身定制的。\n",
        "    \n",
        "\n",
        "\n",
        "4.  `grb`：[GRIB](https://en.wikipedia.org/wiki/GRIB) 文件，代表“网格化二进制”或“二进制形式的通用定期分布信息”，常用于**气象学**。它们是 **ECMWF** 使用的文件类型。已有 3 个版本：i) 版本 0 是旧版，不再使用；版本 2 正在慢慢取代版本 1，但版本 1 今天仍被大多数气象中心使用。来自 ECMWF 的 [`cfgrib`](https://github.com/ecmwf/cfgrib) python 库有助于将 `.grb` 转换为 `xarray`、`netcdf` 或 `hdf`。或者，[`pygrib`](https://github.com/jswhit/pygrib) python 包提供了一种读取 .grb 文件并修改现有文件的方法，但不能创建它们。\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "**Part 2：大白话总结（认准你的“文件后缀”）**\n",
        "\n",
        "这一段是 Practical 的基础，如果不认识这些后缀，你看到文件名就懵了。\n",
        "\n",
        "1.  **文本 vs 二进制：**\n",
        "    * **CSV (文本)**：就是简单的 Excel 表格，**人眼能看懂**。但是气候数据太大（几千万行），存成 CSV 电脑会死机，所以只用来存小数据（比如某个气象站的单点记录）。\n",
        "    * **二进制 (Binary)**：全是 010101，**人眼看不懂**，必须用代码读。但是它存取快、体积小，适合大数据。\n",
        "\n",
        "2.  **三大气候界“网红”格式（Quiz 必考）：**\n",
        "    * **.nc (NetCDF)**：**【最重要】** 哪怕你忘了别的，也要记住这个！这是气候模型（如 UKCP18）的标准格式。它是**多维的**（像一个魔方，有长、宽、高、时间），而且**自带说明书**（Metadata）。你这周主要就和它打交道。\n",
        "    * **.grb (GRIB)**：**气象预报**专用（如 ECMWF 的 ERA5 原始数据）。稍微难读一点，通常我们要把它转成 .nc 再分析。\n",
        "    * **.hdf**：**卫星图**专用。\n",
        "\n",
        "3.  **用什么读？**\n",
        "    * 读 **.nc** 文件：用 Python 的 **`xarray`** 库（神器，专门读多维数据）。\n",
        "    * 读 **.csv** 文件：用 **`pandas`** 库。\n",
        "\n",
        "**一句话攻略：** 看到 **.nc** 后缀别慌，它是朋友，用 `xarray` 打开它；看到 **.csv** 就是小表格，用 `pandas` 打开它。"
      ],
      "metadata": {
        "id": "amPjyNYBspT0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-laS3WKmDmt"
      },
      "source": [
        "* ## **文件聚合与压缩 (File aggregation and compression)**\n",
        "\n",
        "Data often come **aggregated** and **compressed**. These will typically have the following suffixes:\n",
        "\n",
        "1. `.tar`: The .tar suffix represents an **aggregation** of data, i.e. many files grouped togeter into a bundle of joy called a 'tar ball'. 'Tar balls' have to be 'untarred' (disaggregated) before being usable.\n",
        "2. `.zip`, `.gzip`, ...: This relates to data **compression** (this is different than aggregation). Compressed data have to be uncompressed before being usable. Uncompressing will increase the size storage requirement of the data! Because they are smaller, it is convenient to exchange compressed files.\n",
        "\n",
        "It is also common to find files that are both \"tarred\" and \"zipped\", typically with suffix `.tar.gz`.\n",
        "To unzip and untar a file, you can use (on MacOS and Linux):\n",
        ">`tar -xzf <tarfile>`\n",
        "\n",
        "where options flags `-xzf` stand for `Xtract Ze File` ... as \"the Terminator\" would say (`man tar` will of course provide more detail):\n",
        "* x: Extract, i.e. untar the tarfile.\n",
        "* z: Use gzip to uncompress (this option can be omitted if the file is 'just' aggregated and not compressed, i.e. just .tar)\n",
        "* f: specifies the input tarfile to operate on.\n",
        "\n",
        "To *create* an aggregated compressed file, the synthax is similar, just replace option `x` (extract) with `c` (for create):\n",
        ">`tar -czf tarfile <list of files to tar and gzip>`.\n",
        "\n",
        "Note that the problem of packaging data and making them available is becoming quite an important one. As model resolution increases, so is the amount of data generated. There are now projects looking into this problem specifically, for example the [EERIE project](https://eerie-project.eu/blog/2025/12/16/taming-the-petabytes-making-km-scale-climate-data-usable/).\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**这是关于文件打包和压缩的部分。**\n",
        "\n",
        "**这一块主要是教你基本的 Linux/Mac 命令行操作，因为气候数据文件通常都很大，下载下来往往是一个压缩包，你得知道怎么解压。**\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "* **文件聚合与压缩**\n",
        "\n",
        "数据通常以**聚合**和**压缩**的形式出现。这些通常具有以下后缀：\n",
        "\n",
        "1.  `.tar`：.tar 后缀代表数据的**聚合**，即许多文件被组合在一起，形成所谓的“tar ball”（压缩球/归档包）。“Tar balls”必须先“untarred”（解包/解聚合）才能使用。\n",
        "2.  `.zip`, `.gzip`, ...：这与数据**压缩**有关（这不同于聚合）。压缩数据必须先解压缩才能使用。解压缩会增加数据的存储空间需求！因为它们更小，所以交换压缩文件很方便。\n",
        "\n",
        "经常会发现既被“tarred”又被“zipped”的文件，通常带有后缀 `.tar.gz`。\n",
        "要解压和解包一个文件，你可以使用（在 MacOS 和 Linux 上）：\n",
        ">`tar -xzf <tarfile>`\n",
        "\n",
        "这里的选项标志 `-xzf` 代表 `Xtract Ze File`（提取那个文件）……就像“终结者”会说的那样（`man tar` 当然会提供更多细节）：\n",
        "* x: Extract，即解包 tar 文件。\n",
        "* z: 使用 gzip 解压缩（如果文件“仅仅”是聚合而没有压缩，即只是 .tar，则可以省略此选项）。\n",
        "* f: 指定要操作的输入 tar 文件。\n",
        "\n",
        "要*创建*一个聚合的压缩文件，语法是类似的，只需将选项 `x`（提取）替换为 `c`（用于创建 create）：\n",
        ">`tar -czf tarfile <list of files to tar and gzip>`。\n",
        "\n",
        "请注意，打包数据并使其可用的问题正变得相当重要。随着模型分辨率的增加，生成的数据量也在增加。现在有一些项目专门研究这个问题，例如 [EERIE 项目](https://eerie-project.eu/blog/2025/12/16/taming-the-petabytes-making-km-scale-climate-data-usable/)。\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（解压指南）**\n",
        "\n",
        "这一段虽然短，但是**极其、极其实用**。因为你在 CEDA 或其他网站下载的数据，十有八九是 `.tar.gz` 格式的。\n",
        "\n",
        "1.  **分清两个概念：**\n",
        "    * **聚合 (Aggregation, .tar)**：好比把一堆衣服塞进一个**行李箱**里。体积没变小，只是把散乱的东西包在一起了。\n",
        "    * **压缩 (Compression, .zip/.gzip)**：好比用**真空收纳袋**抽气。体积变小了，方便传输。\n",
        "\n",
        "2.  **什么是 .tar.gz？**\n",
        "    * 就是**先塞进行李箱，再抽真空**。这是 Linux/Unix 系统最常见的数据打包方式。\n",
        "\n",
        "3.  **怎么打开？（背下这个命令！）**\n",
        "    * 如果你用的是 Mac 或者 Linux（学校的机房通常是 Linux），只需要记住这一行神咒语：\n",
        "    * `tar -xzf 文件名.tar.gz`\n",
        "    * **记忆口诀：** **X**tract **Z**e **F**ile (提取那个文件)。\n",
        "\n",
        "**注意：** 如果你在 Windows 上，通常右键用 7-Zip 或 WinRAR 也能解压，但在 Jupyter Notebook 或服务器终端里，你必须会敲这个命令。"
      ],
      "metadata": {
        "id": "iY-7EejLtD5I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nftild8_mDmt"
      },
      "source": [
        "<a id='Metadata'></a>\n",
        "\n",
        "## **元数据（MetaData）的重要性(The importance of Metadata)**\n",
        "\n",
        "[Metadata](https://help.ceda.ac.uk/article/4428-metadata-basics) **relate to all the information necessary to interpret, understand and use a given dataset. Metadata are not the data themselves, but are required to provide context and therefore inform the data analysis.**\n",
        "\n",
        "There are two types of metadata, \"**discover metadata**\", which can be used to locate the dataset in a search, and \"**detailed metadata**\", which contain the information necessary to use the data (ideally) without having to ask the data provider for more information (even if one of the key piece of information should be the data provider information and contact detail!).\n",
        "\n",
        "There is no upper limit as to what information should be included in the metadata, but typical metadata comprise information about when and where the observations, or the file, where produced, how they were produced (which instrument, technique, algorithm were used), information about the experiment the data come from, maybe some information about data accuracy (if not provided as seperate data in the file), information about who produced the data or the file, including contact information, or appropriate reference(s), and, if relevant, additional information about the research context, i.e. which project is associated with these data (this could help the user understand if more data from this group/project exist or can be expected). If the data represent a subset of a larger dataset, this should also be mentioned.\n",
        "\n",
        "Another key pieces of information that the metadata should provide are a description of the variables in the file, with variable **names**, and the **units** of these variables.\n",
        "\n",
        "If the data in the file represent a processed version of other raw data, then one should add information about these raw data as well, their provenance, references, and the transformation that was made, etc. One should be able to track the chain of information all the way to the orginal observations (or model output).\n",
        "\n",
        "If the data represent gridded variables, then the nature of the grid on which these data are defined should also be explained.\n",
        "\n",
        "Similarly, if the position, or some other features of the data, depend on a reference system, that reference system should be given. This will be discussed in more depth during the introduction to geostatistics lecture.\n",
        "\n",
        "In the case of climate models (or other models, if relevant), the metadata should also include information about the model itself, the model name, version, the length of the integration, maybe a brief description of the spin-up, boundary conditions, initial conditions used to produce the simulation.\n",
        "\n",
        "Obviously the information that will be included in the metadata vary substantially. You should learn (and will discover - mostly through shear frustration when key information is missing!) what constitute useful metadata in your field of work.\n",
        "\n",
        "Metadata can be included in the file itself (if the file format allows it), as separate files, or in the form of some other documents. When sourcing data, always make a note of where the data come from. This is especially true if getting data online as it is very easy to download data and then forget where they come from. Detailed note-taking is part of the job of the data analyst (and that of any scientist!).\n",
        "\n",
        "When metadata is included in the file directly, this typically comes as a header, or in a specified location in the file. The netCDF format has rules for how/where to include metadata, which is one of the reason why netCDF data are particularly useful and popular in climate science.\n",
        "\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**这是关于元数据 (Metadata) 的部分。**\n",
        "\n",
        "**这一块是理论核心，它解释了为什么我们喜欢用 NetCDF 格式（因为自带说明书），以及为什么你以后做分析时要先看这些信息。**\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "<a id='Metadata'></a>\n",
        "\n",
        "**元数据（MetaData）的重要性**\n",
        "\n",
        "[元数据](https://help.ceda.ac.uk/article/4428-metadata-basics) **涉及解释、理解和使用给定数据集所需的所有信息。元数据不是数据本身，而是提供背景信息所必需的，因此可以为数据分析提供信息**。\n",
        "\n",
        "元数据有两种类型，“**发现元数据**”，可用于在搜索中定位数据集，以及“**详细元数据**”，它包含使用数据所需的信息（理想情况下），而无需向数据提供者询问更多信息（即使关键信息之一应该是数据提供者信息和联系方式！）。\n",
        "\n",
        "关于元数据中应包含哪些信息没有上限，但典型的元数据包括关于观测或文件是何时何地产生的、如何产生的（使用了哪种仪器、技术、算法）、数据来源的实验信息、可能还有关于数据准确性的一些信息（如果未作为单独的数据在文件中提供）、关于谁制作了数据或文件的信息（包括联系信息或适当的参考文献），以及如果相关，关于研究背景的额外信息，即哪个项目与这些数据相关联（这可以帮助用户了解是否存在或可以期待来自该小组/项目的更多数据）。如果数据代表较大数据集的子集，也应提及这一点。\n",
        "\n",
        "元数据应提供的另一个关键信息是对文件中变量的描述，包括变量**名称**和这些变量的**单位**。\n",
        "\n",
        "如果文件中的数据代表其他原始数据的处理版本，那么也应添加关于这些原始数据的信息，它们的出处、参考文献以及所做的转换等。人们应该能够追踪信息链一直到原始观测（或模型输出）。\n",
        "\n",
        "如果数据代表网格化变量，那么定义这些数据的网格性质也应得到解释。\n",
        "\n",
        "同样，如果数据的位置或其他特征取决于参考系统，则应给出该参考系统。这将在地质统计学导论讲座中更深入地讨论。\n",
        "\n",
        "在气候模型（或其他模型，如果相关）的情况下，元数据还应包括关于模型本身的信息、模型名称、版本、积分长度、可能还有关于预热（spin-up）、边界条件、用于产生模拟的初始条件的简要描述。\n",
        "\n",
        "显然，元数据中包含的信息差异很大。你应该学会（并且会发现——主要是通过当关键信息缺失时的纯粹挫败感！）什么构成了你工作领域中有用的元数据。\n",
        "\n",
        "元数据可以包含在文件本身中（如果文件格式允许），作为单独的文件，或以其他文档的形式存在。在获取数据时，始终记录数据的来源。如果在线获取数据，这一点尤其重要，因为很容易下载数据然后忘记它们来自哪里。详细的笔记是数据分析师（以及任何科学家！）工作的一部分。\n",
        "\n",
        "当元数据直接包含在文件中时，这通常作为标头（header），或在文件的特定位置出现。NetCDF 格式有关于如何/在哪里包含元数据的规则，这也是 NetCDF 数据在气候科学中特别有用和流行的原因之一。\n",
        "\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这就是数据的“说明书”）**\n",
        "\n",
        "这一段解释了为什么你不能只拿一个数字就跑，你需要知道这个数字背后的故事。\n",
        "\n",
        "1.  **元数据是什么？**\n",
        "    * 它是**数据的说明书**或**身份证**。\n",
        "    * 比如你看到一个数字 `30`，这是数据。\n",
        "    * 但 `30` 是什么？是摄氏度？是华氏度？是年龄？还是降水量？**告诉你“这是摄氏度”的信息，就是元数据**。\n",
        "\n",
        "2.  **为什么它至关重要？（Quiz 考点）**\n",
        "    * **单位（Units）：** 如果没有元数据告诉你单位，你可能会把 `mm/day`（毫米每天）当成 `mm/hour`（毫米每小时），整个分析就废了。\n",
        "    * **来源（Source）：** 你得知道数据是谁造的、用什么模型跑出来的，否则写论文（或者给 Tattinger 做报告）时没法交代数据来源。\n",
        "\n",
        "3.  **为什么 NetCDF 格式好？**\n",
        "    * 教授最后特意提了，NetCDF 之所以流行，就是因为它**强制把元数据塞进了文件里**。\n",
        "    * 你不用担心说明书丢了，因为说明书就印在数据文件头上。你这周做 Practical 时，用 `ncdump` 或 Python 看到的那些 `units=...`，`standard_name=...` 就是元数据。\n",
        "\n",
        "**一句话攻略：** 拿到数据别急着画图，先看元数据（Metadata），确认好变量的意思和单位，不然会闹笑话。"
      ],
      "metadata": {
        "id": "mygaJIx5tXHB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQzHx7TNmDmu"
      },
      "source": [
        " * ### The Climate and Forecast (CF) Metadata Conventions (prominent in climate science)\n",
        "\n",
        "Given the amount of data produced keeps increasing, data libraries are being developed. As in any library, certain sets of rules are required so that the data can be found later. Metadata are obviously a good way to ensure data can be catalogued and found by various search systems.\n",
        "\n",
        "Multiple types of conventions exist (see for example this [list of netCDF conventions from Unidata](https://www.unidata.ucar.edu/software/netcdf/conventions.html)). Ensuring that data files are produced in a way that follows conventions about content, vocabulary used and layout, allows for batch processing, easy extraction and automation! It is extremely useful (but should not prevent innovation).\n",
        "\n",
        "CEDA, as it focuses on climate and environmental data, relies extensively on the [Climate and Forecast (CF) Conventions](http://cfconventions.org). The CF Conventions are probably the most popular in climate science (it underpins the modelling effort of the IPCC). A detailed description of the latest release of the CF Conventions for netCDF can be found [here](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.9/cf-conventions.html).\n",
        "\n",
        "The CF Conventions were designed specifically for the netCDF format but they can be applied widely. The netCDF format enables creation of self-describing datasets by design. CF Conventions aim to ensure that  files contain sufficient metadata that they are self-describing in the sense that each variable in the file has an associated description of what it represents, including physical units if appropriate, and that each value can be located in space (relative to earth-based coordinates) and time. (Absence of such information in early (historical) datasets has hindered climate change science for decades - how does one measure change, if one cannot locate the observations in time and space?)\n",
        "\n",
        "One example of CF Conventions is to provide a list of **standard_names** for certain commonly-used variables. The [CF standard name table] (http://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html) will show what the names are and describe what they represent.\n",
        "\n",
        "To illustrate, imagine you work with data at the earth surface and label your variable 'temperature'. What did you mean exactly? Surface temperature of the ocean/land, or air temperature? etc. CF conventions ensure that climate scientists name common variables in the same way.\n",
        "\n",
        "A [python package](https://pypi.org/project/cfunits/) called `cfunits` provides an interface to the CF Convention. This is useful to combine and compare variables and convert various units. By relying on a package, fewer user-errors are made.   \n",
        "\n",
        "The CF Conventions are not fixed, they are evolving depending on needs and scientific progress. Although conventions are decided by a committee of experts, anyone can propose a change to the convention by engaging in the [discussion forum](https://cfconventions.org/discussion.html).\n",
        "\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**这一块讲的是气候科学界最通用的“命名规则”。如果说 NetCDF 是文件格式（容器），那 CF Conventions 就是怎么写标签的规则。**\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "* ### 气候与预报 (CF) 元数据公约 (The Climate and Forecast (CF) Metadata Conventions)（在气候科学中非常重要）\n",
        "\n",
        "鉴于产生的数据量持续增加，**数据资料库 (Data Libraries)** 正在被开发出来。就像在任何图书馆一样，需要某些规则集以便日后能找到数据。**元数据 (Metadata)** 显然是确保数据能被各种搜索系统编目和找到的好方法。\n",
        "\n",
        "存在多种类型的**公约 (Conventions)**（例如参见 [Unidata 的 netCDF 公约列表](https://www.unidata.ucar.edu/software/netcdf/conventions.html)）。确保数据文件的生成方式遵循关于内容、所用词汇和布局的公约，可以实现**批处理 (Batch Processing)**、轻松提取和自动化！这极其有用（但不应阻碍创新）。\n",
        "\n",
        "**环境数据分析中心 (CEDA)** 由于专注于气候和环境数据，广泛依赖于 [**气候与预报 (CF) 公约 (Climate and Forecast (CF) Conventions)**](http://cfconventions.org)。**CF 公约 (CF Conventions)** 可能是气候科学中最流行的公约（它支撑了 **政府间气候变化专门委员会 (IPCC)** 的建模工作）。关于最新的针对 **netCDF** 的 **CF 公约 (CF Conventions)** 的详细描述可以在[这里](http://cfconventions.org/Data/cf-conventions/cf-conventions-1.9/cf-conventions.html)找到。\n",
        "\n",
        "**CF 公约 (CF Conventions)** 是专门为 **netCDF** 格式设计的，但它们可以被广泛应用。**netCDF** 格式在设计上就支持创建**自描述 (Self-describing)** 的数据集。**CF 公约 (CF Conventions)** 旨在确保文件包含足够的**元数据 (Metadata)**，使其具有**自描述性 (Self-describing)**，即文件中的每个**变量 (Variable)** 都有相关的描述来说明它代表什么（如果适用，包括物理**单位 (Units)**），并且每个数值都可以在空间（相对于基于地球的坐标）和时间上定位。（早期（历史）数据集中缺乏此类信息阻碍了气候变化科学数十年——如果无法在时间和空间上定位观测结果，我们要如何测量变化？）\n",
        "\n",
        "**CF 公约 (CF Conventions)** 的一个例子是为某些常用**变量 (Variables)** 提供 **标准名称 (standard_names)** 列表。[**CF 标准名称表 (CF standard name table)**](http://cfconventions.org/Data/cf-standard-names/current/build/cf-standard-name-table.html) 将显示名称是什么并描述它们代表什么。\n",
        "\n",
        "举例说明，想象你处理地球表面的数据并将你的**变量 (Variable)** 标记为“温度 (temperature)”。你确切的意思是什么？是海洋/陆地的表面温度，还是空气温度？等等。**CF 公约 (CF Conventions)** 确保气候科学家以相同的方式命名常见变量。\n",
        "\n",
        "一个名为 `cfunits` 的 [**Python 包 (Python package)**](https://pypi.org/project/cfunits/) 提供了 **CF 公约 (CF Conventions)** 的接口。这对于组合和比较**变量 (Variables)** 以及转换各种**单位 (Units)** 非常有用。通过依赖一个包，可以减少用户错误。\n",
        "\n",
        "**CF 公约 (CF Conventions)** 不是固定的，它们随着需求和科学进步而演变。虽然公约由专家委员会决定，但任何人都可以通过参与 [讨论论坛](https://cfconventions.org/discussion.html) 提议对公约进行更改。\n",
        "\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这是“气候界的普通话”）**\n",
        "\n",
        "这一块非常关键，它解决了“大家各说各话”的问题。你今天的 Practical 里，如果看到变量名写得很长很奇怪（比如 `air_temperature` 而不是 `temp`），就是因为这个规则。\n",
        "\n",
        "1.  **CF Conventions (CF 公约) 是什么？**\n",
        "    * 它是气候科学界的 **“官方语言字典”** 。\n",
        "    * 它是专门配合 **NetCDF** 格式使用的。\n",
        "\n",
        "2.  **为什么要用它？（Standard Names 标准名称）**\n",
        "    * **避免歧义：** 你如果你只写 `temp`（温度），别人不知道是“水温”、“气温”还是“体温”。\n",
        "    * **统一标准：** CF 公约规定，大家必须用 **Standard Names (标准名称)**。比如所有的气温都必须叫 `air_temperature`，所有的降水都叫 `precipitation_flux`。\n",
        "    * 这样一来，你在英国写的代码，跑到美国的电脑上也能自动识别出这是气温数据，不需要人工去猜。\n",
        "\n",
        "3.  **它解决了什么历史遗留问题？**\n",
        "    * 以前的数据经常不写清楚时间和地点（坐标），导致现在的科学家拿到旧数据根本没法用（不知道是哪年的，也不知道是哪里的）。\n",
        "    * CF 公约强制要求数据必须**自描述 (Self-describing)**，也就是必须自带时间和空间坐标。\n",
        "\n",
        "4.  **实操提示 (Practical Tip)：**\n",
        "    * 你这周用 Python 处理数据时，可能会用到一个叫 `cfunits` 的包。\n",
        "    * 它的作用就是帮你自动处理单位换算（比如把“米/秒”换算成“千米/小时”），防止你自己算错。\n",
        "\n",
        "**一句话攻略：** **CF Conventions** 就是规定了 NetCDF 文件里的变量该叫什么名字（比如不能叫 `T` 要叫 `air_temperature`），目的是为了让全世界的科学家和电脑都能无障碍交流。"
      ],
      "metadata": {
        "id": "V6OOwhF8t4p3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4zj8FF6mDmu"
      },
      "source": [
        "<a id='NetCDF'></a>\n",
        "\n",
        "## **网络通用数据格式 (Network Common Data Form, NetCDF)(Network Common Data Form (NetCDF))**\n",
        "\n",
        "[NetCDF](https://www.unidata.ucar.edu/software/netcdf/) is one of the most common data format used to store climate data. NetCDF files allow the user to insert metadata in the data file by design, ensuring the data file is self-describing (amongst other properties).\n",
        "\n",
        "*NetCDF is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data.*\n",
        "\n",
        "The netCDF format, which is a type of HDF format, is attractive because it is:\n",
        "\n",
        "* Self-Describing. A netCDF file includes information about the data it contains and metadata.\n",
        "* Portable. A netCDF file can be accessed by computers with different ways of storing integers, characters, and floating-point numbers.\n",
        "* Scalable. Small subsets of large datasets in various formats may be accessed efficiently through netCDF interfaces, even from remote servers.\n",
        "* Appendable. Data may be appended to a properly structured netCDF file without copying the dataset or redefining its structure.\n",
        "* Sharable. One writer and multiple readers may simultaneously access the same netCDF file.\n",
        "* Archivable. Access to all earlier forms of netCDF data will be supported by current and future versions of the software.\n",
        "\n",
        "\n",
        "The NetCDF project is maintained by the Unidata program at the University Corporation for Atmospheric Research ([UCAR](https://www.ucar.edu)). [UCAR](https://www.ucar.edu) also manages [NCAR](https://ncar.ucar.edu), one of the first center to have developped climate models and is today one of the gold-standard on the topic.\n",
        "\n",
        "NCAR also developed [NCL](https://www.ncl.ucar.edu), an interpreted programming language designed specifically for scientific data analysis and visualization (we will not use NCL here, but the python package `ESMValTool` has build-in support for it).\n",
        "\n",
        "[NCAR](https://ncar.ucar.edu) has also developped the [climate data guide](https://climatedataguide.ucar.edu), a tool to search and access >200 data sets specific to the ocean, land and the atmosphere. (Some of these data may or may not also be found on CEDA.)\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**这一块介绍的是本次课程最核心的文件格式——NetCDF。这是气候数据分析的“通用货币”，你之后所有的代码操作基本都是围绕它展开的。**\n",
        "\n",
        "**一句话攻略：** **NetCDF (.nc)** 就是一种自带说明书、不挑电脑、支持只读一小部分内容的超级文件格式。你做 Practical 时，看到 `.nc` 文件，就应该感到安心，因为它最容易处理。\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "<a id='NetCDF'></a>\n",
        "\n",
        "**网络通用数据格式 (Network Common Data Form, NetCDF)**\n",
        "\n",
        "[**NetCDF (Network Common Data Form)**](https://www.unidata.ucar.edu/software/netcdf/) 是用于存储**气候数据 (Climate Data)** 最常见的数据格式之一。**NetCDF 文件 (NetCDF files)** 允许用户通过设计将**元数据 (Metadata)** 插入数据文件中，从而确保数据文件是**自描述的 (Self-describing)**（以及其他特性）。\n",
        "\n",
        "*NetCDF 是一组软件库和机器无关的数据格式，支持**面向数组的科学数据 (Array-oriented scientific data)** 的创建、访问和共享。*\n",
        "\n",
        "**NetCDF 格式 (NetCDF format)** 是一种 **分层数据格式 (HDF format)** 的类型，它之所以具有吸引力是因为它是：\n",
        "\n",
        "* **自描述的 (Self-Describing)**：一个 **NetCDF** 文件包含关于它所包含的数据的信息以及**元数据 (Metadata)**。\n",
        "* **可移植的 (Portable)**：**NetCDF** 文件可以被具有不同整数、字符和浮点数存储方式的计算机访问。\n",
        "* **可扩展的 (Scalable)**：可以通过 **NetCDF** 接口高效地访问各种格式的大型数据集的小型子集，即使是从远程服务器访问。\n",
        "* **可追加的 (Appendable)**：数据可以被追加到结构正确的 **NetCDF** 文件中，而无需复制数据集或重新定义其结构。\n",
        "* **可共享的 (Sharable)**：一个写入者和多个读取者可以同时访问同一个 **NetCDF** 文件。\n",
        "* **可归档的 (Archivable)**：当前和未来版本的软件将支持访问所有早期形式的 **NetCDF** 数据。\n",
        "\n",
        "\n",
        "\n",
        "**NetCDF 项目 (NetCDF project)** 由 **大气研究大学联盟 (University Corporation for Atmospheric Research, UCAR)** 的 **Unidata 项目 (Unidata program)** 维护。[**UCAR**](https://www.ucar.edu) 还管理着 [**国家大气研究中心 (NCAR)**](https://ncar.ucar.edu)，这是最早开发**气候模型 (Climate Models)** 的中心之一，也是如今该领域的黄金标准之一。\n",
        "\n",
        "**NCAR** 还开发了 [**NCL (NCAR Command Language)**](https://www.ncl.ucar.edu)，这是一种专门为**科学数据分析 (Scientific Data Analysis)** 和**可视化 (Visualization)** 设计的解释型编程语言（我们这里不会使用 NCL，但 Python 包 `ESMValTool` 内置了对它的支持）。\n",
        "\n",
        "[**NCAR**](https://ncar.ucar.edu) 还开发了 [**气候数据指南 (Climate Data Guide)**](https://climatedataguide.ucar.edu)，这是一个用于搜索和访问超过 200 个针对海洋、陆地和大气的数据集的工具。（其中一些数据可能在 **CEDA** 上也能找到，也可能找不到。）\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这是气候界的“万能容器”）**\n",
        "\n",
        "这一段介绍了你这门课最重要的工具。如果把气候数据比作快递，那 **NetCDF** 就是那个最标准的快递箱。\n",
        "\n",
        "1.  **NetCDF 是什么？**\n",
        "    * 全称是 **Network Common Data Form (网络通用数据格式)**。\n",
        "    * 它是气候科学界用来存数据的**标准容器**。\n",
        "\n",
        "2.  **为什么它这么牛？（六大特性，Quiz 考点）**\n",
        "    * **自描述的 (Self-Describing) [最重要]：** 就像一个快递箱，外面贴了详细的运单（**Metadata 元数据**），写清楚了里面装的是什么（温度还是降水）、单位是什么、时间地点是哪里。你不需要打开箱子就能知道怎么处理它。\n",
        "    * **可移植的 (Portable)：** 不管你是用 Windows、Mac 还是超级计算机，也不管你是 32 位还是 64 位系统，这个文件都能打开，不会出现乱码。\n",
        "    * **可扩展的 (Scalable)：** 假设这个文件有 100GB，你只想要其中一小块数据。NetCDF 允许你只读取那一小块，不需要把 100GB 全加载到内存里（不然电脑早崩了）。\n",
        "    * **可追加的 (Appendable)：** 你可以往文件屁股后面加新数据（比如有了新的一年的数据），而不需要把整个文件重写一遍。\n",
        "\n",
        "3.  **谁在维护它？**\n",
        "    * **UCAR** 和 **NCAR**：这俩是美国大气研究的老大。\n",
        "    * 只要知道它们是权威机构，它们开发的东西（包括 NetCDF 和 NCL）是行业标准就行了。\n",
        "\n"
      ],
      "metadata": {
        "id": "m8HSrpTCuXRO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9Fo61wDmDmu"
      },
      "source": [
        "## **读取、创建或操作 NetCDF 文件的有用工具 (Useful tools to read, create or manipulate NetCDF files)**\n",
        "\n",
        "Because they are so common, various tools have been developped to effectively  manipulate NetCDF files. We will here only discuss a few of them, notably the `netCDF4` python interface, and the `nco` routines which work on the command-line.\n",
        "\n",
        "---\n",
        "* ### Python support for NetCDF\n",
        "---\n",
        "\n",
        "`netCDF4`: As expected, Unidata provides an [API for its NetCDF C library](http://unidata.github.io/netcdf4-python/). This can be installed with `pip` or `conda`, via the conda-forge: `conda install -c conda-forge netCDF4`.\n",
        "  It can read/write files in any of the NetCDF formats (NETCDF3_CLASSIC, NETCDF3_64BIT_OFFSET, NETCDF3_64BIT_DATA, NETCDF4_CLASSIC, and NETCDF4). The latest NETCDF4 format create files readable by HDF5 clients as well.\n",
        "  \n",
        "  Detailed instructions on the use of `netCDF4` can be found [here](http://unidata.github.io/netcdf4-python/). We are here only going to demonstrate a few features.\n",
        "\n",
        "Before looking at the internal structure of a file, let's create an empty NetCDF file using python into a new `output/` directory. To do this, we need to import the `Dataset` constructor from `netCDF4`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**一句话攻略：** 这里的代码演示了如何用 Python 的 **`netCDF4` 库** 创建一个空的 `.nc` 文件。你只需要记住 `Dataset('文件名', 'w')` 是创建/写入，而 `Dataset('文件名', 'r')` 是读取。\n",
        "\n",
        "**这一块正式进入代码实操阶段了。**\n",
        "\n",
        "**刚才讲了那么多理论（NetCDF 是什么），现在教授教你怎么用 Python 代码来创建一个 NetCDF 文件。图片里的代码就是在演示这个过程。**\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "**读取、创建或操作 NetCDF 文件的有用工具**\n",
        "\n",
        "因为它们如此常见，人们开发了各种工具来有效地操作 **NetCDF 文件 (NetCDF files)**。我们这里只讨论其中的几个，特别是 `netCDF4` **Python 接口 (Python interface)**，以及在命令行上工作的 `nco` **例程 (Routines)**。\n",
        "\n",
        "---\n",
        "* ### 对 NetCDF 的 Python 支持 (Python support for NetCDF)\n",
        "---\n",
        "\n",
        "`netCDF4`：正如预期的那样，Unidata 为其 NetCDF C **库 (Library)** 提供了一个 [**API (应用程序接口)**](http://unidata.github.io/netcdf4-python/)。这可以通过 `pip` 或 `conda` 安装，通过 conda-forge 渠道：`conda install -c conda-forge netCDF4`。\n",
        "  它可以读取/写入任何 **NetCDF 格式 (NetCDF formats)**（NETCDF3_CLASSIC, NETCDF3_64BIT_OFFSET, NETCDF3_64BIT_DATA, NETCDF4_CLASSIC, 和 NETCDF4）。最新的 NETCDF4 格式创建的文件也可以被 **HDF5 客户端 (HDF5 clients)** 读取。\n",
        "  \n",
        "  关于 `netCDF4` 使用的详细说明可以在 [这里](http://unidata.github.io/netcdf4-python/) 找到。我们这里只演示几个功能。\n",
        "\n",
        "在查看文件的内部结构之前，让我们先使用 python 在一个新的 `output/` **目录 (Directory)** 中创建一个空的 **NetCDF 文件 (NetCDF file)**。为此，我们需要从 `netCDF4` 导入 `Dataset` **构造函数 (Constructor)**。\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（开始写代码了！）**\n",
        "\n",
        "这一段结合你提供的图片，其实就是在做一件事：**教你如何用 Python “无中生有”变出一个 .nc 文件。**\n",
        "\n",
        "1.  **准备工作（建文件夹）：**\n",
        "    * 图片里第一段代码 (`import os...`) 是在检查你电脑里有没有一个叫 `output` 的文件夹。\n",
        "    * 如果没有，就自动创建一个。这是为了把生成的数据文件归类整理，不要乱放。\n",
        "\n",
        "2.  **核心动作（创建文件）：**\n",
        "    * 图片里第二段代码是重点：`Dataset(\"output/test.nc\", \"w\", format=\"NETCDF4\")`。\n",
        "    * **`Dataset`**：这是 `netCDF4` 库里的一个核心功能，你可以把它理解为“打开文件”的命令。\n",
        "    * **`\"w\"` (Write Mode)**：意思是**“写入模式”**。如果文件不存在，就创建它；如果文件存在，就清空重写。**注意：** 以后你要读取数据时，这里要改成 `\"r\"` (Read Mode)。\n",
        "    * **`format=\"NETCDF4\"`**：指定生成那个最先进的 NETCDF4 格式。\n",
        "\n",
        "3.  **为什么要做这一步？**\n",
        "    * 虽然你今天的任务主要是“下载”和“读取”现成的数据（ERA5/UKCP18），但教授想让你先学会怎么**自己造一个**，这样你才能理解这个文件的内部结构（比如里面怎么分层、怎么存变量）。\n"
      ],
      "metadata": {
        "id": "oqde6KwRu95L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBhgS9Y8mDmw"
      },
      "source": [
        "### **关于远程数据访问的附注(A sidenote on remote data access)**\n",
        "\n",
        "Note that `Dataset` could also read a remote [OPeNDAP](https://www.opendap.org)-hosted dataset over http if a URL is provided instead of a filename. If you can access a file remotely, it may be best to do so, as it unburdens you from the cost of storage! However, you should also ensure that users will have continous access to the file for the duration of the project, and maybe later also if archiving of the results is required.\n",
        "\n",
        "[OPeNDAP](https://www.opendap.org) stands for Open-source Project for a Network Data Protocol and is a software for remote data retrieval, commonly used in Earth science. It allows one to access data via the internet, allowing one to retrieve data when needed, without having to necessarily download all the data locally, what facilitates real-time analysis.\n",
        "\n",
        "Other commonly encountered remote-access protocols are [THREDDS](https://www.unidata.ucar.edu/software/tds/current/) and [ERDDAP](https://upwell.pfeg.noaa.gov/erddap/index.html).\n",
        "\n",
        "[THREDDS](https://www.unidata.ucar.edu/software/tds/current/) stands for Thematic Real-time Environmental Distributed Data Services. The THREDDS Data Server ([TDS](https://docs.unidata.ucar.edu/tds/current/userguide/index.html)) is another Unidata service that provides remote access to real-time or archived datasets. It is a web server that provides metadata and data access for scientific datasets, using a variety of remote data access protocols, including OPeNDAP. The THREDDS data catalogue of meteorological data is available [here](https://thredds.ucar.edu/thredds/catalog.html).\n",
        "\n",
        "[ERDDAP](https://upwell.pfeg.noaa.gov/erddap/index.html) is also a data server by the National Oceanic and Atmospheric Administration (NOAA, USA) that acts as a middleman between the user and various other data servers. You can see a [list of datasets available through ERDDAP here](https://upwell.pfeg.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000). ERDDAP provides a consistent way to download subsets of gridded and tabular data in common file formats. ERDDAP is  a  tool that facilitates user interactions so the user doesnt have to know about OPeNDAP or other remote access protocols (such as OPeNDAP).\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**这是关于远程数据访问 (Remote Data Access) 的部分。**\n",
        "\n",
        "**这一块是在教你一种“不下载也能用数据”的高级技巧。在处理动辄几百 GB 的气候数据时，这个技巧非常救命。**\n",
        "\n",
        "----\n",
        "\n",
        "**Part 1：翻译**\n",
        "\n",
        "**关于远程数据访问的附注**\n",
        "\n",
        "请注意，如果提供的是 URL 而不是文件名，`Dataset` 也可以通过 http 读取远程托管在 [**OPeNDAP**](https://www.opendap.org) 上的数据集。如果你可以远程访问文件，这样做可能是最好的，因为它可以减轻你的存储成本负担！ 但是，你应该确保用户在项目持续期间能够持续访问该文件，如果需要归档结果，可能以后也需要能访问。\n",
        "\n",
        "[**OPeNDAP**](https://www.opendap.org) 代表 **网络数据协议开源项目 (Open-source Project for a Network Data Protocol)**，是一种用于**远程数据检索 (remote data retrieval)** 的软件，常用于地球科学。它允许人们通过互联网访问数据，允许人们在需要时检索数据，而不必在本地下载所有数据，这促进了**实时分析 (real-time analysis)**。\n",
        "\n",
        "\n",
        "\n",
        "其他经常遇到的**远程访问协议 (remote-access protocols)** 是 [**THREDDS**](https://www.unidata.ucar.edu/software/tds/current/) 和 [**ERDDAP**](https://upwell.pfeg.noaa.gov/erddap/index.html)。\n",
        "\n",
        "[**THREDDS**](https://www.unidata.ucar.edu/software/tds/current/) 代表 **专题实时环境分布式数据服务 (Thematic Real-time Environmental Distributed Data Services)**。**THREDDS 数据服务器 (THREDDS Data Server, TDS)** ([TDS](https://docs.unidata.ucar.edu/tds/current/userguide/index.html)) 是另一项 Unidata 服务，提供对实时或归档数据集的远程访问。它是一个网络服务器，使用各种远程数据访问协议（包括 OPeNDAP）为科学数据集提供**元数据 (metadata)** 和数据访问。气象数据的 THREDDS 数据目录可[在此处](https://thredds.ucar.edu/thredds/catalog.html)获得。\n",
        "\n",
        "[**ERDDAP**](https://upwell.pfeg.noaa.gov/erddap/index.html) 也是一个由 **美国国家海洋和大气管理局 (NOAA, USA)** 提供的数据服务器，它充当用户与各种其他数据服务器之间的“中间人” (middleman)。你可以在[这里看到通过 ERDDAP 可用的数据集列表](https://upwell.pfeg.noaa.gov/erddap/info/index.html?page=1&itemsPerPage=1000)。ERDDAP 提供了一种一致的方式来下载常见文件格式的网格化和表格化数据的**子集 (subsets)**。ERDDAP 是一个促进用户交互的工具，因此用户不必了解 OPeNDAP 或其他远程访问协议（如 OPeNDAP）。\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这是数据的“流媒体播放”）**\n",
        "\n",
        "这一段介绍了一种省硬盘的操作。\n",
        "\n",
        "1.  **OPeNDAP 是什么？**\n",
        "    * 把它想象成数据的 **Netflix (流媒体)**。\n",
        "    * 传统方法是：把 10GB 的电影（数据）下载到硬盘 -> 打开看。\n",
        "    * OPeNDAP 的方法是：直接在线看，**随看随加载**，不占硬盘空间。\n",
        "\n",
        "2.  **怎么用？**\n",
        "    * 记得刚才那个 `Dataset('文件名', 'r')` 吗？\n",
        "    * 如果你把“文件名”换成一个“OPeNDAP 的网址 (URL)”，Python 就能直接通过网线读取远程服务器上的数据，操作起来和读本地文件一模一样。\n",
        "\n",
        "3.  **THREDDS 和 ERDDAP 是什么？**\n",
        "    * 它们就像是 **“视频网站”**（比如 YouTube 或 B站）。\n",
        "    * 它们是服务器软件，专门用来搭建网站，把科学家的数据挂到网上，通过 OPeNDAP 协议让别人来“流式播放”。\n",
        "\n",
        "4.  **优缺点（Practical 提示）：**\n",
        "    * **优点：** 省硬盘！特别是气候模型数据动不动就几 TB，你不可能全下载下来。\n",
        "    * **缺点：** **太依赖网速**。如果学校网断了，或者对方服务器崩了，你的代码就跑不通了。所以这次 Practical 这种小作业，教授建议还是下载下来（Download）比较稳妥，但如果是做大项目，就要考虑这种远程读取的方法。\n",
        "\n",
        "**一句话攻略：** **OPeNDAP** 让你能像看在线视频一样“在线读取”数据，不用下载。这周作业虽然未必用得上，但以后处理超大数据时它是神器。"
      ],
      "metadata": {
        "id": "jVTyGb3-vXL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***NetCDF（文件格式）**"
      ],
      "metadata": {
        "id": "AwVajUa4TN4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***代码部分：创建与读取 NetCDF 文件 (Creating and Reading NetCDF files)**\n",
        "\n",
        "**分为三个步骤：**\n",
        "\n",
        "**1. 建文件夹，**\n",
        "\n",
        "**2. 创建文件，**\n",
        "\n",
        "**3. 读取文件。**\n",
        "\n",
        "**这是做 Practical 时最基础的操作流程。**"
      ],
      "metadata": {
        "id": "ixsTZssKwst8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1：创建一个存放输出文件的目录**"
      ],
      "metadata": {
        "id": "J5iDIEofxM8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OfvejT-mDmv"
      },
      "outputs": [],
      "source": [
        "# lets first create a new directory called 'output' where we will save this file\n",
        "# 首先创建一个名为 'output' 的新目录，我们将把这个文件保存在那里\n",
        "import os\n",
        "mypath = 'output'\n",
        "\n",
        "# for more complicated path, we could use mypath = os.path.join('dir','other-dir').\n",
        "# 对于更复杂的路径，可以使用 os.path.join('dir','other-dir')\n",
        "if not os.path.isdir(mypath): # if the directory does not exist, 如果目录不存在，\n",
        "    os.makedirs(mypath) # let's create it"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2：创建一个空的 NetCDF 文件**"
      ],
      "metadata": {
        "id": "lwsTOgVhxmrK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW5oMrlnmDmv",
        "outputId": "58bfb049-144d-4b38-f3c4-387cb54eea15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NETCDF4\n"
          ]
        }
      ],
      "source": [
        "# 创建 NetCDF 文件： （Now, lets create the NetCDF file:）\n",
        "# 导入 Dataset 构造函数 （import the Dataset constructor）\n",
        "from netCDF4 import Dataset\n",
        "\n",
        "# 创建一个 .nc 文件，使用默认的 NETCDF4 格式。 （create a .nc file, using the default NETCDF4 format.）\n",
        "# 'w' 选项代表 'write'（写入）。（the 'w' option is for 'write'.）\n",
        "# 如果文件已经存在，可以使用 'r' 选项代表 'read'（读取）。（one could use a 'r' option for 'read' if the file already exists. ）\n",
        "rootgrp = Dataset(\"output/test.nc\", \"w\", format=\"NETCDF4\")\n",
        "\n",
        "# 可以使用 \"data_model\" 属性来显示文件处于什么格式 （one can use the \"data_model\" attribute to show what format the file is in）\n",
        "print(rootgrp.data_model)\n",
        "\n",
        "# 最后，可以使用 Dataset.close 方法关闭文件。Finally, one can close the file using the Dataset.close method.\n",
        "rootgrp.close()\n",
        "\n",
        "# [注意：如果你运行这段代码两次，你可能会得到一个错误。 [note: you may get an error if you run this twice.\n",
        "# 这是因为 output/test.nc 已经存在了。 This is because the output/test.nc already exist.\n",
        "# 删除 test.nc 后可以再次运行，或者修改代码以检查文件是否已存在。] Delete test.nc and this will run again, or change this code to check for prior existance.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJFuhnWNmDmw"
      },
      "source": [
        "Let's look into the `output` directory to see if a `.nc` file now exists. There are [many ways to do this](https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory) (e.g. `os`, `glob`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3：检查文件是否存在并读取它**"
      ],
      "metadata": {
        "id": "A5Z41Z4Qys6k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erIov8XkmDmw",
        "outputId": "52ce0603-08f5-4226-8df1-7c339a5c2409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['output/ERAtest_subarea_1deg1deg.nc', 'output/ERA5test.nc', 'output/test.nc']\n"
          ]
        }
      ],
      "source": [
        "# 使用 'glob'，因为它能进行模式匹配  using 'glob' as it does pattern matching\n",
        "import glob\n",
        "ncfiles = glob.glob(\"output/*.nc\")\n",
        "print(ncfiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKjZbQgEmDmx"
      },
      "source": [
        "One can also use `Dataset` to read existing files. To do so, simply repace the `\"w\"` option with the `\"r\"` option (for read). One could also use the `\"a\"` option to append new data to an existing file.\n",
        "\n",
        "**也可以使用 Dataset 来读取现有文件。要这样做，只需将 \"w\" 选项替换为 \"r\" 选项（用于读取 read）。**\n",
        "\n",
        "**还可以使用 \"a\" 选项将新数据追加 (append) 到现有文件中。**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D28RGeiSmDmx",
        "outputId": "0885f4a6-294e-4694-ddc4-e188b5034fa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'netCDF4._netCDF4.Dataset'>\n",
            "root group (NETCDF4 data model, file format HDF5):\n",
            "    dimensions(sizes): \n",
            "    variables(dimensions): \n",
            "    groups: \n"
          ]
        }
      ],
      "source": [
        "# 让我们尝试打开我们刚刚创建的 'test.nc' 文件  lets try to open the 'test.nc' file we just created\n",
        "ncdata = Dataset(\"output/test.nc\", \"r\")\n",
        "print(ncdata)\n",
        "# 完成后别忘了关闭文件 don't forget to close the file when done\n",
        "ncdata.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shBkIYWhmDmx"
      },
      "source": [
        "We see that even if we added no information to 'test.nc' when we created it, the file already contains structure.\n",
        "\n",
        "Another convenient way to look at the structure of netCDF file on the command line is to use `ncdump -h <filename.nc>`. `ncdump` is part of the netCDF C library that needs to be installed for anything related to netCDF to work (it comes 'for free'!)\n",
        "\n",
        "In particular, there is a `group`, and that 'group' seems to posses some placeholders for `dimensions` and `variables` information. We also see that variables have dimensions, and that dimensions have sizes.\n",
        "\n",
        "- **Groups**: `groups` are the NetCDF file analogue to a directory file system (only implemented in NETCDF4 - so not all NetCDF files will have groups). That is, a NetCDF file can contain different groups, which are like containers for `variables`, `dimensions` and other `attributes`. Groups could also contain other groups and the `root group` created by `Dataset` is like the root directory.\n",
        "\n",
        "![netcdf](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/hdf5-example-data-structure.jpg?raw=1)\n",
        "\n",
        "One can create new groups using `Dataset.createGroup`. Lets create two new groups (\"forecast\" and \"analyses\") within 'test.nc'. And then let's create two other subgroups within the \"foecast\" group (i.e. nested groups, just like subdirectories).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**这部分讲的是 NetCDF 文件的内部装修。**\n",
        "\n",
        "刚才创建了一个空房子（.nc 文件），现在教你如何给这个房子打隔断、分房间（这就是 Groups/组的概念）。\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "我们看到，即使我们在创建 'test.nc' 时没有添加任何信息，该文件也已经包含了结构。\n",
        "\n",
        "在命令行上查看 netCDF 文件结构的另一种便捷方法是使用 `ncdump -h <filename.nc>`。`ncdump` 是 netCDF C **库 (library)** 的一部分，为了让任何与 netCDF 相关的东西工作，都需要安装它（它是“免费赠送”的！）。\n",
        "\n",
        "特别是，存在一个 `group`（组），并且该 'group' 似乎拥有一些用于 `dimensions`（维度）和 `variables`（变量）信息的占位符。我们还看到变量具有维度，并且维度具有大小。\n",
        "\n",
        "- **组 (Groups)**：`groups` 是 NetCDF 文件中对应于**目录文件系统 (directory file system)** 的类似物（仅在 NETCDF4 中实现 - 因此并非所有 NetCDF 文件都有组）。也就是说，一个 NetCDF 文件可以包含不同的组，它们就像是 `variables`（变量）、`dimensions`（维度）和其他 `attributes`（属性）的**容器 (containers)**。组也可以包含其他组，由 `Dataset` 创建的 `root group`（根组）就像是**根目录 (root directory)**。\n",
        "\n",
        "![netcdf](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/hdf5-example-data-structure.jpg?raw=1)\n",
        "\n",
        "可以使用 `Dataset.createGroup` 创建新组。让我们在 'test.nc' 中创建两个新组（\"forecast\" 和 \"analyses\"）。然后让我们在 \"forecast\" 组中创建另外两个子组（即嵌套组，就像子目录一样）。\n",
        "\n",
        "**Part 2：大白话总结（这就是电脑里的“文件夹”）**\n",
        "\n",
        "这一块为了防止你把所有数据一股脑扔进文件里太乱，教你如何整理文件结构。\n",
        "\n",
        "1. **Groups (组) = 文件夹 (Folders)：**\n",
        "    * 想象你的电脑硬盘。你不会把几千张照片全扔在桌面上，你会建文件夹：“2023年照片”、“2024年照片”。\n",
        "    * NetCDF 文件内部也是一样。你可以建一个叫 `forecasts`（预测）的“文件夹”，再建一个叫 `analyses`（分析）的“文件夹”，把相关的数据分类放进去。\n",
        "\n",
        "2. **Root Group (根组) = C盘/根目录：**\n",
        "    * 当你刚打开文件时，你就在“根目录”下。\n",
        "    * 代码里的 `createGroup(\"/forecasts/model1\")` 就好比你在电脑上新建文件夹路径 `/Users/Name/forecasts/model1` 一样直观。\n",
        "\n",
        "3. **神器 `ncdump` (Quiz 考点)：**\n",
        "    * 教授顺带提了一个非常重要的工具叫 **`ncdump`**。\n",
        "    * 如果你不想写 Python 代码，只是想快速看一眼这个 `.nc` 文件里有什么（比如“这里面有温度数据吗？”），你就在命令行敲 `ncdump -h 文件名.nc`。\n",
        "    * **-h** 的意思是 **header (只看头部信息)**，不看具体数据（不然屏幕会被几亿个数字刷屏）。\n",
        "\n",
        "> **一句话攻略：** **Group (组)** 就是 NetCDF 文件肚子里的**文件夹**，用来把变量分门别类整理好，防止乱套。"
      ],
      "metadata": {
        "id": "lBheiEnvzt62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4：Groups: 给刚刚创建的NetCDF的空文件（Step 2）创建 Groups/组**"
      ],
      "metadata": {
        "id": "mINVEqeW1KI0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9M426Fg5mDmx",
        "outputId": "c3078353-833e-475f-89f5-b2251977857f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'forecasts': <class 'netCDF4._netCDF4.Group'>\n",
            "group /forecasts:\n",
            "    dimensions(sizes): \n",
            "    variables(dimensions): \n",
            "    groups: , 'analyses': <class 'netCDF4._netCDF4.Group'>\n",
            "group /analyses:\n",
            "    dimensions(sizes): \n",
            "    variables(dimensions): \n",
            "    groups: }\n",
            "----now see the subgroups in forecasts: \n",
            "{'forecasts': <class 'netCDF4._netCDF4.Group'>\n",
            "group /forecasts:\n",
            "    dimensions(sizes): \n",
            "    variables(dimensions): \n",
            "    groups: model1, model2, 'analyses': <class 'netCDF4._netCDF4.Group'>\n",
            "group /analyses:\n",
            "    dimensions(sizes): \n",
            "    variables(dimensions): \n",
            "    groups: }\n"
          ]
        }
      ],
      "source": [
        "# 向现有文件追加 \"a\" 新信息（注意：这里用的 'a' 代表 append，如果用 'w' 会把之前的文件清空！） we are going to append \"a\" new information to the existing file\n",
        "rootgrp = Dataset(\"output/test.nc\", \"a\")\n",
        "\n",
        "# 创建 2 个新组（相当于在根目录下建两个文件夹） lets create 2 new groups\n",
        "fcstgrp = rootgrp.createGroup(\"forecasts\")\n",
        "analgrp = rootgrp.createGroup(\"analyses\")\n",
        "\n",
        "# show the results below\n",
        "print(rootgrp.groups)\n",
        "\n",
        "# 在 forecasts 组里再创建 2 个子组（文件夹里的子文件夹） now let's create 2 new subgroups in forecasts\n",
        "# 如果 'forecasts' 还没存在，它会被自动创建（类似 mkdir -p） if 'forecasts' did not already exist, it would have been created, analogous to the unix 'mkdir -p' command.\n",
        "fcstgrp1 = rootgrp.createGroup(\"/forecasts/model1\")\n",
        "fcstgrp2 = rootgrp.createGroup(\"/forecasts/model2\")\n",
        "\n",
        "# 查看 forecasts 中的子组\n",
        "print('----now see the subgroups in forecasts: ')\n",
        "print(rootgrp.groups)\n",
        "\n",
        "# 记得最后要关闭文件，养成好习惯\n",
        "# rootgrp.close()\n",
        "# (原本教授没写这行，但实际写代码时最好加上，防止数据丢失)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5：Dimensions: 定义维度 (Defining Dimensions)**"
      ],
      "metadata": {
        "id": "F2jphsrb2pOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**在 Group（组）里面定义一套“骨架”（Dimensions），供 Variables（变量）使用。**\n",
        "\n",
        "**在放入数据之前，我们必须先定义数据的“骨架”（Dimensions）**\n",
        "\n"
      ],
      "metadata": {
        "id": "NWb6DGLx22IL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xS7LD5MmDmx"
      },
      "source": [
        "- **Dimensions**: In NetCDF files, we define the sizes of all the variables using `dimensions`. Dimensions are created using the `Dataset.createDimension` method. We must provide a name and a value, but one can create dimensions that are finite (fixed value), or unlimited. Unlimited dimensions are useful as it can be appended to. To create unlimited dimensions, we can set its value to `None`, or `0`.  \n",
        "\n",
        "If needed, one can rename dimensions with `Datatset.renameDimension`.\n",
        "\n",
        "Here is an example, where we create 4 dimensions, level, time, lat and lon. Level and time are unlimited (useful so we can append new data), while lat and lon are fixed (i.e. the spatial grid used does not change). (NB: prior to NETCDF4, only one unlimited dimension could be set.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "- **维度 (Dimensions)**：在 NetCDF 文件中，我们使用 `dimensions`（维度）来定义所有变量的大小。维度是使用 `Dataset.createDimension` 方法创建的。我们必须提供一个名称和一个值，但可以创建有限（固定值）或无限的维度。无限维度很有用，因为可以向其追加数据。要创建无限维度，我们可以将其值设置为 `None` 或 `0`。\n",
        "\n",
        "如果需要，可以使用 `Datatset.renameDimension` 重命名维度。\n",
        "\n",
        "这里有一个例子，我们创建了 4 个维度：level（层级）、time（时间）、lat（纬度）和 lon（经度）。Level 和 time 是无限的（这很有用，以便我们可以追加新数据），而 lat 和 lon 是固定的（即所使用的空间网格不会改变）。（注意：在 NETCDF4 之前，只能设置一个无限维度。）\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这就是数据的“骨架”）**\n",
        "\n",
        "这一块专门讲怎么定义数据的 **“形状”**。\n",
        "\n",
        "1.  **Dimension (维度) 是什么？**\n",
        "    * 它是数据的**尺子**。你想存一堆温度数据，得先告诉电脑这堆数据是按什么形状排的：是按时间排？还是按地图上的格子排？\n",
        "\n",
        "2.  **最重要的两种类型（Unlimited vs Fixed）：**\n",
        "    * **固定维度 (Fixed/Finite)：** 比如**地图的大小**（Lat/Lon）。你画好的地图格子通常不会变，所以这把尺子的长度是固定的（比如 73 行 144 列）。\n",
        "    * **无限维度 (Unlimited)：** 比如**时间**（Time）。时间是会一直往前走的。为了以后能往文件里接着写明年的数据，我们把这把尺子的长度设为 `None`（无限/不固定），这样以后就可以随时往屁股后面加新数据。\n",
        "\n",
        "3.  **技术小知识（NB: 注意事项）：**\n",
        "    * 教授顺嘴提了一句历史：以前的老格式（NETCDF3）只能有一个无限维度（通常给时间），但现在的新格式（NETCDF4）允许有好几个无限维度（比如时间和高度都可以是无限的）。\n",
        "\n",
        "> **一句话攻略：** 这一步就是给数据定规矩：地图大小定死（Fixed），时间长短不设限（Unlimited/None），方便以后加新数据。"
      ],
      "metadata": {
        "id": "G6IlW5Zu3y0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**下面这段代码展示了如何使用 `createDimension` 来定义四个维度：**"
      ],
      "metadata": {
        "id": "UpHZa1F73Mue"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcaO5jCCmDmy",
        "outputId": "35dfe485-4930-44e8-8798-073893b8c2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensions are stored in a python dictionary:\n",
            "{'level': <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'level', size = 0, 'time': <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'time', size = 0, 'lat': <class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 73, 'lon': <class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 144}\n"
          ]
        }
      ],
      "source": [
        "# 1. 创建“无限”维度 (Unlimited Dimensions)\n",
        "# 这里的 None 表示大小不固定，可以无限增加。\n",
        "# 通常用于 'time' (时间) 和 'level' (高度层)，因为未来可能会有新的时间点数据加进来。\n",
        "level = rootgrp.createDimension(\"level\", None)\n",
        "time = rootgrp.createDimension(\"time\", None)\n",
        "\n",
        "# 2. 创建“固定”维度 (Fixed Dimensions)\n",
        "# 这里写死了 73 和 144，表示这个网格是固定的。\n",
        "# lat (纬度) 有 73 个格子，lon (经度) 有 144 个格子。\n",
        "lat = rootgrp.createDimension(\"lat\", 73)\n",
        "lon = rootgrp.createDimension(\"lon\", 144)\n",
        "\n",
        "# 维度存储在一个 python 字典中：\n",
        "print(\"Dimensions are stored in a python dictionary:\")\n",
        "print(rootgrp.dimensions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6：创建Variable（往Dimension里填入Variable）**"
      ],
      "metadata": {
        "id": "TMfHitGf5yBI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPsw-N6-mDmy"
      },
      "source": [
        "- **Variables**: NetCDF variables are a bit like python arrays from `numpy`. Variables can have any number of dimensions (i.e. 1D, 2D, 3D, 4D, ...). One can create a new variable with `Dataset.createVariable`, where we must provide a name and a data type for the variable. The dimensions of a variables are provided as a 'tuple'. One can also create a 0D variable (i.e. a scalar variable), in which case we simply leave out the dimension information.\n",
        "\n",
        "Datatype must be one of the following: 'f8','f4','i8','i4','i2','i1','u8','u4','u2','u1','S1'. This is similar to 'numpy'; 'f' is for floating point, 'i' for signed-integer, 'u' for unsigned-integers, 'S' for single character string; 8 is for 64-bit, 4 for 32-bit, 2 for 16-bit and 1 for 8-bit. ('i8' and 'u8' are only available on NETCDF4).\n",
        "\n",
        "Variable names can be changed with `Dataset.renameVariable`.\n",
        "\n",
        "Note that the dimensions themselves will eventually have to hold some values, so they are defined as 'coordinate variables', a special type of variables.\n",
        "\n",
        "Let's define coordinate variables (dimensions) and also a new variable ('temp') in 'test.nc'."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "- **变量 (Variables)**：NetCDF **变量 (Variables)** 有点像来自 `numpy` 的 python **数组 (arrays)**。**变量 (Variables)** 可以有任意数量的**维度 (Dimensions)**（即 1D, 2D, 3D, 4D, ...）。我们可以使用 `Dataset.createVariable` 创建一个新变量，我们需要为变量提供一个**名称 (name)** 和一个**数据类型 (data type)**。变量的**维度 (dimensions)** 作为“**元组 (tuple)**”提供。我们也可以创建一个 0D 变量（即**标量变量 (scalar variable)**），这种情况下我们只需省略维度信息。\n",
        "\n",
        "**数据类型 (Datatype)** 必须是以下之一：'f8','f4','i8','i4','i2','i1','u8','u4','u2','u1','S1'。\n",
        "\n",
        "- 这类似于 `numpy`；\n",
        "\n",
        "- 'f' 代表**浮点数 (floating point)**，\n",
        "\n",
        "- 'i' 代表**有符号整数 (signed-integer)**，\n",
        "\n",
        "- 'u' 代表**无符号整数 (unsigned-integers)**，\n",
        "\n",
        "- 'S' 代表单字符**字符串 (string)**；\n",
        "\n",
        "- 8 代表 64 位，\n",
        "\n",
        "- 4 代表 32 位，\n",
        "\n",
        "- 2 代表 16 位，\n",
        "\n",
        "- 1 代表 8 位。('i8' 和 'u8' 仅在 NETCDF4 上可用)。\n",
        "\n",
        "可以使用 `Dataset.renameVariable` 更改**变量名称 (Variable names)**。\n",
        "\n",
        "请注意，**维度 (dimensions)** 本身最终也必须包含一些值，因此它们被定义为 **‘坐标变量’ (coordinate variables)**，这是一种特殊类型的变量。\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（填肉）**\n",
        "\n",
        "这一步是真正创建“存数据的地方”。\n",
        "\n",
        "1.  **Variable (变量) 是什么？**\n",
        "    * 就是具体的**数据**。比如“温度”、“降水”、“风速”。\n",
        "    * 它们是挂在刚才定义的 **Dimensions (维度/挂衣杆)** 上的。\n",
        "    * 比如你创建一个叫 `temp` 的变量，你告诉电脑：这个变量是 4 维的（时间、高度、纬度、经度），那电脑就会给你留出相应的空间。\n",
        "\n",
        "2.  **数据类型 (Datatype) 怎么选？（Quiz 考点）**\n",
        "    * 这一串代号其实很好记：\n",
        "        * **f (Float)** = 小数（比如温度 23.5）。`f4` 是普通精度，`f8` 是高精度。\n",
        "        * **i (Integer)** = 整数（比如年份 2023）。\n",
        "    * **实用建议：** 存温度、降水这种带小数点的，一般用 **`'f4'` (32位浮点数)** 就够了，省空间。\n",
        "\n",
        "3.  **Coordinate Variables (坐标变量) 是个啥？**\n",
        "    * 这是一个这节课最容易绕晕的概念。\n",
        "    * **Dimension** 只是说“这把尺子有 10 厘米长”。\n",
        "    * **Coordinate Variable** 是在这把尺子上刻上刻度：“1cm, 2cm, 3cm...”。\n",
        "    * 所以，`lat` (纬度) 既是一个 **Dimension** (告诉 `temp` 变量地图有几行)，它自己也是一个 **Variable** (存储具体的纬度数值，如 30度, 31度, 32度)。\n",
        "\n",
        "> **一句话攻略：** 用 `createVariable` 来创建具体的**数据容器**。记得要给数据选对类型（小数用 f4），并且别忘了给刚才定义的维度也创建对应的**坐标变量**（给尺子刻上刻度）。"
      ],
      "metadata": {
        "id": "bWQ2n0uu54yV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**代码总结（把容器造好）**\n",
        "\n",
        "这一段代码是在做两件事：\n",
        "1.  **创建坐标变量 (Coordinate Variables)**：给刚才定义的维度（time, level, lat, lon）创建对应的变量，用来存具体的刻度值。\n",
        "2.  **创建数据变量 (Data Variables)**：创建一个叫 `temp` 的变量，用来存核心数据（比如温度），并告诉它要用哪几个维度。"
      ],
      "metadata": {
        "id": "HHA143Jg6kZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "这一步是把“空架子”变成“能装东西的盒子”。\n",
        "\n",
        "1. **Coordinate Variables (坐标变量) 的创建：**\n",
        "    * 代码的前四行 (`times`, `levels`...) 是在给刚才的“尺子”造刻度槽。\n",
        "    * 你必须告诉系统：`time` 这个变量，它用的维度就是 `time`。这听起来像废话，但 NetCDF 需要这样显式地绑定。\n",
        "\n",
        "2. **核心变量 `temp` 的创建：**\n",
        "    * `temp = rootgrp.createVariable(\"temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",))`\n",
        "    * 这行代码翻译过来就是：**“给我造一个叫 `temp` 的容器，它存的是小数 (f4)，它的形状是四维的（时间、高度、纬度、经度）。”**\n",
        "    * 这就是未来你存香槟区气温数据的地方。\n",
        "\n",
        "3. **在 Group 里创建变量：**\n",
        "    * 教授还顺手演示了怎么把变量塞进之前的“子文件夹”里。\n",
        "    * 只要在名字前加路径就行：`\"/forecasts/model1/temp\"`。\n",
        "\n",
        "> **一句话攻略：** `createVariable` 是这一步的核心命令。你要先给所有的维度创建对应的变量（叫坐标变量），然后再创建你真正关心的科学变量（比如 `temp`），并把它们关联起来。"
      ],
      "metadata": {
        "id": "grWltox-7j_v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpJ40EkAmDmy",
        "outputId": "06225d17-374f-4979-bc65-e8c632620223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'netCDF4._netCDF4.Variable'>\n",
            "float32 temp(time, level, lat, lon)\n",
            "unlimited dimensions: time, level\n",
            "current shape = (0, 0, 73, 144)\n",
            "filling on, default _FillValue of 9.969209968386869e+36 used\n",
            "--- model1 ---\n",
            "<class 'netCDF4._netCDF4.Group'>\n",
            "group /forecasts/model1:\n",
            "    dimensions(sizes): \n",
            "    variables(dimensions): float32 temp(time, level, lat, lon)\n",
            "    groups: \n",
            "---model1/temp---\n",
            "<class 'netCDF4._netCDF4.Variable'>\n",
            "float32 temp(time, level, lat, lon)\n",
            "path = /forecasts/model1\n",
            "unlimited dimensions: time, level\n",
            "current shape = (0, 0, 73, 144)\n",
            "filling on, default _FillValue of 9.969209968386869e+36 used\n",
            "--- python variable dictionary ---\n",
            "{'time': <class 'netCDF4._netCDF4.Variable'>\n",
            "float64 time(time)\n",
            "unlimited dimensions: time\n",
            "current shape = (0,)\n",
            "filling on, default _FillValue of 9.969209968386869e+36 used, 'level': <class 'netCDF4._netCDF4.Variable'>\n",
            "int32 level(level)\n",
            "unlimited dimensions: level\n",
            "current shape = (0,)\n",
            "filling on, default _FillValue of -2147483647 used, 'lat': <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 lat(lat)\n",
            "unlimited dimensions: \n",
            "current shape = (73,)\n",
            "filling on, default _FillValue of 9.969209968386869e+36 used, 'lon': <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 lon(lon)\n",
            "unlimited dimensions: \n",
            "current shape = (144,)\n",
            "filling on, default _FillValue of 9.969209968386869e+36 used, 'temp': <class 'netCDF4._netCDF4.Variable'>\n",
            "float32 temp(time, level, lat, lon)\n",
            "unlimited dimensions: time, level\n",
            "current shape = (0, 0, 73, 144)\n",
            "filling on, default _FillValue of 9.969209968386869e+36 used}\n"
          ]
        }
      ],
      "source": [
        "# 定义坐标变量（时间、层级、纬度、经度） （define the coordinate variables (time, level, lat, lon)）\n",
        "# 这里的 \"f8\", \"i4\", \"f4\" 就是刚才说的数据类型（浮点数、整数等）。\n",
        "# 注意 (\"time\",) 这种写法，表示它只依赖这一维。\n",
        "times = rootgrp.createVariable(\"time\",\"f8\",(\"time\",))\n",
        "levels = rootgrp.createVariable(\"level\",\"i4\",(\"level\",))\n",
        "latitudes = rootgrp.createVariable(\"lat\",\"f4\",(\"lat\",))\n",
        "longitudes = rootgrp.createVariable(\"lon\",\"f4\",(\"lon\",))\n",
        "\n",
        "# 新变量 'temp' 有 4 个维度，其中 2 个是无限的（time 和 level）（new variable 'temp' has 4 dimensions, 2 of which are unlimited (time and level）)\n",
        "temp = rootgrp.createVariable(\"temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",))\n",
        "\n",
        "# 打印 'temp' 变量的摘要  （we can print a summary of the 'temp' variable）\n",
        "print(temp)\n",
        "\n",
        "\n",
        "# 我们也可以在一个组内创建变量（例如在 forecasts/model1 中） （we can also creat a variable inside a group (e.g. in forecasts/model1)）\n",
        "# 注意路径写法 \"/forecasts/model1/temp\"\n",
        "ftemp = rootgrp.createVariable(\"/forecasts/model1/temp\",\"f4\",(\"time\",\"level\",\"lat\",\"lon\",))\n",
        "\n",
        "\n",
        "# 查询组的状态   （we can also query the status of the group）\n",
        "print('--- model1 ---')\n",
        "print(rootgrp[\"/forecasts/model1\"])\n",
        "\n",
        "# 或者直接查询组内的变量  （or the variable in the group directly）\n",
        "print('---model1/temp---')\n",
        "print(rootgrp[\"/forecasts/model1/temp\"])\n",
        "\n",
        "# Dataset 中的变量存储为一个 python 字典 （variables in the Dataset are stored as a python dictionary）\n",
        "print('--- python variable dictionary ---')\n",
        "print(rootgrp.variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7：Attributes: 给Variable或者Dimension添加/定义Attributes（属性）**"
      ],
      "metadata": {
        "id": "fThuLzCk794L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIB9PLmtmDmy"
      },
      "source": [
        "- **Attributes**: The last category of information required are 'attributes'. Attributes can be `global' or 'variable'-specific. 'Global' attributes are for the whole Dataset or the group. 'Variable' attributes only inform on a specific variable (or coordinate variable).  \n",
        "\n",
        "`Dataset.ncattrs` can be used to retrieve attributes from a NetCDF file. Alternatively, the `__dict__` attribute of a Dataset, Group or Variable will return the name/value pairs for all the attributes.\n",
        "\n",
        "Let's define some attribute to the time coordinate variable (or dimension):\n",
        "\n",
        "----\n",
        "\n",
        "**Part 1： 翻译**\n",
        "\n",
        "- **属性 (Attributes)**：所需信息的最后一个类别是“属性”。属性可以是“全局的” (`global`) 或“特定于变量的” ('variable'-specific)。“全局”属性针对整个 **数据集 (Dataset)** 或 **组 (group)**。“变量”属性仅提供关于特定 **变量 (variable)**（或坐标变量）的信息。\n",
        "\n",
        "可以使用 `Dataset.ncattrs` 从 NetCDF 文件中检索属性。或者，Dataset、Group 或 Variable 的 `__dict__` 属性将返回所有属性的名称/值对。\n",
        "\n",
        "----\n",
        "\n",
        "**Part 2：大白话总结（贴标签/说明书）**\n",
        "\n",
        "这一步是在完善数据的 **元数据 (Metadata)**，也就是我们在理论课里强调的“说明书”。\n",
        "\n",
        "1. **Global Attributes (全局属性)：**\n",
        "    * 写在 `rootgrp` 上的。\n",
        "    * 比如：这个文件是谁创建的？什么时候创建的？数据来源是哪？\n",
        "    * **作用：** 以后你（或者 Tattinger 的人）打开这个文件，一眼就能看到这些信息，不会是一头雾水。\n",
        "\n",
        "2. **Variable Attributes (变量属性)：**\n",
        "    * 写在具体变量（如 `temp`, `latitudes`）上的。\n",
        "    * **Units (单位) [最重要]：** 必须写！比如 `temp.units = \"K\"` 告诉别人这是开尔文，不是摄氏度。如果不写，别人分析时绝对会出错。\n",
        "    * **Calendar (日历)：** 时间变量通常需要这个，因为地球上有闰年、平年，不同的日历计算方式不同。\n",
        "\n",
        "3. **代码操作很简单：**\n",
        "    * 虽然概念很高级，但代码其实就是简单的赋值：`变量名.属性名 = \"内容\"`。\n",
        "\n",
        "> **一句话攻略：** 这一步是给你的数据文件**注入灵魂**。没有这些 `units` 和 `description`，你的数据就是一堆毫无意义的数字垃圾。务必记得给每个变量都加上单位！"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**为时间坐标变量（或维度）定义一些Attributes(属性)：**"
      ],
      "metadata": {
        "id": "QQBAW6kg8SD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMc4sXoFmDmz",
        "outputId": "f2247282-5b82-4f2d-86c5-9450c89c3320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- method 1: ncattrs ---\n",
            "Global attr description = Example of how to create and manipulate test.nc\n",
            "Global attr history = Created Mon Nov 22 15:15:38 2021\n",
            "Global attr source = netCDF4 python module tutorial: http://unidata.github.io/netcdf4-python/#tutorial\n",
            "--- method 2: __dict__ ---\n",
            "{'description': 'Example of how to create and manipulate test.nc', 'history': 'Created Mon Nov 22 15:15:38 2021', 'source': 'netCDF4 python module tutorial: http://unidata.github.io/netcdf4-python/#tutorial'}\n"
          ]
        }
      ],
      "source": [
        "# let's import the 'time' library, to add to our metadata\n",
        "# 导入 'time' 库，以添加到我们的元数据（Metadata）中（为了获取当前时间）\n",
        "import time\n",
        "\n",
        "# now let's add some attributes, to add to our metadata\n",
        "# 现在添加一些Attributes（属性），以添加到我们的元数据中\n",
        "\n",
        "# 1. 添加全局属性 (Global Attributes) - 描述整个文件的\n",
        "rootgrp.description = \"Example of how to create and manipulate test.nc\"\n",
        "rootgrp.history = \"Created \" + time.ctime(time.time())  # 记录创建时间\n",
        "rootgrp.source = \"netCDF4 python module tutorial: http://unidata.github.io/netcdf4-python/#tutorial\"\n",
        "\n",
        "# 2. 添加变量属性 (Variable Attributes) - 主要是单位 (units)\n",
        "# 这些单位定义非常重要，最好遵循 CF Conventions\n",
        "# Now, let's assing units to the variables/coordinate variables (these could follow CF Conventions)\n",
        "latitudes.units = \"degrees north\"\n",
        "longitudes.units = \"degrees east\"\n",
        "levels.units = \"hPa\"\n",
        "times.units = \"hours since 0001-01-01 00:00:00.0\"  # 时间的单位通常是“既然某时刻起的小时数”\n",
        "times.calendar = \"gregorian\"  # 指定日历格式（公历）\n",
        "temp.units = \"K\"  # 温度单位：开尔文\n",
        "\n",
        "\n",
        "# 现在看看我们做了什么，并展示如何检索该信息\n",
        "# now let's see what we did, and show how that information could be retrieved\n",
        "# so it could be integrated/used in a programme designed to analyze data from a NetCDF file.\n",
        "\n",
        "# 方法 1：使用 Dataset.ncattrs 获取信息\n",
        "# method 1: get information with Dataset.ncattrs\n",
        "print('--- method 1: ncattrs ---')\n",
        "for name in rootgrp.ncattrs():\n",
        "     print(\"Global attr {} = {}\".format(name, getattr(rootgrp, name)))\n",
        "\n",
        "# 方法 2：使用 rootgrp.__dict__ 获取信息作为 python 字典\n",
        "# method 2: get information as a python dictionary, with rootgrp.__dict__\n",
        "print('--- method 2: __dict__ ---')\n",
        "print(rootgrp.__dict__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 8： Adding Data（加入数据）**"
      ],
      "metadata": {
        "id": "MzUtnTPt-IAw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq0p_um6mDmz"
      },
      "source": [
        "**Adding data**: Finally, we are prepared to add data to the file...which is the point! That is simple, we can just treat the 'variables' as arrays and assign data into them."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**添加数据 (Adding data)**：最后，我们准备好向文件中添加数据了……这才是重点！这很简单，我们可以直接将 'variables'（变量）视为 **数组 (arrays)** 并将数据赋值给它们。"
      ],
      "metadata": {
        "id": "3vC16b5O-OY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u9FYmGPmDmz",
        "outputId": "dfd96cb1-67af-48b2-a2c9-800d48f0889c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "latitudes =\n",
            "[-90.  -87.5 -85.  -82.5 -80.  -77.5 -75.  -72.5 -70.  -67.5 -65.  -62.5\n",
            " -60.  -57.5 -55.  -52.5 -50.  -47.5 -45.  -42.5 -40.  -37.5 -35.  -32.5\n",
            " -30.  -27.5 -25.  -22.5 -20.  -17.5 -15.  -12.5 -10.   -7.5  -5.   -2.5\n",
            "   0.    2.5   5.    7.5  10.   12.5  15.   17.5  20.   22.5  25.   27.5\n",
            "  30.   32.5  35.   37.5  40.   42.5  45.   47.5  50.   52.5  55.   57.5\n",
            "  60.   62.5  65.   67.5  70.   72.5  75.   77.5  80.   82.5  85.   87.5\n",
            "  90. ]\n",
            "longitudes =\n",
            "[-180.  -177.5 -175.  -172.5 -170.  -167.5 -165.  -162.5 -160.  -157.5\n",
            " -155.  -152.5 -150.  -147.5 -145.  -142.5 -140.  -137.5 -135.  -132.5\n",
            " -130.  -127.5 -125.  -122.5 -120.  -117.5 -115.  -112.5 -110.  -107.5\n",
            " -105.  -102.5 -100.   -97.5  -95.   -92.5  -90.   -87.5  -85.   -82.5\n",
            "  -80.   -77.5  -75.   -72.5  -70.   -67.5  -65.   -62.5  -60.   -57.5\n",
            "  -55.   -52.5  -50.   -47.5  -45.   -42.5  -40.   -37.5  -35.   -32.5\n",
            "  -30.   -27.5  -25.   -22.5  -20.   -17.5  -15.   -12.5  -10.    -7.5\n",
            "   -5.    -2.5    0.     2.5    5.     7.5   10.    12.5   15.    17.5\n",
            "   20.    22.5   25.    27.5   30.    32.5   35.    37.5   40.    42.5\n",
            "   45.    47.5   50.    52.5   55.    57.5   60.    62.5   65.    67.5\n",
            "   70.    72.5   75.    77.5   80.    82.5   85.    87.5   90.    92.5\n",
            "   95.    97.5  100.   102.5  105.   107.5  110.   112.5  115.   117.5\n",
            "  120.   122.5  125.   127.5  130.   132.5  135.   137.5  140.   142.5\n",
            "  145.   147.5  150.   152.5  155.   157.5  160.   162.5  165.   167.5\n",
            "  170.   172.5  175.   177.5]\n",
            "temp shape before adding data = (0, 0, 73, 144)\n",
            "temp shape after adding data = (5, 10, 73, 144)\n",
            "levels shape after adding 'level' data to variable temp = (10,)\n"
          ]
        }
      ],
      "source": [
        "# 编造一些数据（并处理维度）\n",
        "import numpy as np\n",
        "\n",
        "# Create latitudes and longitudes\n",
        "# 创建纬度和经度\n",
        "\n",
        "# lats from -90 to +90 in 2.5 degrees increments\n",
        "# 纬度从 -90 到 +90，增量为 2.5 度\n",
        "lats =  np.arange(-90,91,2.5)\n",
        "\n",
        "# lons  from -180 to +180 in 2.5 degrees increments\n",
        "# 经度从 -180 到 +180，增量为 2.5 度\n",
        "lons =  np.arange(-180,180,2.5)\n",
        "\n",
        "# assign lats/lons data to a slice (of latitudes/longitudes arrays)\n",
        "# 将 lats/lons 数据赋值给（纬度/经度数组的）切片\n",
        "# 注意：[:] 表示“全部填满”\n",
        "latitudes[:] = lats\n",
        "longitudes[:] = lons\n",
        "\n",
        "# let's see what we made:\n",
        "print(\"latitudes =\\n{}\".format(latitudes[:]))\n",
        "print(\"longitudes =\\n{}\".format(longitudes[:]))\n",
        "\n",
        "\n",
        "# Let's add random numbers for temperature 'temp'.\n",
        "# Recalling we created variable temp with the following dimensions:\n",
        "# temp(time, level, lat, lon). 'temp' is a 4D dataset.\n",
        "# Dimensions time and level are unlimited, while lat and lon are finite.\n",
        "# 为温度 'temp' 添加随机数。\n",
        "# 回想一下我们创建的变量 temp 具有以下维度：\n",
        "# temp(time, level, lat, lon)。'temp' 是一个 4D 数据集。\n",
        "# 维度 time 和 level 是无限的，而 lat 和 lon 是有限的。\n",
        "print(\"temp shape before adding data = {}\".format(temp.shape))\n",
        "\n",
        "\n",
        "from numpy.random import uniform\n",
        "\n",
        "# we now create 5 different time slices, and 10 different levels.\n",
        "# Levels represent altitude slices of a 4D dataset.\n",
        "# 现在创建 5 个不同的时间切片和 10 个不同的Levels(层级)。\n",
        "# Levels（层级）代表 4D 数据集的高度切片\n",
        "nlats = len(rootgrp.dimensions[\"lat\"])\n",
        "nlons = len(rootgrp.dimensions[\"lon\"])\n",
        "\n",
        "# 核心操作：一次性填入 5个时间点 x 10个高度层 的数据\n",
        "temp[0:5, 0:10, :, :] = uniform(size=(5, 10, nlats, nlons))\n",
        "\n",
        "print(\"temp shape after adding data = {}\".format(temp.shape))\n",
        "\n",
        "# !! Beware: unlike numpy arrays, variables with unlimited dimensions will grow\n",
        "# along these dimensions if we assign data that exceed the size of the finite dimensions!\n",
        "#\n",
        "# Since we have created a 4D temp dataset, the coordinate variable 'levels' has grown as well,\n",
        "# even if we have not yet assigned values to each 'level';\n",
        "# as can be seen here:\n",
        "# !! 注意：与 numpy 数组不同，具有无限维度的变量将会增长\n",
        "# 如果我们赋予的数据超过了有限维度的大小，它会沿着这些维度增长！\n",
        "# 既然我们创建了一个 4D 的 temp 数据集，坐标变量 'levels' 也随之增长了，\n",
        "# 即使我们还没有给每个 'level' 赋值；\n",
        "# 如下所示：\n",
        "print(\"levels shape after adding 'level' data to variable temp = {}\".format(levels.shape))\n",
        "\n",
        "# Let's now assign some values to 'levels':\n",
        "# 现在给 'levels' 赋一些值：\n",
        "levels[:] =  [1000.,850.,700.,500.,300.,250.,200.,150.,100.,50.]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **NetCDF4的Slicing and Indexing**"
      ],
      "metadata": {
        "id": "sOUsY_7d_qIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NetCDF4 的切片规则和 NumPy 不完全一样。**"
      ],
      "metadata": {
        "id": "pQTPBMNe_9MM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1BDZTdTmDmz"
      },
      "source": [
        "### **Slicing and indexing**:   \n",
        "\n",
        "Definition of slices are similar to numpy, i.e. `start:stop:step` triplet can be used, and so can an integer index `i` to take the i-th element, blicing rules in NetCDF4 works a little differently than in numpy. For example:\n",
        "\n",
        "> temp[0, 0, [0,1,2,3], [0,1,2,3]].shape\n",
        "\n",
        "returns (4,4), so this would be a 4 rows x 4 column array, corresponding to the first time point and the first level only.\n",
        "\n",
        "In numpy, this would result in only 4 elements (e.g. a vector with 4 elements).\n",
        "\n",
        "We can use slicing and indexing to select data from N-dimensional arrays. For example, to extract the 0th, 2nd and 4th index of the time dimension, the 850, 500 and 200 hPa pressure levels, keep only data from the Northern Hemisphere and Eastern Hemisphere from the the 'temp' variable we defined in test.nc, we could simply do:\n",
        "\n",
        "---\n",
        "\n",
        "**Part 1：翻译**\n",
        "\n",
        "**切片和索引 (Slicing and indexing)**：切片的定义与 numpy 相似，即可以使用 `start:stop:step`（开始:结束:步长）三元组，也可以使用整数索引 `i` 来获取第 i 个元素，但 NetCDF4 中的切片规则与 numpy 中的工作方式略有不同。例如：\n",
        "\n",
        "> temp[0, 0, [0,1,2,3], [0,1,2,3]].shape\n",
        "\n",
        "返回 (4,4)，因此这是一个 4 行 x 4 列的数组，对应于第一个时间点和第一个层级。\n",
        "\n",
        "在 numpy 中，这只会产生 4 个元素（例如，一个包含 4 个元素的向量）。\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（切蛋糕的技巧）**\n",
        "\n",
        "这一步教你如何**只取出你想要的那部分数据**。\n",
        "\n",
        "1. **NetCDF vs NumPy 的区别（Quiz 必考坑点）：**\n",
        "    * **NumPy 的逻辑：** 如果你给它两个列表 `[0,1]` 和 `[0,1]`，NumPy 会认为你要取坐标为 `(0,0)` 和 `(1,1)` 的**两个点**。\n",
        "    * **NetCDF 的逻辑：** 如果你给它同样的列表，NetCDF 会认为你要取第 0 行、第 1 行 **以及** 第 0 列、第 1 列交叉形成的**网格区域**（共 2x2=4 个点）。它会自动做“正交组合”。\n",
        "    * **结论：** 在 NetCDF 里切片取出来的数据通常是一个**块（Block/Grid）**，而不是散落的点。\n",
        "\n",
        "\n",
        "\n",
        "2. **高级切片技巧（Fancy Slicing）：**\n",
        "    * **`::2`**：表示“隔一个取一个”。比如时间上有 5 个点，取 0, 2, 4。\n",
        "    * **`[1,3,6]`**：表示“挑着取”。比如我只想要第 1 层、第 3 层和第 6 层的数据。\n",
        "    * **`lats>0`**：表示“条件筛选”。这叫**布尔索引 (Boolean Indexing)**。只要纬度大于 0（北半球）的格子我都要，南半球的不要。\n",
        "\n",
        "> **一句话攻略：** NetCDF 的切片比 NumPy 更符合直觉（给横纵坐标就给你切出一个矩形区域）。你可以混合使用“冒号切片”、“列表索引”和“条件筛选”来精准定位你需要的数据块。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以使用切片和索引从 N 维数组中选择数据。\n",
        "\n",
        "例如，要提取时间维度的第 0、第 2 和第 4 个索引，提取 850、500 和 200 hPa 的气压层级，并从我们在 test.nc 中定义的 'temp' 变量中仅保留来自**北半球 (Northern Hemisphere)** 和 **东半球 (Eastern Hemisphere)** 的数据，我们可以简单地这样做："
      ],
      "metadata": {
        "id": "qZ2fJmHwBxYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-xWpM7GmDmz",
        "outputId": "6bc77caa-255b-4413-a6c3-0741dc285c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of fancy temp slice = (3, 3, 36, 71)\n"
          ]
        }
      ],
      "source": [
        "# extract the 0th, 2nd and 4th index of the time dimension (step=2)\n",
        "# 提取时间维度的第 0, 2, 4 个索引（步长为 2）\n",
        "# extract specific levels by index [1, 3, 6] (corresponding to 850, 500, 200 hPa)\n",
        "# 通过索引 [1, 3, 6] 提取特定层级（对应 850, 500, 200 hPa）\n",
        "# keep Northern Hemisphere (lats > 0) and Eastern Hemisphere (lons > 0)\n",
        "# 保留北半球 (lats > 0) 和东半球 (lons > 0)\n",
        "\n",
        "tempdat = temp[::2, [1,3,6], lats>0, lons>0]\n",
        "print(\"shape of fancy temp slice = {}\".format(tempdat.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0fsLXLtmDm0"
      },
      "source": [
        "### **Scalar variables**:\n",
        "\n",
        "When dealing with scalar variables (e.g. variable 'v') with no dimensions, one can use numpy to extract a numpy scalar array from the data: `numpy.asarray(v)`, or `v[...]`.\n",
        "\n",
        "---\n",
        "**Part 1翻译：**\n",
        "\n",
        "**标量变量 (Scalar variables)**：当处理没有维度的**标量变量**（例如变量 'v'）时，可以使用 numpy 从数据中提取 numpy 标量数组：`numpy.asarray(v)`，或 `v[...]`。\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（读取“单独的一个数”）**\n",
        "\n",
        "这一步是针对**0 维数据**（只有一个值）的读取技巧。\n",
        "\n",
        "1.  **什么叫标量变量 (Scalar Variable)？**\n",
        "    * 就是**只有一个数**的变量。\n",
        "    * 比如：`global_mean_temp`（全球平均气温），它没有长度、宽度、高度，它就是单独的一个值 `14.5`。\n",
        "\n",
        "2.  **怎么读取它？（特殊写法）**\n",
        "    * 因为它没有维度，你不能写 `v[0]`（因为它不是一个列表，没有第 0 个元素）。\n",
        "    * **方法一：** `v[...]`。这里的三个点 `...` (Ellipsis) 意思是“把里面的东西全倒出来”。\n",
        "    * **方法二：** `numpy.asarray(v)`。用 NumPy 的工具把它强制转换成数字。\n",
        "\n",
        "> **一句话攻略：** 如果变量只是单独的一个数（0 维），别用 `[0]` 去读，要用 `[...]` 或者 `numpy.asarray()`。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDDhC-5imDm0"
      },
      "source": [
        "### **Masks（掩码）**\n",
        "\n",
        "`Masks` are often used in climate science data to separate data from different regions (e.g. ocean vs land, etc.). By default, the python NetCDF4 API returns numpy arrays that mask entries that equal the `missing_value` or `_FillValue` attribute. One can force unmasking (i.e. return all values, regardless) using the `Dataset.set_auto_mask` method. To switch back auto-masking, use `Dataset.set_always_mask`. When writing a masked array to a variable in a netcdf file (i.e. an array with missing data), the masked elements are filled with the value that is defined in the `missing_value` attribute that was defined when creating the netcdf fie.\n",
        "\n",
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "**掩码 (Masks)**：`Masks`（掩码）经常被用于气候科学数据中，以区分不同区域的数据（例如海洋与陆地等）。\n",
        "\n",
        "\n",
        "\n",
        "默认情况下，Python `NetCDF4` API 返回的 numpy 数组会将等于 `missing_value`（缺失值）或 `_FillValue`（填充值）属性的条目进行**掩盖 (mask)**。\n",
        "\n",
        "可以使用 `Dataset.set_auto_mask` 方法强制**取消掩码 (unmasking)**（即无论如何都返回所有值）。要切换回自动掩码，请使用 `Dataset.set_always_mask`。\n",
        "\n",
        "当将掩码数组写入 NetCDF 文件中的变量时（即包含缺失数据的数组），被掩盖的元素会被填充为在创建 NetCDF 文件时定义的 `missing_value` 属性中的值。\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（给无效数据打马赛克）**\n",
        "\n",
        "这一步解释了 Python 如何处理 **“坏数据”** 或 **“空数据”** 。\n",
        "\n",
        "1.  **为什么要用 Mask (掩码)？**\n",
        "    * 想象你有一张全球海洋温度图。陆地上是没有海水温度的。\n",
        "    * 那些陆地区域的数据通常会用一个极端的数字来填充（比如 `-9999` 或 `1e20`），这就叫 `_FillValue`。\n",
        "    * 如果你不处理，直接算全球平均温度，这堆 `-9999` 会把结果拉到地狱去。\n",
        "\n",
        "2.  **Python 帮你做了什么？（Auto-masking）**\n",
        "    * `netCDF4` 这个库很聪明。当你读取数据时，它一旦看到 `-9999`（或者文件里定义的任何缺失值），它就会自动给这个位置**盖上一层布 (Mask)**。\n",
        "    * 此时，这个位置在 Python 里就不再是数字 `-9999` 了，而变成了 `--` (无效值)。\n",
        "    * 这样你算平均数时，Python 会自动忽略这些点，结果就是正确的。\n",
        "\n",
        "3.  **实操提示：**\n",
        "    * **读取时：** 默认开启掩码，让你只看到有效数据。如果你非要看原始的 `-9999`，可以用 `set_auto_mask(False)` 关掉它。\n",
        "    * **写入时：** 当你把处理完的数据存回文件，Python 会自动把那些 `--` 的空位重新填回 `-9999`，以便其他软件识别。\n",
        "\n",
        "> **一句话攻略：** Mask 就是数据的“防呆保护”。它自动把代表“无效”的怪异数字（如 -9999）隐藏起来，防止你计算出错。默认情况下不需要动它。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MglQeyg5mDm0"
      },
      "source": [
        "### **时间坐标（Time coordinates）**:\n",
        "\n",
        "The time coordinate ofen poses a special challenge when working with environmental data. Not only the format of the time information is complex (e.g. YY-MM-DD hh:mm:ss) and awkard to deal with, but this can also depend on the calendar used, or requires some additional information about when T=0 was if the time coordinate is given as relative time. CF Conventions advocate for a measure of time given relative to a fixed data and using a defined calendar, and given in the YY-MM-DD hh:mm:ss format.\n",
        "\n",
        "The `cftime` and `datetime` packages provide functions to decode time units and variable values from a netCDF file, if that file conforms to CF Conventions. The following three functions can greatly simplify the problem of dealing with time:\n",
        "1. [`num2date`](http://unidata.github.io/netcdf4-python/#num2date): converts numeric values to time in specified `units` with a given `calendar` to so-called \"datetime objects\". All calendars defined in the CF Conventions are supported: ‘standard’, ‘gregorian’, ‘proleptic_gregorian’ ‘noleap’, ‘365_day’, ‘360_day’, ‘julian’, ‘all_leap’, ‘366_day’.\n",
        "2. [`date2num`](http://unidata.github.io/netcdf4-python/#date2num): this does the opposite of `num2date`\n",
        "3. [`date2index`](http://unidata.github.io/netcdf4-python/#date2index): returns the indices of a netCDF time variable that correspond to given dates\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "**时间坐标 (Time coordinates)**：在处理环境数据时，时间坐标通常会带来特殊的挑战。不仅时间信息的格式复杂（例如 YY-MM-DD hh:mm:ss）且难以处理，而且这还可能取决于所使用的**日历 (calendar)**，或者如果时间坐标是以相对时间给出的，则需要关于 T=0 何时发生的一些额外信息。**CF 公约 (CF Conventions)** 提倡相对于固定日期并使用定义的日历给出的时间度量，并以 YY-MM-DD hh:mm:ss 格式给出。\n",
        "\n",
        "`cftime` 和 `datetime` 包提供了从 NetCDF 文件解码时间单位和变量值的函数（如果该文件符合 CF 公约）。以下三个函数可以大大简化处理时间的问题：\n",
        "\n",
        "1.  [`num2date`](http://unidata.github.io/netcdf4-python/#num2date)：将数值转换为具有给定 `calendar`（日历）和指定 `units`（单位）的时间，转换为所谓的“datetime 对象”。支持 CF 公约中定义的所有日历：‘standard’, ‘gregorian’, ‘proleptic_gregorian’ ‘noleap’, ‘365_day’, ‘360_day’, ‘julian’, ‘all_leap’, ‘366_day’。\n",
        "2.  [`date2num`](http://unidata.github.io/netcdf4-python/#date2num)：这与 `num2date` 相反（将日期对象转换为数值）。\n",
        "3.  [`date2index`](http://unidata.github.io/netcdf4-python/#date2index)：返回对应于给定日期的 NetCDF 时间变量的索引。\n",
        "\n",
        "----\n",
        "\n",
        "**Part 2：大白话总结（人话翻译机）**\n",
        "\n",
        "这一步解决的是 **“电脑存的时间”**和**“人看的时间”** 之间的翻译问题。\n",
        "\n",
        "1. **痛点是什么？**\n",
        "    * **电脑的逻辑：** 电脑不存 \"2001-03-01\"，它存的是一个数字，比如 `17533104`。这代表“从 0001 年 1 月 1 日起过了多少个小时”。\n",
        "    * **人的逻辑：** 我们只看年月日。\n",
        "    * **日历的坑：** 气候模型有时候为了计算方便，一年只有 360 天（每月 30 天），或者没有闰年。如果你用普通的 Python 日历去算，绝对会报错。\n",
        "\n",
        "2. **神器 `cftime` 的两个核心功能（Quiz 必考）：**\n",
        "    * **`date2num` (人 -> 电脑)：** 把人类读得懂的 `datetime` 对象（比如 2001年3月1日），翻译成电脑能存的数字（比如 17533104）。写文件时用这个。\n",
        "    * **`num2date` (电脑 -> 人)：** 把文件里读出来的奇怪数字，翻译回人类日期。**画图（Plotting）时必用这个**，因为你不想 X 轴显示一堆乱七八糟的数字，你想显示年份。\n",
        "\n",
        "3. **代码实操逻辑：**\n",
        "    * 先用 Python 生成一串人类日期列表 (`dates`)。\n",
        "    * 用 `date2num` 把它变成数字，存进 NetCDF 文件的 `times[:]` 变量里。\n",
        "    * 存进去的时候，必须告诉它基于什么单位（`units`，比如 \"hours since...\"）和什么日历（`calendar`，比如 \"gregorian\"）。\n",
        "\n",
        "> **一句话攻略：** NetCDF 文件里存的时间都是**数字**（偏移量）。存数据时用 `date2num` 把日期变数字；读数据画图时用 `num2date` 把数字变日期。千万别自己手算，会死在闰年上，一定要用这两个函数！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bffJ1nuxmDm0",
        "outputId": "bcb48972-0211-4c2d-abb4-b21e0414fa4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of time points for the 'temp' variable:\n",
            "5\n",
            "dates created for each time point:\n",
            "[datetime.datetime(2001, 3, 1, 0, 0), datetime.datetime(2001, 3, 1, 12, 0), datetime.datetime(2001, 3, 2, 0, 0), datetime.datetime(2001, 3, 2, 12, 0), datetime.datetime(2001, 3, 3, 0, 0)]\n",
            "Numerical time values (in units hours since 0001-01-01 00:00:00.0):\n",
            "[17533104. 17533116. 17533128. 17533140. 17533152.]\n",
            "dates corresponding to time values:\n",
            "[cftime.DatetimeGregorian(2001, 3, 1, 0, 0, 0, 0, has_year_zero=False)\n",
            " cftime.DatetimeGregorian(2001, 3, 1, 12, 0, 0, 0, has_year_zero=False)\n",
            " cftime.DatetimeGregorian(2001, 3, 2, 0, 0, 0, 0, has_year_zero=False)\n",
            " cftime.DatetimeGregorian(2001, 3, 2, 12, 0, 0, 0, has_year_zero=False)\n",
            " cftime.DatetimeGregorian(2001, 3, 3, 0, 0, 0, 0, has_year_zero=False)]\n"
          ]
        }
      ],
      "source": [
        "# Example of dealing with time in python/netCDF4\n",
        "# load the proper functions\n",
        "# 加载适当的函数\n",
        "from datetime import datetime, timedelta\n",
        "from cftime import num2date, date2num\n",
        "\n",
        "\n",
        "# Recall the 'temp' variable in our toy netCDF file\n",
        "# We build the 'temp' variable with 5 time points:\n",
        "# the '0' index of course picks the first dimension, here 'time'\n",
        "# 回想一下我们玩具 netCDF 文件中的 'temp' 变量\n",
        "# 我们构建了具有 5 个时间点的 'temp' 变量：\n",
        "# '0' 索引当然选取第一个维度，这里是 'time'\n",
        "print(\"Number of time points for the 'temp' variable:\\n{}\".format(temp.shape[0]))\n",
        "\n",
        "\n",
        "# We now use the 'datetime' and 'timedelta' functions, from the 'datetime' package\n",
        "# to create 5 time points, spaced by 12 hours, starting on  March 1st 2001.\n",
        "# 我们现在使用 'datetime' 包中的 'datetime' 和 'timedelta' 函数\n",
        "# 创建 5 个时间点，间隔 12 小时，从 2001 年 3 月 1 日开始。\n",
        "dates = [datetime(2001,3,1)+n*timedelta(hours=12) for n in range(temp.shape[0])]\n",
        "\n",
        "\n",
        "# Let's see the 5 time points, noticing that the date automatically goes to march 2nd 3rd\n",
        "# 让我们看看这 5 个时间点，注意日期会自动变为 3 月 2 日、3 日\n",
        "print(\"dates created for each time point:\\n{}\".format(dates))\n",
        "\n",
        "\n",
        "# We can then use the 'date2num' function, from 'cftime' to turn these dates into numeric values\n",
        "# Numerical values are much easier to plot or deal with, due to their simpler format.\n",
        "# Numerical values make no intuitive sense, though!\n",
        "# Numeric time is useful for analysis, but not no good for communication.\n",
        "# Note that 'units' and 'calendar' are specified.\n",
        "# 然后我们可以使用 'cftime' 中的 'date2num' 函数将这些日期转换为数值\n",
        "# 数值更容易绘制或处理，因为它们的格式更简单。\n",
        "# 注意指定了 'units'（单位）和 'calendar'（日历）。\n",
        "# 将转换后的数字存入 NetCDF 文件中的 times 变量\n",
        "times[:] = date2num(dates,units=times.units,calendar=times.calendar)\n",
        "\n",
        "\n",
        "# Numeric time values are relative to a starting point!\n",
        "# 数值时间值是相对于起点的！\n",
        "print(\"Numerical time values (in units {}):\\n{}\".format(times.units, times[:]))\n",
        "\n",
        "\n",
        "# we can use 'num2date' to turn numerical times back into calendar dates\n",
        "# 我们可以使用 'num2date' 将数值时间转回日历日期\n",
        "dates = num2date(times[:],units=times.units,calendar=times.calendar)\n",
        "print(\"dates corresponding to time values:\\n{}\".format(dates))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPHtCqxHmDm0"
      },
      "source": [
        "\n",
        "<a id='NCO'></a>\n",
        "### **NetCDF C 操作符 (NCO) - 命令行（NetCDF C operators (NCO) - command line）**\n",
        "---\n",
        "NetCDF Operators ([NCO](http://nco.sourceforge.net)) are (in my opinion) fantastic!\n",
        "\n",
        "The NCO operators are a dozen standalone programmes that allow users to manipulate and analyze data from NetCDF, HDF or DAP files directly from the command line without having to open file or load them into memory in an interpreted language and produce output to screen or in text, binary, or netCDF formats. They are written in C (Fortran versions also available) and build on the [GNU scientific library](https://www.gnu.org/software/gsl/), making them very fast and efficient. They are build to work with the CF Conventions for metadata, and can work with OPeNDAP as well.\n",
        "\n",
        "They are meant to be used to transform primary data into secondary data (stuff that is then plotted and carries interpretable meaning as decided by the analyst). NCO operators are not for plotting, plotting will be done in a higer-level language (e.g. python, R, Matlab, etc.).\n",
        "\n",
        "NCO are \"race horses\", designed to minimize system memory required and improve speed. NCO routines are not meant to do everything and anything, they are tools, not a full-fledged programming language. They are  a set of (very useful) functions that do a small set of manipulations common to a lot of analysis workflows specific to gridded unstructured data interpretation exercises.\n",
        "\n",
        "NCO operators are quite specific in what they do but they can be daisy-chained to perform complex tasks.\n",
        "\n",
        "...sadly, they are not that easy or intuitive to use - it takes practice. But they are powerful as they can be embeded in shell scripts or in any code (with a call to the system), where they can help solve certain memory issues (depending on the interpreted language used), or help speed up the workflow.\n",
        "\n",
        "NCO routines [can be installed](http://nco.sourceforge.net/#Executables) using package managers like homebrew (on MacOS) or even from conda (or they can be compiled from source too). Some NCO operators can support shared memory parallelism with OpenMP threading (if compiled to do so), but most of the NCO tools do not benefit from great speed up as processing speed tends to be limited by I/O operations.   \n",
        "\n",
        "The 12 NCO operators are ([detailed explanation and examples are found in the user guide](http://nco.sourceforge.net/nco.html)):\n",
        "\n",
        "* [ncap2](http://nco.sourceforge.net/nco.html#ncap2) netCDF Arithmetic Processor\n",
        "* [ncatted](http://nco.sourceforge.net/nco.html#ncatted-netCDF-Attribute-Editor) netCDF ATTribute EDitor\n",
        "* [ncbo](http://nco.sourceforge.net/nco.html#ncbo-netCDF-Binary-Operator) netCDF Binary Operator (addition, multiplication...)\n",
        "* [ncclimo](http://nco.sourceforge.net/nco.html#ncclimo-netCDF-Climatology-Generator) netCDF CLIMatOlogy Generator\n",
        "* [nces](http://nco.sourceforge.net/nco.html#nces-netCDF-Ensemble-Statistics) netCDF Ensemble Statistics\n",
        "* [ncecat](http://nco.sourceforge.net/nco.html#ncecat-netCDF-Ensemble-Concatenator) netCDF Ensemble conCATenator\n",
        "* [ncflint](http://nco.sourceforge.net/nco.html#ncflint-netCDF-File-Interpolator) netCDF FiLe INTerpolator\n",
        "* [ncks](http://nco.sourceforge.net/nco.html#ncks-netCDF-Kitchen-Sink) netCDF Kitchen Sink\n",
        "* [ncpdq](http://nco.sourceforge.net/nco.html#ncpdq-netCDF-Permute-Dimensions-Quickly) netCDF Permute Dimensions Quickly, Pack Data Quietly\n",
        "* [ncra](http://nco.sourceforge.net/nco.html#ncra-netCDF-Record-Averager) netCDF Record Averager\n",
        "* [ncrcat](http://nco.sourceforge.net/nco.html#ncrcat-netCDF-Record-Concatenator) netCDF Record conCATenator\n",
        "* [ncremap](http://nco.sourceforge.net/nco.html#ncremap-netCDF-Remapper) netCDF REMAPer\n",
        "* [ncrename](http://nco.sourceforge.net/nco.html#ncrename-netCDF-Renamer) netCDF RENAMEer\n",
        "* [ncwa](http://nco.sourceforge.net/nco.html#ncwa-netCDF-Weighted-Averager) netCDF Weighted Averager\n",
        "\n",
        "\n",
        "The most useful one, for me, have been `ncks`, `ncpdq`, `ncra`.\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "**Part 1:翻译**\n",
        "\n",
        "<a id='NCO'></a>\n",
        "\n",
        "**NetCDF C 操作符 (NCO) - 命令行**\n",
        "---\n",
        "NetCDF 操作符 ([NCO](http://nco.sourceforge.net))（在我看来）棒极了！\n",
        "\n",
        "NCO 操作符是十几个独立的程序，允许用户直接从命令行操作和分析来自 NetCDF、HDF 或 DAP 文件的数据，而无需在解释型语言中打开文件或将其加载到内存中，并以文本、二进制或 netCDF 格式生成屏幕输出。它们是用 C 语言编写的（也有 Fortran 版本），并建立在 [GNU 科学库](https://www.gnu.org/software/gsl/) 之上，这使得它们非常快速和高效。它们旨在与用于元数据的 CF Conventions 一起工作，也可以与 OPeNDAP 一起工作。\n",
        "\n",
        "它们旨在用于将原始数据转换为二级数据（即由分析师决定进行绘图并具有可解释意义的内容）。NCO 操作符**不是**用来绘图的，绘图将在更高级的语言（例如 python, R, Matlab 等）中完成。\n",
        "\n",
        "NCO 是“赛马 (race horses)”，旨在最大限度地减少所需的系统内存并提高速度。NCO 例程并不意味着要做所有事情，它们是工具，而不是一个成熟的编程语言。它们是一组（非常有用的）函数，执行一小组操作，这些操作对于许多特定于网格化非结构化数据解释练习的分析工作流来说是通用的。\n",
        "\n",
        "NCO 操作符在它们所做的事情上非常具体，但它们可以**链式调用 (daisy-chained)** 以执行复杂的任务。\n",
        "\n",
        "...遗憾的是，它们使用起来并不那么容易或直观——需要练习。但它们功能强大，因为它们可以嵌入到 shell 脚本或任何代码中（通过系统调用），在这些代码中它们可以帮助解决某些内存问题（取决于所使用的解释型语言），或帮助加快工作流程。\n",
        "\n",
        "NCO 例程[可以安装](http://nco.sourceforge.net/#Executables) 使用像 homebrew（在 MacOS 上）甚至 conda 这样的包管理器（或者它们也可以从源代码编译）。一些 NCO 操作符可以支持带有 OpenMP 线程的共享内存并行（如果编译时支持），但大多数 NCO 工具不会从巨大的加速中受益，因为处理速度往往受到 I/O（输入/输出）操作的限制。\n",
        "\n",
        "12 个 NCO 操作符是（[详细解释和示例可在用户指南中找到](http://nco.sourceforge.net/nco.html)）：\n",
        "\n",
        "* [ncap2](http://nco.sourceforge.net/nco.html#ncap2) netCDF 算术处理器 (Arithmetic Processor)\n",
        "* [ncatted](http://nco.sourceforge.net/nco.html#ncatted-netCDF-Attribute-Editor) netCDF 属性编辑器 (ATTribute EDitor)\n",
        "* [ncbo](http://nco.sourceforge.net/nco.html#ncbo-netCDF-Binary-Operator) netCDF 二进制操作符 (Binary Operator)（加法、乘法...）\n",
        "* [ncclimo](http://nco.sourceforge.net/nco.html#ncclimo-netCDF-Climatology-Generator) netCDF 气候学生成器 (CLIMatOlogy Generator)\n",
        "* [nces](http://nco.sourceforge.net/nco.html#nces-netCDF-Ensemble-Statistics) netCDF 集合统计 (Ensemble Statistics)\n",
        "* [ncecat](http://nco.sourceforge.net/nco.html#ncecat-netCDF-Ensemble-Concatenator) netCDF 集合连接器 (Ensemble conCATenator)\n",
        "* [ncflint](http://nco.sourceforge.net/nco.html#ncflint-netCDF-File-Interpolator) netCDF 文件插值器 (FiLe INTerpolator)\n",
        "* [ncks](http://nco.sourceforge.net/nco.html#ncks-netCDF-Kitchen-Sink) netCDF 厨房水槽 (Kitchen Sink)（意为无所不包的工具箱）\n",
        "* [ncpdq](http://nco.sourceforge.net/nco.html#ncpdq-netCDF-Permute-Dimensions-Quickly) netCDF 快速置换维度 (Permute Dimensions Quickly)、安静打包数据\n",
        "* [ncra](http://nco.sourceforge.net/nco.html#ncra-netCDF-Record-Averager) netCDF 记录平均器 (Record Averager)\n",
        "* [ncrcat](http://nco.sourceforge.net/nco.html#ncrcat-netCDF-Record-Concatenator) netCDF 记录连接器 (Record conCATenator)\n",
        "* [ncremap](http://nco.sourceforge.net/nco.html#ncremap-netCDF-Remapper) netCDF 重映射器 (REMAPer)\n",
        "* [ncrename](http://nco.sourceforge.net/nco.html#ncrename-netCDF-Renamer) netCDF 重命名器 (RENAMEer)\n",
        "* [ncwa](http://nco.sourceforge.net/nco.html#ncwa-netCDF-Weighted-Averager) netCDF 加权平均器 (Weighted Averager)\n",
        "\n",
        "对我来说，最有用的是 `ncks`、`ncpdq`、`ncra`。\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（命令行里的“瑞士军刀”）**\n",
        "\n",
        "这一块介绍的是**进阶玩家**必备工具。虽然 Python 很强，但有时候直接在终端敲命令更快。\n",
        "\n",
        "1.  **NCO 是什么？**\n",
        "    * 它们不是 Python 库，而是一堆**独立的小软件**（命令行工具）。\n",
        "    * 它们的特点是**快、省内存**。被称为“赛马 (Race Horses)”。\n",
        "    * 它们主要用来做**预处理**：比如在把数据读进 Python 画图之前，先用 NCO 把那个 100GB 的大文件切小一点，或者算个平均值。\n",
        "\n",
        "2.  **为什么不直接用 Python？**\n",
        "    * Python（尤其是 Jupyter Notebook）在处理超大文件时容易内存溢出（Memory Error）。\n",
        "    * NCO 是用 C 语言写的，它可以直接在硬盘上操作文件，不需要把整个文件加载到内存里，所以处理 TB 级数据毫无压力。\n",
        "\n",
        "3.  **最常用的三个命令（Quiz 考点）：**\n",
        "    * **`ncks` (Kitchen Sink)：** **最最常用的神器！** 就像一把瑞士军刀。主要用来**切片**（提取特定变量、特定时间段的数据）。\n",
        "        * *例子：* `ncks -v temp input.nc output.nc` （只把 temp 变量提取出来）。\n",
        "    * **`ncra` (Record Averager)：** 用来**算平均**。比如把逐日数据算成逐年平均。\n",
        "    * **`ncrcat` (Record Concatenator)：** 用来**拼接文件**。比如你有 `2000.nc`, `2001.nc`, `2002.nc`，用这个命令可以把它们拼成一个 `2000-2002.nc`。\n",
        "\n",
        "**一句话攻略：** NCO 是气候数据处理的“手术刀”。虽然全黑的命令行界面看着吓人，但当你遇到电脑打不开的大文件时，用 `ncks` 切一下有奇效。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KByR1I0pmDm0"
      },
      "source": [
        "### **气候数据操作符（Climate Data Operators (CDO)）**\n",
        "The Climate Data Operators ([CDO](https://code.mpimet.mpg.de/projects/cdo)) software is a collection of >600 operators for standard processing of climate and forecast model data. They are slighly less memory/speed efficient than NCO, but they do more (>600 vs 12!). CDO support the main data formats used in the field, such as GRIB and netCDF.\n",
        "\n",
        "They too can be daisy-chained for specific applications. There are [many recipes available](https://code.mpimet.mpg.de/projects/cdo/embedded/cdo_eca.pdf) to calculate various climate indices and diagnostics, what makes them very attractive to evaluate gridded climate data or climate model output.\n",
        "\n",
        "The [CDO user guide](https://code.mpimet.mpg.de/projects/cdo/embedded/index.html) provides details instructions and examples.\n",
        "\n",
        "[They are also very useful, but we don't have time to discuss them in detail here. You are encouraged to practice with CDO on your own time]\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "**气候数据操作符 ([CDO](https://code.mpimet.mpg.de/projects/cdo))** 软件是包含超过 600 个操作符的集合，用于气候和预报模型数据的标准处理。它们在内存/速度效率上比 NCO 稍差，但它们能做更多的事情（>600 个 vs 12 个！）。CDO 支持该领域使用的主要数据格式，如 **GRIB** 和 **netCDF**。\n",
        "\n",
        "它们也可以被**链式调用 (daisy-chained)** 以用于特定应用。有[许多现成的食谱 (recipes)](https://code.mpimet.mpg.de/projects/cdo/embedded/cdo_eca.pdf) 可用于计算各种**气候指数 (climate indices)** 和诊断，这使得它们在评估网格化气候数据或气候模型输出方面非常有吸引力。\n",
        "\n",
        "[**CDO 用户指南**](https://code.mpimet.mpg.de/projects/cdo/embedded/index.html) 提供了详细的说明和示例。\n",
        "\n",
        "[它们也非常有用，但我们没有时间在这里详细讨论。鼓励你在自己的时间里练习使用 CDO]\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（NCO 的大哥）**\n",
        "\n",
        "这一块介绍了一个比 NCO 功能更全的工具箱。\n",
        "\n",
        "1.  **CDO vs NCO (Quiz 考点)：**\n",
        "    * **NCO (赛马)：** 只有 12 个命令。特点是**快、省内存**。主要用来做“物理切割”（切片、剪裁、拼接）。\n",
        "    * **CDO (重型卡车)：** 有 600 多个命令。特点是**功能全**。主要用来做**“数学计算”**和**“科学分析”**。\n",
        "\n",
        "2.  **为什么要用 CDO？**\n",
        "    * **算指数神器：** 比如你想算“今年有多少天是热浪？”或者“连续干旱天数是多少？”。NCO 做这个很累，但 CDO 里面内置了现成的命令（比如 `cdo eca_hwdi` 就能直接算热浪指数），非常方便。\n",
        "    * **格式通吃：** CDO 对 **GRIB** 格式（气象预报常用的格式）支持得更好，而 NCO 主要是针对 NetCDF。\n",
        "\n",
        "3.  **课程地位：**\n",
        "    * 教授表示这个工具**非常有用**，但是课上**没时间教**。\n",
        "    * **建议：** 如果你以后的毕业论文涉及大量的气候指数计算（比如研究极端天气），一定要去自学一下 CDO，能省你写几百行 Python 代码的时间。\n",
        "\n",
        "> **一句话攻略：** NCO 用来切数据（预处理），CDO 用来算指标（科学分析）。虽然这门课不细讲，但它是做极端天气研究的必备工具。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***ERA5（下载数据）**"
      ],
      "metadata": {
        "id": "Lk57Z3uVTWG6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYGpkVhKmDm1"
      },
      "source": [
        "<a id='reanalysis'></a>\n",
        "\n",
        "## **再分析产品（Reanalysis products）**\n",
        "\n",
        "![C3S](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/c3s-logo.png?raw=1)\n",
        "![ECMWF](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/logo-ecmwf.png?raw=1)\n",
        "\n",
        "[*ERA5 provides hourly estimates of a large number of atmospheric, land and oceanic climate variables. The data cover the Earth on a 30km grid and resolve the atmosphere using 137 levels from the surface up to a height of 80km. ERA5 includes information about uncertainties for all variables at reduced spatial and temporal resolutions.Quality-assured monthly updates of ERA5 (1979 to present) are published within 3 months of real time. Preliminary daily updates of the dataset are available to users within 5 days of real time.*](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5)\n",
        "\n",
        "[ERA5](https://confluence.ecmwf.int/display/CKB/ERA5) is a [family of datasets](https://confluence.ecmwf.int/display/CKB/The+family+of+ERA5+datasets). It currently comprises ERA5, ERA5.1 and ERA5-Land. ERA5 is the fifth generation ECMWF atmospheric reanalysis of the global climate covering the period from January 1950 to present. ERA5 is produced by the Copernicus Climate Change Service ([C3S](https://confluence.ecmwf.int/pages/viewpage.action?pageId=151530614)) at the European Center for Medium-Range Weather Forecasts ([ECMWF](https://www.ecmwf.int)) and made available via the [Climate Change Service](https://climate.copernicus.eu).\n",
        "\n",
        "Importantly, ERA5 is a [**reanalysis product**](https://www.youtube.com/watch?v=FAGobvUGl24), meaning a **model that assimilates data**. A model of the climate (weather) is run, and adjusted (following certain laws of physics and constraints) to fit [as many observations](https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Observations) as possible using a technique called 4D-var. One must realize that, even if assimimated prodcuts such as ERA5 are often used 'in lieu' of observations, they are **\\*not\\*** observations: they are a model product, but a product that is made to look as much like the data as possible given computational, mathematical, physical limitations of the model.\n",
        "\n",
        "ERA5 is one of various [reanalysis products](https://reanalyses.org) available globally. Another well-known produce is the [NCEP/NCAR Reanalysis product](https://en.wikipedia.org/wiki/NCEP/NCAR_Reanalysis). [MERRA-2](https://gmao.gsfc.nasa.gov/reanalysis/MERRA-2/), produced by NASA, is another.\n",
        "\n",
        "The articles by [Hersbach et al. 2020](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.3803) and [Soci et al. 2024](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.4803) discusses the ERA global Reanalysis prodcuct in more detail.\n",
        "\n",
        "While observations are only available in specific locations and at specific times, the reanalysis product provides a clever way to dynamically interpolate between these observations. ERA5 also comes as a gridded product, making it very convenient to use. In the model-world, one can access a complete/global picture every time-step, with a spatial resolution as high as computational limits allow. I re-emphasize that this is not the same as observations! ... but it is as close to observations as we can get if one is trying to work with a spatially and temporall interpolated product.\n",
        "\n",
        "**Grid geometry depends on data format**: Note that the grid geometry of the output data of ERA5 [depends on the format the data that are being downloaded](https://confluence.ecmwf.int/display/CKB/ERA5%3A+What+is+the+spatial+reference). Native GRIB format data are delivered on the model's native grid geometry (this is not a regular lat/lon grid!). On the other hand, data in NetCDF format are automatically interpolated and regridded to a regular lat/lon grid. While this is not very important for most application, one must remember that interpolated data in the NetCDF files are not the same as the original model output and this could have implication for the conservation properties of some variables. It can be easier/convenient to work with data interpolated on a regular lat/lon grid, however.  \n",
        "\n",
        "**Grid definition and wrap-around**: The gridded ERA5 archive is provided on a [-90;+90] latitude grid and a [0;+360] longitude grid, with decimal degrees, referenced to the Greenwich Prime Meridian. While latitude is generally not an issue, care must be taken when working with longitude as one must remember that 0 and 360 are the same point. One must account for the wrap-around issue: although the first column and last column of a datasets on the [0;+360] grid are far away from each other in terms of index, these points are geographically very close. Some software can automatically deal with this wrap-around and convert to [-180;+180] or other system as required, but this should not be taken for granted.\n",
        "\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "**Part 1：翻译**\n",
        "\n",
        "<a id='reanalysis'></a>\n",
        "\n",
        "**再分析产品 (Reanalysis products)**\n",
        "\n",
        "![C3S](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/c3s-logo.png?raw=1)\n",
        "![ECMWF](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/logo-ecmwf.png?raw=1)\n",
        "\n",
        "[*ERA5 提供大量大气、陆地和海洋气候变量的小时估算值。数据覆盖地球的 30km 网格，并将大气从地表到 80km 高度解析为 137 层。ERA5 包含所有变量在降低的空间和时间分辨率下的不确定性信息。ERA5（1979 年至今）经过质量保证的月度更新会在实时发生后 3 个月内发布。该数据集的初步每日更新会在实时发生后 5 天内提供给用户。*](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5)\n",
        "\n",
        "[ERA5](https://confluence.ecmwf.int/display/CKB/ERA5) 是一个[数据集家族](https://confluence.ecmwf.int/display/CKB/The+family+of+ERA5+datasets)。它目前包括 ERA5, ERA5.1 和 ERA5-Land。ERA5 是 ECMWF 对全球气候的第五代大气再分析，涵盖了从 1950 年 1 月至今的时段。ERA5 由**欧洲中期天气预报中心 ([ECMWF](https://www.ecmwf.int))** 的 **哥白尼气候变化服务 ([C3S](https://confluence.ecmwf.int/pages/viewpage.action?pageId=151530614))** 制作，并通过 [气候变化服务](https://climate.copernicus.eu) 提供。\n",
        "\n",
        "重要的是，ERA5 是一个 [**再分析产品 (reanalysis product)**](https://www.youtube.com/watch?v=FAGobvUGl24)，这意味着它是一个 **同化数据的模型 (model that assimilates data)**。一个气候（天气）模型在运行，并使用一种称为 **4D-var** 的技术进行调整（遵循某些物理定律和约束），以尽可能拟合 [尽可能多的观测](https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Observations)。必须认识到，即使像 ERA5 这样的同化产品经常被用来“代替”观测，它们 **\\*不是\\*** 观测：它们是模型产品，但在模型的计算、数学和物理限制下，它们被制作得尽可能像数据。\n",
        "\n",
        "ERA5 是全球可用的各种 [再分析产品](https://reanalyses.org) 之一。另一个著名的产品是 [NCEP/NCAR 再分析产品](https://en.wikipedia.org/wiki/NCEP/NCAR_Reanalysis)。由 NASA 制作的 [MERRA-2](https://gmao.gsfc.nasa.gov/reanalysis/MERRA-2/) 是另一个。\n",
        "\n",
        "[Hersbach 等人 2020](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.3803) 和 [Soci 等人 2024](https://rmets.onlinelibrary.wiley.com/doi/full/10.1002/qj.4803) 的文章更详细地讨论了 ERA 全球再分析产品。\n",
        "\n",
        "虽然观测仅在特定地点和特定时间可用，但再分析产品提供了一种巧妙的方法来在这些观测之间进行**动态插值**。ERA5 也作为**网格化产品**提供，使用起来非常方便。在模型世界中，人们可以在每个时间步长访问一个完整/全球的画面，其空间分辨率高达计算限制所允许的程度。我再次强调，这与观测不同！……但如果试图处理空间和时间上插值的产品，这是我们能得到的最接近观测的结果。\n",
        "\n",
        "**网格几何形状取决于数据格式 (Grid geometry depends on data format)**：请注意，ERA5 输出数据的网格几何形状 [取决于正在下载的数据格式](https://confluence.ecmwf.int/display/CKB/ERA5%3A+What+is+the+spatial+reference)。原生 **GRIB** 格式数据是在模型的原生网格几何形状上交付的（这不是规则的经纬度网格！）。另一方面，**NetCDF** 格式的数据会自动插值并重新网格化为规则的经纬度网格。虽然这对大多数应用来说不是很重要，但必须记住，NetCDF 文件中的插值数据与原始模型输出不同，这可能会对某些变量的**守恒性质**产生影响。不过，处理在规则经纬度网格上插值的数据通常更容易/更方便。\n",
        "\n",
        "**网格定义和环绕 (Grid definition and wrap-around)**：网格化的 ERA5 档案提供在 [-90;+90] 纬度网格和 [0;+360] 经度网格上，使用十进制度数，以格林威治本初子午线为参考。虽然纬度通常不是问题，但在处理**经度**时必须小心，因为必须记住 **0 和 360 是同一个点**。必须考虑**环绕问题 (wrap-around issue)**：虽然 [0;+360] 网格上的数据集的第一列和最后一列在索引方面相距甚远，但这些点在地理上非常接近。某些软件可以自动处理这种环绕并根据需要转换为 [-180;+180] 或其他系统，但这不应被视为理所当然。\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（ERA5 避坑指南）**\n",
        "\n",
        "这一块非常核心，因为它定义了你手里拿到的数据到底是个什么东西。\n",
        "\n",
        "1.  **什么是再分析 (Reanalysis)？**\n",
        "    * **比喻：** 想象一下，世界各地的气象站就像拼图的碎片，但是丢失了很多块（海洋上、极地上没多少气象站）。\n",
        "    * **原理：** ERA5 就是用物理模型（超级计算机模拟）把这些丢失的拼图块**补全**了，同时也让现有的碎片拼接得更平滑。\n",
        "    * **结论：**它是**“最接近真实的模拟数据”**，但它不是 100% 的真实观测。写论文时不能说“ERA5 观测到了...”，要说“ERA5 数据显示...”。\n",
        "\n",
        "2.  **网格陷阱 (NetCDF vs GRIB)：**\n",
        "    * **GRIB：** 是 ERA5 的“原味”格式，但是它的网格很难处理（不是方方正正的）。\n",
        "    * **NetCDF：** 是经过“再加工”的格式。ECMWF 把它拉伸成了方方正正的经纬度网格，方便我们用 Python 处理。\n",
        "    * **注意：** 因为拉伸过，所以数据会有极微小的偏差，但一般做分析可以忽略不计。\n",
        "\n",
        "3.  **地图环绕陷阱 (0 vs 360)：**\n",
        "    * ERA5 的经度是从 **0 到 360 度** 存的。\n",
        "    * **0度**（英国）是数组的**最左边**。\n",
        "    * **360度**（也是英国）是数组的**最右边**。\n",
        "    * **坑点：** 如果你画图或者算平均时，不小心把这俩当成两个遥远的地方，你的计算就会出错。记住它们是挨在一起的（无缝连接）。\n",
        "\n",
        "**一句话攻略：** ERA5 是填补了空白的“完美天气记录”，我们用 NetCDF 格式是因为它被处理成了规则网格，方便好用，但要小心处理经度 0度和360度重合的问题。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAd8I3-ImDm1"
      },
      "source": [
        "### **通过气候数据存储 (CDS) 下载 ERA5（Downloading ERA5 via the Climate Data Store (CDS)）**\n",
        "The instructions to download ERA5 data are [here](https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5). There are two ways to download data:\n",
        "\n",
        "1. The simplest (if one only need to do this once), is to use the [web interface](https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5#HowtodownloadERA5-3-DownloadingonlineERA5familydatathroughtheCDSwebinterface).\n",
        "\n",
        "2. However, one can also download ERA5 via installation and understanding of yet another service and its [API](https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5#HowtodownloadERA5-3-DownloadingonlineERA5familydatathroughtheCDSwebinterface), namely, the [Climate Data Store API](https://confluence.ecmwf.int/display/CKB/Climate+Data+Store+%28CDS%29+infrastructure+and+API). (Beware, the installation instructions depend on the operating system). This provides the user with a mechanism to write data request scripts to download files automatically (or as necessary whithin a workflow).\n",
        "\n",
        "#### The CDS API\n",
        "  \n",
        "**Note that before using this service, one must first register for a free CDS account [here](https://cds.climate.copernicus.eu/user/register)**. Registration is fast, only a minute.\n",
        "\n",
        "The next step is to setup the uid:API key detail, as prescribed in the installation instructions.\n",
        "\n",
        "Then install the CDS API itself; follow the instructions [here](https://cds.climate.copernicus.eu/api-how-to). (Note, this can also be done with [conda](https://anaconda.org/conda-forge/cdsapi): `conda install -c conda-forge cdsapi`.)\n",
        "\n",
        "The Copernicus Climate Change Service [provides tutorials and training material](https://confluence.ecmwf.int/display/COPSRV/CDS+web+API+%28cdsapi%29+training) on the CDS API.\n",
        "\n",
        "The following shows an example of how one can download Air Temperature, here on the 1000 hPa pressure level, for a given time point in both NetCDF or GRIB format.\n",
        "\n",
        "Note that the CDS web interface can be used to build the CDS API download script. In the web interface, in the `Download data` tab, after selecting some files of interest, one can click the button `Show API request` at the bottom left, which will generate a script. This script can then be modified easily as required (for example, to access similar data from different dates - what would be convenient if building a self-updating dashboard, etc.).\n",
        "\n",
        "One can also study the systematic structure of the ERA archive to build CDS API scripts more [efficiently](https://confluence.ecmwf.int/display/CKB/Climate+Data+Store+%28CDS%29+documentation#ClimateDataStore(CDS)documentation-Efficiencytips). Some people online have also attempted to build supporting interfaces or improved scripts do download that data; see [here](https://medium.com/@akash.pathaikara/downloading-era5-data-faster-using-parallel-processing-in-python-4200a912daed) for instance.\n",
        "\n",
        "A full list of available variables available for download is provided [here](https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Parameterlistings).\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "**通过气候数据存储 (CDS) 下载 ERA5 (Downloading ERA5 via the Climate Data Store (CDS))**\n",
        "\n",
        "下载 ERA5 数据的说明在[这里](https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5)。有两种下载数据的方法：\n",
        "\n",
        "1.  最简单的方法（如果只需要做一次），是使用 [**Web 界面 (web interface)**](https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5#HowtodownloadERA5-3-DownloadingonlineERA5familydatathroughtheCDSwebinterface)。\n",
        "\n",
        "2.  然而，也可以通过安装和理解另一种服务及其 [API](https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5#HowtodownloadERA5-3-DownloadingonlineERA5familydatathroughtheCDSwebinterface) 来下载 ERA5，即 [**气候数据存储 API (Climate Data Store API)**](https://confluence.ecmwf.int/display/CKB/Climate+Data+Store+%28CDS%29+infrastructure+and+API)。（注意，安装说明取决于操作系统）。这为用户提供了一种机制来编写数据请求脚本以自动下载文件（或在工作流中根据需要下载）。\n",
        "\n",
        "#### CDS API\n",
        "\n",
        "**请注意，在使用此服务之前，必须先在[这里](https://cds.climate.copernicus.eu/user/register)注册一个免费的 CDS 账户**。注册很快，只需一分钟。\n",
        "\n",
        "下一步是按照安装说明中的规定设置 `uid:API key` 详细信息。\n",
        "\n",
        "然后安装 CDS API 本身；请遵循[这里](https://cds.climate.copernicus.eu/api-how-to)的说明。（注意，这也可以用 [conda](https://anaconda.org/conda-forge/cdsapi) 完成：`conda install -c conda-forge cdsapi`。）\n",
        "\n",
        "哥白尼气候变化服务提供了关于 CDS API 的 [教程和培训材料](https://confluence.ecmwf.int/display/COPSRV/CDS+web+API+%28cdsapi%29+training)。\n",
        "\n",
        "\n",
        "\n",
        "下面的内容将展示一个示例，说明如何以 NetCDF 或 GRIB 格式下载特定时间点、1000 hPa 气压层的气温数据。\n",
        "\n",
        "**注意：** CDS Web 界面可用于构建 CDS API 下载脚本。在 Web 界面中，在 `Download data`（下载数据）选项卡中，选择一些感兴趣的文件后，可以点击左下角的 `Show API request`（显示 API 请求）按钮，这将生成一个脚本。然后可以根据需要轻松修改此脚本（例如，从不同的日期访问类似的数据——这在构建自动更新的仪表板等时非常方便）。\n",
        "\n",
        "人们还可以研究 ERA 档案的系统结构，以更[高效](https://confluence.ecmwf.int/display/CKB/Climate+Data+Store+%28CDS%29+documentation#ClimateDataStore(CDS)documentation-Efficiencytips)地构建 CDS API 脚本。网上也有人尝试构建支持接口或改进的脚本来下载该数据；例如参见[这里](https://medium.com/@akash.pathaikara/downloading-era5-data-faster-using-parallel-processing-in-python-4200a912daed)。\n",
        "\n",
        "[这里](https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Parameterlistings)提供了可供下载的变量的完整列表。\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（这就是“进货渠道”）**\n",
        "\n",
        "这一块教你**去哪里拿数据**。以前这些数据都要用硬盘拷，现在都可以在线下载了。\n",
        "\n",
        "1.  **两种进货方式：**\n",
        "    * **方式一：逛淘宝 (Web Interface)**。登录网站，勾选你要的年份、变量（比如“2023年”、“气温”），然后点击“Download”。适合**偶尔用一次**。\n",
        "    * **方式二：写代码 (CDS API)**。在你的 Python 代码里写几行指令，让电脑自动去下载。适合**批量下载**（比如要下载过去 50 年的数据，手点会断的）。\n",
        "\n",
        "2.  **准备工作（必做）：**\n",
        "    * 你必须去 **Copernicus Climate Data Store (CDS)** 官网注册一个账号。\n",
        "    * 注册完会给你一个 **API Key**（类似于密码），你要把它配置到你的电脑里，这样你的代码才有权限下载数据。\n",
        "\n",
        "3.  **超级技巧：`Show API Request` 按钮（Quiz/Practical 救命稻草）**\n",
        "    * **痛点：** API 代码很难写，参数很多（比如气温叫 `2m_temperature` 还是 `t2m`？）。\n",
        "    * **解法：** 教授教了一个**作弊技巧**。你可以先去“淘宝页面”（Web Interface）上把你要的数据勾选好，不要点下载，而是点左下角的 **`Show API request`** 按钮。\n",
        "    * 它会直接吐出一段 Python 代码给你！你直接**复制粘贴**到你的脚本里就行了。根本不需要背参数名！\n",
        "\n",
        "**一句话攻略：** 注册 CDS 账号 -> 去网页上勾选数据 -> 点击 `Show API request` -> 复制生成好的代码 -> 粘贴到 Python 里运行。搞定！\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1：用CDS API（写代码）来下载ERA5数据：**\n",
        "\n",
        "**这段代码展示了如何不打开浏览器，直接用 Python 代码命令服务器把数据发给你。它对应的是刚才提到的“方式二：写代码 (CDS API)”**"
      ],
      "metadata": {
        "id": "ruCEpHNwLS7B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "no1g_8EQmDm1",
        "outputId": "2286adf4-5084-451d-fb33-86ae5cf825cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-07 22:10:12,481 INFO Welcome to the CDS\n",
            "2021-11-07 22:10:12,482 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-pressure-levels\n",
            "2021-11-07 22:10:12,510 INFO Request is queued\n",
            "2021-11-07 22:10:13,541 INFO Request is running\n",
            "2021-11-07 22:10:15,072 INFO Request is completed\n",
            "2021-11-07 22:10:15,074 INFO Downloading https://download-0005.copernicus-climate.eu/cache-compute-0005/cache/data1/adaptor.mars.internal-1636323013.50075-12027-16-4a1dd4dd-58a0-45e2-8511-49ef05de9d17.nc to output/ERA5test.nc (2M)\n",
            "2021-11-07 22:10:15,461 INFO Download rate 5.1M/s\n",
            "2021-11-07 22:10:15,504 INFO Welcome to the CDS\n",
            "2021-11-07 22:10:15,505 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-pressure-levels\n",
            "2021-11-07 22:10:15,533 INFO Request is queued\n",
            "2021-11-07 22:10:16,565 INFO Request is completed\n",
            "2021-11-07 22:10:16,566 INFO Downloading https://download-0001.copernicus-climate.eu/cache-compute-0001/cache/data5/adaptor.mars.internal-1636323015.8604436-19368-20-595f97a6-e99b-4c4c-a9c5-c103da7811f8.grib to output/ERA5test.grib (2M)\n",
            "2021-11-07 22:10:16,926 INFO Download rate 5.5M/s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Result(content_length=2076600,content_type=application/x-grib,location=https://download-0001.copernicus-climate.eu/cache-compute-0001/cache/data5/adaptor.mars.internal-1636323015.8604436-19368-20-595f97a6-e99b-4c4c-a9c5-c103da7811f8.grib)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "# 注释部分，解释这是下载 1000 hPa 气温数据的示例)\n",
        "# example useage of the CDS API: https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5#HowtodownloadERA5-3-DownloadingonlineERA5familydatathroughtheCDSwebinterface\n",
        "#\n",
        "# Downloading Air Temperature on the 1000 hPa level for a Jan 1st 2021 at 12:00.\n",
        "\n",
        "\n",
        "import cdsapi\n",
        "\n",
        "# 1. 启动客户端 (Handshake)\n",
        "# 这行代码会自动去你的电脑里找 .cdsapirc 文件（里面有你的 API Key）。\n",
        "# 相当于拿着你的身份证去敲 ECMWF 的大门。\n",
        "c = cdsapi.Client()\n",
        "\n",
        "# 2. 发送请求 (The Order) - 下载 NetCDF 格式\n",
        "c.retrieve(\n",
        "    'reanalysis-era5-pressure-levels',    # 数据集名称：ERA5 气压层数据\n",
        "    {\n",
        "        'product_type': 'reanalysis',  # 产品类型：再分析\n",
        "        'variable': 'temperature',    #  变量：气温\n",
        "        'pressure_level': '1000',    # 高度：1000 hPa (近地面)\n",
        "        'year': '2021',         # 年\n",
        "        'month': '01',          # 月\n",
        "        'day': '01',           # 日\n",
        "        'time': '12:00',        # 时间\n",
        "        'format': 'netcdf',     # 格式为：NetCDF（重要）\n",
        "    },\n",
        "    'output/ERA5test.nc')       # 保存路径：存为 output 文件夹下的 ERA5test.nc Output file. Adapt as you wish\n",
        "\n",
        "\n",
        "# 3. 再次发送请求 - 下载 GRIB 格式\n",
        "# 逻辑完全一样，只是把 format 改成了 'grib'，文件名改成了 .grib\n",
        "# Retrieve the same files in GRIB format\n",
        "c.retrieve(\n",
        "    'reanalysis-era5-pressure-levels',\n",
        "    {\n",
        "        'product_type': 'reanalysis',\n",
        "        'variable': 'temperature',\n",
        "        'pressure_level': '1000',\n",
        "        'year': '2021',\n",
        "        'month': '01',\n",
        "        'day': '01',\n",
        "        'time': '12:00',\n",
        "        'format': 'grib',       # GRIB format\n",
        "    },\n",
        "    'output/ERA5test.grib')     # 保存路径 Output file. Adapt as you wish."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2：修改 CDS API 脚本对数据进行裁剪 (Modifying the CDS API download script)（Optional 不是必须的，需要再裁）**\n",
        "\n",
        "刚才那个代码是“全盘下载”（全球数据 + 最高清晰度），文件会很大。 这段代码教你如何 **“只下载你要的那一小块”** ，既省流量又省硬盘空间。"
      ],
      "metadata": {
        "id": "gJrN9QW7Myb8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xc_HGxc5mDm1"
      },
      "source": [
        "One can also modify the CDS API download script using optional post-processing arguments.\n",
        "\n",
        "For example, one can use the `grid` option to change the grid on which the data are presented.  \n",
        "\n",
        "The native grid of ERA data on CDS is 0.25°x0.25° (atmosphere), 0.5°x0.5° (ocean waves) for variables, while derived quantities, such as mean and variance are on a 0.5°x0.5° (atmosphere) and a coarser 1°x1° (ocean waves). ERA5-Land is provided as 0.1°x0.1°. These grid resolutions will be provided by default, but perhaps one does not needs such high resolution for our application, and one may only be interested in a subset of the global dataset, in which case, it makes sense to minimize the local computational and storage burden and to use the CDS API to provide a coarser and more focused data file.\n",
        "\n",
        "The following script shows an [example](https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5#HowtodownloadERA5-3-DownloadingonlineERA5familydatathroughtheCDSwebinterface), where the resolution is decreased to 1 degree by 1 degree and a sub-domain is selected (in case one does not need global coverage):\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "我们还可以使用可选的后处理参数来修改 CDS API 下载脚本。\n",
        "\n",
        "例如，可以使用 `grid` 选项来更改显示数据的网格。\n",
        "\n",
        "* CDS 上 ERA 数据的**原生网格**是 0.25°x0.25°（大气），0.5°x0.5°（海浪）。\n",
        "* ERA5-Land 为 0.1°x0.1°。\n",
        "* 默认情况下会提供这些高分辨率网格，但也许我们的应用不需要这么高的分辨率，或者我们只对全球数据集的一个**子集 (subset)** 感兴趣。\n",
        "* 在这种情况下，最大限度地减少本地计算和存储负担是有意义的，可以使用 CDS API 提供更粗糙和更集中的数据文件。\n",
        "\n",
        "----\n",
        "**Part 2：大白话总结（精准打击，拒绝臃肿）**\n",
        "\n",
        "这一步是在教你如何 **“去肥肉”**。\n",
        "\n",
        "1. **为什么要做这一步？**\n",
        "    * ERA5 的全球数据非常大。如果你只研究“法国的气温”，没必要把“太平洋”的数据也下载下来。\n",
        "    * 如果你只是画个简单的示意图，没必要用 4K 高清（0.25度），用 720p（1.0度）就够了，速度快几十倍。\n",
        "\n",
        "2. **两个核心参数（Practical 必用）：**\n",
        "    * **`area` (区域剪裁)：**\n",
        "        * 就像截图一样，只截取你关心的那部分地图。\n",
        "        * **口诀：北、西、南、东 (N, W, S, E)**。千万别填反了，否则会报错或者下载个寂寞。\n",
        "        * *例子：* `[60, -11, 34, 35]` 大概就是欧洲的范围。\n",
        "    * **`grid` (分辨率调整)：**\n",
        "        * 就像看视频选清晰度。\n",
        "        * 默认是最高清（0.25度，约 30km 一格）。\n",
        "        * 填 `[1.0, 1.0]` 就是让服务器先把数据压缩成 1度（约 100km 一格）再发给你。\n",
        "\n",
        "3. **效果对比：**\n",
        "    * 看图片最下面的 Log，这个文件下载下来只有 **3.9 KB**！而之前下载全球数据是 **2 MB**。体积瞬间缩小了 500 倍！这就是精准裁剪的威力。\n",
        "\n",
        "> **一句话攻略：** 只要你不是做全球研究，**强烈建议**在 API 里加上 `'area'` 参数。这能救你的硬盘一命，也能让你的代码跑得飞快。记得顺序是：**北、西、南、东**。\n",
        "\n",
        "下面的脚本显示了一个 [示例](https://confluence.ecmwf.int/display/CKB/How+to+download+ERA5)，其中分辨率降低到 **1度 x 1度**，并且选择了一个 **子区域 (sub-domain)**（以防不需要全球覆盖）："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUAZc9z8mDm1",
        "outputId": "30a35003-b058-4a2b-da65-65397276b5bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-07 22:55:17,737 INFO Welcome to the CDS\n",
            "2021-11-07 22:55:17,738 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-pressure-levels\n",
            "2021-11-07 22:55:17,764 INFO Request is queued\n",
            "2021-11-07 22:55:18,794 INFO Request is running\n",
            "2021-11-07 22:55:22,618 INFO Request is completed\n",
            "2021-11-07 22:55:22,619 INFO Downloading https://download-0002.copernicus-climate.eu/cache-compute-0002/cache/data1/adaptor.mars.internal-1636325721.1884649-10109-12-29518ea8-a89a-4c1c-a103-206d5b95ad02.nc to output/ERAtest_subarea_1deg1deg.nc (3.9K)\n",
            "2021-11-07 22:55:22,731 INFO Download rate 34.7K/s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Result(content_length=3948,content_type=application/x-netcdf,location=https://download-0002.copernicus-climate.eu/cache-compute-0002/cache/data1/adaptor.mars.internal-1636325721.1884649-10109-12-29518ea8-a89a-4c1c-a103-206d5b95ad02.nc)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "import cdsapi\n",
        "\n",
        "c = cdsapi.Client()\n",
        "\n",
        "c.retrieve(\n",
        "    'reanalysis-era5-pressure-levels',\n",
        "    {\n",
        "        'product_type': 'reanalysis',\n",
        "        'variable': 'temperature',\n",
        "        'pressure_level': '1000',\n",
        "        'year': '2021',\n",
        "        'month': '01',\n",
        "        'day': '01',\n",
        "        'time': '12:00',\n",
        "        'format': 'netcdf',  # Format: NetCDF\n",
        "\n",
        "        # --- 新增的关键参数 ---\n",
        "        # 1. 区域裁剪 (Area Selection)\n",
        "        # 格式：[北, 西, 南, 东] (North, West, South, East)\n",
        "        # 这里选择了欧洲区域：北纬60度, 西经11度, 北纬34度, 东经35度\n",
        "        # Default area is global; provide [North, West, South, East] limits to select an area (here selecting Europe)\n",
        "        'area'          : [60., -11., 34., 35.],\n",
        "\n",
        "        # 2. 网格重采样 (Regridding)\n",
        "        # 格式：[经度间隔, 纬度间隔]\n",
        "        # 原生是 0.25 x 0.25，这里强制降级为 1.0 x 1.0 (粗糙一点，文件更小)\n",
        "        # Default atmospheric resolution is 0.25 x 0.25; provie [Latitude/longitude] grid resolution to interpolate data to something else.\n",
        "        'grid'          : [1.0, 1.0],\n",
        "    },\n",
        "    'output/ERAtest_subarea_1deg1deg.nc')   # 输出文件名备注了是子区域和1度网格, Output file. Adapt as you wish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IinQEEOsmDm1"
      },
      "source": [
        "### **The CDS Toolbox（云端计算，把成品数据打包带走）**\n",
        "\n",
        "**(5 Jan 2026: the 'toolbox' link not longer works, this funcionality seems no longer active)**\n",
        "\n",
        "\n",
        "An extention to the CDS API is the [CDS Toolbox](https://cds.climate.copernicus.eu/toolbox/doc/index.html). The Toolbox is a programming interface. It is free and available to everyone. It links raw data to an online computing facility, thereby removing the computing barrier for users worldwide. The Toolbox allows users to develop python scripts for the CDS and to run them online. The users than then simply download the maps, graph or secondary data without needing to incur the costs associated with storing and maintaining the large climate data archive locally, nor is it required to invest in a powerful computer.\n",
        "\n",
        "\n",
        "The Toolbox works hand-in-hand with the CDS API.\n",
        "\n",
        "That is, one can issue commmands to the Toolbox via the CDS API, or develop a python script that does certain things, and then use the CDS API functionality to send it to CDS for evaluation, only to download the finished product locally.\n",
        "\n",
        "Examples of how to proceed are given [here](https://confluence.ecmwf.int/display/COPSRV/Call+a+service+with+the+CDS+API).\n",
        "\n",
        "Note, it is also possible to use the CDS Toolbox to plot data from other services (i.e. not originally stored on CDS), using the `remote` function instead of the `service` function. This is possible thanks to protocols such as OPeNDAP. Here is an example using data from UNIDATA.\n",
        "\n",
        "For illustration of how the Toolbox can be used, the following piece of code, run locally, after the CDS API is properly configured and assuming one is connected to the internet, will produce, and then download to our `output/` directory, a map of global temperature on January 1st 2021 at 14:00.\n",
        "\n",
        "----\n",
        "**这是一个更高级（但目前可能已失效）的概念：云端计算。**\n",
        "\n",
        "**简单来说，之前的步骤是“把食材（数据）买回家自己做饭”，而这一步是“在餐馆点好菜，直接打包成品（图/表）回家”。**\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "**CDS 工具箱 (The CDS Toolbox)**\n",
        "\n",
        "**注意：标题中特别注明 \"5 Jan 2026: the 'toolbox' link not longer works...\"，说明这个功能在 2026 年似乎已经不再活跃或已失效。但在学习时理解其概念依然很重要。**\n",
        "\n",
        "CDS API 的一个扩展是 [CDS Toolbox](https://cds.climate.copernicus.eu/toolbox)。Toolbox 是一个编程接口。它是免费的，所有人都可以使用。它将原始数据连接到一个**在线计算设施 (online computing facility)**，从而消除了世界各地用户的计算障碍。\n",
        "\n",
        "Toolbox 允许用户为 CDS 开发 python 脚本并在**线运行它们**。用户随后只需下载**地图、图表或二级数据**，而无需在本地产生存储和维护大型气候数据档案的相关成本，也不需要投资强大的计算机。\n",
        "\n",
        "Toolbox 与 CDS API 协同工作。也就是说，人们可以通过 CDS API 向 Toolbox 发出命令，或者开发一个 python 脚本来做某些事情，然后使用 CDS API 功能将其发送到 CDS 进行评估，最后只在本地下载**成品 (finished product)**。\n",
        "\n",
        "----\n",
        "**Part 2：大白话总结（云端“代客以此”）**\n",
        "\n",
        "这一块介绍了一种 **“偷懒”** 的高级玩法。\n",
        "\n",
        "1. **痛点是什么？**\n",
        "    * 下载 ERA5 数据（几百 GB）太占硬盘。\n",
        "    * 处理这些数据需要很好的电脑（大内存、好 CPU）。\n",
        "    * 有时候我只想要一张图插在 PPT 里，不想下载几个 G 的数据自己慢慢画。\n",
        "\n",
        "2. **CDS Toolbox 的解决方案：**\n",
        "    * **云端处理：** 你写好代码，发给 ECMWF 的超级计算机。\n",
        "    * **代劳：** 它们的电脑帮你把数据找出来，帮你算平均，甚至**帮你把图画好**。\n",
        "    * **结果：** 你只需要下载最后生成的那个 **PNG 图片** 或者 **很小的统计结果**。\n",
        "\n",
        "3. **代码的关键区别：**\n",
        "    * 之前的代码是 `c.retrieve(...)` -> 下载原始数据。\n",
        "    * 现在的代码是 `c.service('map.plot', ...)` -> 命令服务器画图。\n",
        "\n",
        "4. **重要现状（避坑）：**\n",
        "    * 文档开头那句红色的备注非常重要：**这个功能在 2026 年似乎已经挂了（不再维护）**。\n",
        "    * **结论：** 知道有这么个“云端计算”的概念就行，实操中如果跑不通报错，不要死磕，直接用回上一节教的“下载数据到本地处理”的方法。\n",
        "\n",
        "> **一句话攻略：** 这是“让服务器帮你干活”的模式，省硬盘省算力。但鉴于提示说它可能已失效，了解一下概念即可，主要还是靠自己下载数据（上一节的方法）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7GePsATmDm2",
        "outputId": "2a8939d2-6898-42de-eb94-127e40c65b35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-07 23:24:33,573 INFO Welcome to the CDS\n",
            "2021-11-07 23:24:33,574 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/tasks/services/catalogue/retrieve/clientid-b51ae2dfb7874262977f338d0cf6f978\n",
            "2021-11-07 23:24:33,640 INFO Request is completed\n",
            "2021-11-07 23:24:33,655 INFO Welcome to the CDS\n",
            "2021-11-07 23:24:33,656 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/tasks/services/map/plot/clientid-5a7d81efe86142cb9c058dd2d90761f3\n",
            "2021-11-07 23:24:33,824 INFO Downloading https://download-0010.copernicus-climate.eu/cache-compute-0010/cache/plots/map.plot-1636326997.4671943-21549-20-e380a147-00d8-4697-a88a-b79f4f1a80a0.png to output/ERA5_testmap.png (335.7K)\n",
            "2021-11-07 23:24:33,957 INFO Download rate 2.5M/s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'output/ERA5_testmap.png'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import cdsapi\n",
        "\n",
        "# full_stack=True 表示我们要使用完整的 Toolbox 服务\n",
        "c = cdsapi.Client(full_stack=True, debug=True)\n",
        "\n",
        "# 1. 在服务器端检索数据 (Retrieve)\n",
        "# 注意这里用的是 c.service 而不是 c.retrieve\n",
        "r = c.service(\"catalogue.retrieve\",\n",
        "    'reanalysis-era5-single-levels',\n",
        "    {\"variable\": \"2t\",        # 2米气温\n",
        "     \"product_type\": \"reanalysis\",\n",
        "     \"date\": \"2021-01-01\",\n",
        "     \"time\": \"14:00\"})\n",
        "\n",
        "# 2. 在服务器端画图 (Plot)\n",
        "# 核心区别：这行代码是在命令服务器“把刚才拿到的数据画成一张图”\n",
        "r = c.service('map.plot', r) # ask the Toolbox to make a map of the data\n",
        "\n",
        "# 3. 下载成品图片 (Download)\n",
        "# 下载下来的不是 .nc 数据文件，而是 .png 图片文件！\n",
        "r.download(\"output/ERA5_testmap.png\") # Check the output/ directory for a new map!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtO3fD3FmDm2"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***UKCP18（下载数据）**"
      ],
      "metadata": {
        "id": "Pv1_QG_CQ5_y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIgQZ-9zmDm2"
      },
      "source": [
        "<a id='ukcp18'></a>\n",
        "\n",
        "\n",
        "## **迈向气候服务（Towards Climate Services）**\n",
        "\n",
        "\n",
        "* ## UKCP18: latest high resolution model output for the UK\n",
        "[UKCP](https://www.metoffice.gov.uk/research/approach/collaboration/ukcp/index) stands for UK Climate Projections. UKCPs uses climate models to provide the most up-to-date assessment of how the UK climate may change in the future.\n",
        "\n",
        "[UKCP18](https://www.metoffice.gov.uk/research/approach/collaboration/ukcp/using-ukcp/ukcp-data) represents a set of high resolution climate projections for the UK and the globe ('18' because this was released in 2018). It is an update of UKCP09 (done in 2009).\n",
        "\n",
        "UKCP18 provides model projections of climate change until 2100 over the UK at resolutions ranging from 2.2km, 12km, and also provides global projections at 60km resolution. Although 2.2km resolution is only available in some region, we note that is rougly the resolution used for weather forecasting! ... but in this case, the forecasts were carried forward for about a century (i.e. that's a lot of data)! This nesting, or resolution enhancement, allows the model to resolve localised high-impact events, such as heavy rainfalls. UKCP18 also comes with a set of marine projections of sea-level rise and storm surge.\n",
        "\n",
        "Although UKCP18 is 'just' a model product, it is based on consistent physics and the products presents the best tool available today to try to tease out the likely effect of climate change on the UK in the next few decades. [UKCP18 simulation ouptputs](https://catalogue.ceda.ac.uk/uuid/c700e47ca45d4c43b213fe879863d589/) are intented as a tool to help decision-makers in the UK to assess exposure to climate risk.  It is part of the Met Office Hadley Centre Climate Programme.\n",
        "\n",
        "### Accessing UKCP18 data\n",
        "\n",
        "Instructions for downloading UKCP18 data are available [here](https://www.metoffice.gov.uk/binaries/content/assets/metofficegovuk/pdf/research/ukcp/ukcp18_data_availability_jul-2021.pdf). The UKCP18 project provides a [web-interface](https://ukclimateprojections-ui.metoffice.gov.uk/ui/home) to facilitate data download, but this doesn't contain all the data. The full data is accessible from [CEDA](http://catalogue.ceda.ac.uk/?q=ukcp18&sort_by=).\n",
        "\n",
        "Accessing the data through the web-interface requires registration (this only takes a few minutes).\n",
        "\n",
        "There is also a UKCP Web Processing Service (WPS) that can be used to build Apps, and a python [API](https://ukclimateprojections-ui.metoffice.gov.uk/help/api), [`ukcp-api-client`](https://github.com/ukcp-data/ukcp-api-client), is available from git that allows remote applications to talk directly to the UKCP service to submit requests for products.   \n",
        "\n",
        "[Data availability](https://www.metoffice.gov.uk/binaries/content/assets/metofficegovuk/pdf/research/ukcp/ukcp18_data_availability_jul-2021.pdf): The resolution (temporal and spatial) of data available to download varies between domain (ocean, land, atmosphere) and what is available depends on what service is used to access the data (the full archive is available through CEDA...but this may not be the easiest way depending on what information we are looking for!)\n",
        "\n",
        "### Example: will the weather become rainier over Imperial College London?\n",
        "Our goal here is to dowload high temporal resolution model output, i.e. hourly data, from UKCP18, using the web-interface.\n",
        "To use the web-interface, first create an account and remember your login credentials.\n",
        "\n",
        "**ADVICE**: You are strongly encouraged to install/work with a [password manager](https://www.techradar.com/uk/best/password-manager) on your computer ... you will need to create lots of accounts when working with data! It is not advisable at all to reuse passwords for multiple data servcies as one cannot guarantee the safety of each data provider.  \n",
        "\n",
        "Once you have your login account, login. We are now going to walk through the dowload procedure to access rainfall predictions for `Imperial College London`.\n",
        "\n",
        " Note, we will get the data for the RCP8.5 scenario, for two time periods, 1981-2000 and 2080-2100, and we are going to ask for the data to be saved in the CF-NetCDF format (i.e. netcdf file complient with CF conventions). Because we want two time periods, we would need to repeat the whole request procedure twice.\n",
        "\n",
        "Tick the following boxes in \"Product Selection\":\n",
        "1. Collection: Land projections: local (2.2km)\n",
        "2. Scenario: RCP8.5 (more on what the scenario means later)\n",
        "3. Output: Data only\n",
        "4. Climate Change Type: Absolute values\n",
        "   \n",
        "This will create a shortlist of datasets, shown on the right side.\n",
        "1. Select **\"Data: Variables from local projections (2.2km) regridded to 5km over UK for subdaily data (hourly and 3-hourly)\"**.\n",
        "2. Click on `View Details` and familiarize yourself with what the data contain. You will note the statement `We recommend that you consider results from more than one 5km grid box, i.e. you should also consider surrounding grid boxes.` Of course, the weather is highly variable, and in order to develop a spatially and temporally coherent picture of what climate change may look like in an area, in a way that account for uncertainties and variability, one should follow their advice. Because we are here only going through the process to illustrate how to get data, we will only ask for data from 1 grid box (i.e. the South Kensington area). Once you finish looking at the detail, go back (`click back` on your browser).\n",
        "3. Assuming these are the data we want, click `Submit a Request`.\n",
        "4. For \"Variable\", select `Precipitation rate (mm/day)`.\n",
        "5. On the Map, click on the `red search button`, and type `Imperial College London`. Various choices come up, pick the first one. We note that all the options are within the same 5km by 5km grid cell anyways. Remember, we are working with gridded model products here, so there is no point in choosing a location with too much detail (i.e the model can't differentiate between the Royal School or Mine or the Natural History Museum given the model resolution).\n",
        "6. On the Map, click on the grid cell with Imperial College (the grid cell should be highlighted in blue).\n",
        "7. For \"Temporal Average\", select `hourly`. This means we will get data for every hour (that's a lot of data!).\n",
        "8. For \"Time range\", we will select two periods: `1981-2000` and `2061-2080`.\n",
        "9. The next pull-down menu asks which `ensemble member` we are interested in. Choose the first one `HadGEM3-GC3.05-r001i1p00000`. \"Ensembles\" are basically replicates. In this case, the model was run 12 times! Because the weather is turbulent, the results from each ensemble will vary! A complete assessement of uncertainties associated with these results would have to consider all available ensembles, and check that the conclusions of any analysis is consistently reproduced across the ensemble set, or report a probability stating how likely the outcome is. Since we are here only doing an example (and time is limited), we will only work with a single ensemble member.\n",
        "10. For \"Data Format\", choose \"CV-netCDF\". As we know, netCDF files will come with a bunch of metadata, and this one is also conveniently compliant with \"CF conventions\" which would make automation of analysis easier.\n",
        "11. Add a title for \"Job label\". Although optional, as a scientist, you will want to keep notes of what you are doing. One suggested title could be `EDSML_UKCP18_2061-2080_precip-hourly`.\n",
        "12. Click `submit`. You will see that the web-interface now carries you to the next phase `generating output`. Don't close your browser! By clicking submit, a reqest was made to fetch the data from the data archive and to put them in a file. This can take a few minutes (>5-10 min), be patient.\n",
        "13. When ready, you will see a blue `Download` button. The file size should be about 1.4Mb (quite small, but only because we only selected data from 1 grid cell. If we were to do the analysis over the UK, the file would be much bigger!). This should look somethink like the following screenshot:\n",
        "\n",
        "![UKCP18download](https://github.com/ada-tm325/environmental-data-week1/blob/main/img/UKCP18_download.png?raw=1)\n",
        "\n",
        "Note the other tabs (Outputs, Job Details, ASCII, XML Response) and explore these.\n",
        "\n",
        "**Repeat steps 1-12**: After the data are downloaded, click on `Edit inputs` (bottom right). Because we are looking to dowload data from two periods, we'll have to repeat the download precedure, making sure to select the correct time periods. Our goal will be to compare precipitation results over Imperial College from the historical period 1981-2000 with model prediction for the 2061-2080 period, assuming humanity achieves not reduction in greenhouse gas emissions (i.e. the RCP8.5 scenario is essentially a `buisness-as-usual scenario`).\n",
        "\n",
        "Make sure you move your downloaded files to a convenient folder, where you remember what these files are! (e.g. such as in an `output/` folder within this lecture folder)\n",
        "\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "----\n",
        "**Part 1：翻译**\n",
        "\n",
        "---\n",
        "\n",
        "<a id='ukcp18'></a>\n",
        "# 迈向气候服务 (Towards Climate Services)\n",
        "\n",
        "\n",
        "* ## UKCP18：最新的英国高分辨率模型输出\n",
        "[UKCP](https://www.metoffice.gov.uk/research/approach/collaboration/ukcp/index) 代表英国气候预测 (UK Climate Projections)。UKCP 使用气候模型来提供关于英国气候未来可能如何变化的最新评估。\n",
        "\n",
        "[UKCP18](https://www.metoffice.gov.uk/research/approach/collaboration/ukcp/using-ukcp/ukcp-data) 代表了一组针对英国和全球的高分辨率气候预测（'18' 是因为它是在 2018 年发布的）。它是 UKCP09（2009 年完成）的更新。\n",
        "\n",
        "UKCP18 提供了直到 2100 年的英国气候变化模型预测，分辨率范围从 **2.2km**, 12km 不等，并提供分辨率为 60km 的全球预测。虽然 2.2km 分辨率仅在某些区域可用，但我们注意到这大致相当于**天气预报**使用的分辨率！……但在这种情况下，预报是向前推演了一个世纪（即那是大量的数据）！这种嵌套或分辨率增强，使得模型能够解析**局部的高影响事件**，例如暴雨。UKCP18 还附带了一组关于海平面上升和风暴潮的海洋预测。\n",
        "\n",
        "虽然 UKCP18 “仅仅”是一个模型产品，但它基于一致的物理学，并且该产品代表了当今可用的最佳工具，试图梳理出未来几十年气候变化对英国可能产生的影响。[UKCP18 模拟输出](https://catalogue.ceda.ac.uk/uuid/c700e47ca45d4c43b213fe879863d589/) 旨在作为一个工具，帮助英国的决策者评估气候风险暴露。它是英国气象局哈德利中心气候计划的一部分。\n",
        "\n",
        "### **访问 UKCP18 数据**\n",
        "\n",
        "下载 UKCP18 数据的说明在[这里](https://www.metoffice.gov.uk/binaries/content/assets/metofficegovuk/pdf/research/ukcp/ukcp18_data_availability_jul-2021.pdf)。UKCP18 项目提供了一个 [Web 界面](https://ukclimateprojections-ui.metoffice.gov.uk/ui/home) 来促进数据下载，但这并不包含所有数据。完整数据可从 [CEDA](http://catalogue.ceda.ac.uk/?q=ukcp18&sort_by=) 访问。\n",
        "\n",
        "通过 Web 界面访问数据需要注册（这只需要几分钟）。\n",
        "\n",
        "还有一个 UKCP Web 处理服务 (WPS) 可用于构建 App，并且有一个 python [API](https://ukclimateprojections-ui.metoffice.gov.uk/help/api)，[`ukcp-api-client`](https://github.com/ukcp-data/ukcp-api-client) 可从 git 获得，允许远程应用程序直接与 UKCP 服务对话以提交产品请求。\n",
        "\n",
        "[数据可用性](https://www.metoffice.gov.uk/binaries/content/assets/metofficegovuk/pdf/research/ukcp/ukcp18_data_availability_jul-2021.pdf)：可下载数据的分辨率（时间和空间）在不同领域（海洋、陆地、大气）之间有所不同，可用的内容取决于用于访问数据的服务。\n",
        "\n",
        "### **示例：帝国理工学院的天气会变得更多雨吗？**\n",
        "\n",
        "我们这里的目标是使用 Web 界面从 UKCP18 下载高时间分辨率模型输出，即**每小时数据**。\n",
        "要使用 Web 界面，首先创建一个帐户并记住您的登录凭据。\n",
        "\n",
        "**建议**：强烈建议您在计算机上安装/使用 [密码管理器](https://www.techradar.com/uk/best/password-manager)……处理数据时您需要创建很多帐户！完全不建议对多个数据服务重复使用密码，因为无法保证每个数据提供商的安全性。\n",
        "\n",
        "有了登录帐户后，请登录。我们现在将逐步完成下载程序，以访问 `Imperial College London` 的降雨预测。\n",
        "\n",
        "注意，我们将获取 **RCP8.5 场景**（高排放/一切照旧）的数据，针对两个时间段：**1981-2000**（历史）和 **2061-2080**（未来），并且我们将要求数据保存为 **CF-NetCDF** 格式。因为我们需要两个时间段，所以我们需要重复整个请求过程两次。\n",
        "\n",
        "在 \"Product Selection\"（产品选择）中勾选以下框：\n",
        "1.  **Collection**: Land projections: local (2.2km)\n",
        "\n",
        "2.  **Scenario**: RCP8.5（稍后详细介绍场景意味着什么）\n",
        "\n",
        "3.  **Output**: Data only\n",
        "4.  **Climate Change Type**: Absolute values\n",
        "\n",
        "这将在右侧创建一个数据集短名单。\n",
        "1.  选择 **\"Data: Variables from local projections (2.2km) regridded to 5km over UK for subdaily data (hourly and 3-hourly)\"**。\n",
        "2.  点击 `View Details` 并熟悉数据包含的内容。您会注意到声明 `We recommend that you consider results from more than one 5km grid box...`（我们建议您考虑多个 5km 网格框的结果……）。当然，天气变化很大，为了全面了解，应该听从他们的建议。因为我们这里只是演示如何获取数据，我们将只请求 **1 个网格框**（即南肯辛顿地区）的数据。看完详情后，返回（在浏览器上点击返回）。\n",
        "3.  假设这些是我们想要的数据，点击 `Submit a Request`。\n",
        "4.  对于 \"Variable\"（变量），选择 `Precipitation rate (mm/day)`（降水率）。\n",
        "5.  在地图上，点击 `red search button`（红色搜索按钮），输入 `Imperial College London`。会出现各种选择，选第一个。记得我们处理的是网格化模型产品，所以没必要选太细的位置（模型分辨不出具体的建筑物）。\n",
        "6.  在地图上，点击包含帝国理工学院的网格单元（该单元应以蓝色突出显示）。\n",
        "7.  对于 \"Temporal Average\"（时间平均），选择 `hourly`。这意味着我们将获得每小时的数据（数据量很大！）。\n",
        "8.  对于 \"Time range\"（时间范围），我们将选择两个时段：`1981-2000` 和 `2061-2080`。\n",
        "9.  下一个下拉菜单询问我们要哪个 `ensemble member`（集合成员）。选择第一个 `HadGEM3-GC3.05-r001i1p00000`。“集合”基本上就是重复实验。模型运行了 12 次！因为天气是湍流的，每个集合的结果都会不同！完整的评估应该考虑所有集合，但在本例中我们只选一个。\n",
        "10. 对于 \"Data Format\"（数据格式），选择 \"CV-netCDF\"。这样方便自动化分析。\n",
        "11. 为 \"Job label\" 添加标题，例如 `EDSML_UKCP18_2061-2080_precip-hourly`。\n",
        "12. 点击 `submit`。Web 界面现在将带您进入下一阶段 `generating output`。**不要关闭浏览器！** 这可能需要几分钟（>5-10 分钟），请耐心等待。\n",
        "13. 准备好后，您会看到一个蓝色的 `Download` 按钮。文件大小应约为 1.4Mb（因为只选了一个格子）。\n",
        "\n",
        "**重复步骤 1-12**：下载数据后，点击 `Edit inputs`（右下角）。因为我们要比较两个时期，所以必须重复下载过程，确保选择正确的时间段。我们的目标是比较帝国理工学院上空 1981-2000 年的历史降水结果与 **RCP8.5 场景**（即一切照旧场景，人类不减少温室气体排放）下 2061-2080 年的模型预测。\n",
        "\n",
        "确保将下载的文件移动到一个方便的文件夹中！(例如本讲座文件夹内的 `output/` 文件夹)\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n",
        "\n",
        "**Part 2：大白话总结（英国未来的水晶球）**\n",
        "\n",
        "这一大堆是在教你如何**下载英国未来的气候预测数据**。\n",
        "\n",
        "1.  **UKCP18 是什么？**\n",
        "    * 它是英国气象局做的**高科技算命**。\n",
        "    * 它可以预测到 2100 年，分辨率高达 **2.2km**（这非常惊人，通常全球模型只有 100km）。这意味着它能模拟出**暴雨**这种局部事件。\n",
        "\n",
        "2.  **我们要干什么？（Practical 任务）**\n",
        "    * 我们想知道：**如果不控制碳排放（RCP8.5），几十年后帝国理工学院（伦敦）会不会经常下暴雨？**\n",
        "    * 为了回答这个问题，我们需要下载两份数据来做对比：\n",
        "        * **历史数据：** 1981-2000 年。\n",
        "        * **未来数据：** 2061-2080 年。\n",
        "\n",
        "3.  **下载步骤避坑指南：**\n",
        "    * **注册账号：** 必须去 UKCP 官网注册。\n",
        "    * **选格子 (Grid Cell)：** 地图上选帝国理工的位置。别纠结具体建筑，在那个 5km 的方格子里就行。\n",
        "    * **选变量 (Variable)：** 选 `Precipitation rate` (降水)。\n",
        "    * **选场景 (Scenario)：** `RCP8.5` (最悲观的剧本，假设大家都不环保)。\n",
        "    * **选集合 (Ensemble)：** 选第一个成员就行。因为模型跑了 12 次（为了模拟不确定性），我们上课只用第 1 次的结果。\n",
        "    * **做两次：** 下载完 1981 的，别忘了点 `Edit inputs` 再去下载 2061 的。\n",
        "\n",
        "> **一句话攻略：** 这是一个点鼠标的任务。去官网，注册，按教程勾选参数，下载两个 NetCDF 文件（一个代表过去，一个代表未来），后面我们会用 Python 来对比这两份文件。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtT-WojqmDm2"
      },
      "source": [
        "### **查看刚刚在UKCP18官网里下载好的数据（Looking at the data）**\n",
        "\n",
        "Now take a look at the downloaded data.\n",
        "\n",
        "You should see folders with the following format (`output_<some string>`):\n",
        "```\n",
        "output_08b295c097368efd78f0c78b53b320d7_20211114_234836/\n",
        "output_1819e6a692e9503460dcf1416043a7c2_20211114_231924/\n",
        "```\n",
        "\n",
        "Looking in these folders, we see 4 files:\n",
        "```\n",
        ">>> ls output_08b295c097368efd78f0c78b53b320d7_20211114_234837\n",
        "\n",
        "2021-11-14T23-44-34.nc\n",
        "data_licence.txt\n",
        "input_paths.txt\n",
        "request.txt\n",
        "```\n",
        "`input_path.txt` shows the path on the CEDA archive from where the data were obtained. We see a long list of path, each corresponding to a different time step.\n",
        "```\n",
        "http://data.ceda.ac.uk/badc/ukcp18/data/land-cpm/uk/5km/rcp85/01/pr/1hr/v20210615/pr_rcp85_land-cpm_uk_5km_01_1hr_20601201-20601230.nc\n",
        "http://data.ceda.ac.uk/badc/ukcp18/data/land-cpm/uk/5km/rcp85/01/pr/1hr/v20210615/pr_rcp85_land-cpm_uk_5km_01_1hr_20610101-20610130.nc\n",
        "...\n",
        "```\n",
        "\n",
        "The web-interface conveniently isolated the one data point from each one of these files for us, and concatenated the results into a single file for us. Without the web-interface, we would have had to this this ourselves, manually! (now you also understand why it took a few minutes to retrieve our data file!)\n",
        "\n",
        "`request.txt` is just a summary of the request we made. This is convenient as it can be used to retrace our steps and know how/where the data were obtained.\n",
        "\n",
        "Note the `data_licence.txt` file. When accessing/downloading data, always know under what license these are provided. For academic research, there are usually very few restrictions (except the need to reference the data adequately!), but this may not be the case for commercial applications (...better check the license before facing a lawsuit!). Here, data_license.txt tells us  that:\n",
        "`The data on this web site are available under the Open Government Licence, see http://www.nationalarchives.gov.uk/doc/open-government-licence/`\n",
        "\n",
        "**Question**: Given this licence, would you be free to use UKCP18 data for commercial application?\n",
        "\n",
        "**Answer**: [Yes](http://www.nationalarchives.gov.uk/doc/open-government-licence/) (make sure you know why!)\n",
        "\n",
        "\n",
        "`2021-11-14T23-44-34.nc` is the netCDF file with the data. Navigate to the directory where the data are saved and let's use the command line tool `ncdump -h <filename>` (on the terminal), to see the headers for the netcdf file.\n",
        "\n",
        "```\n",
        ">>> ncdump -h 2021-11-14T23-44-34.nc\n",
        "\n",
        "netcdf \\2021-11-14T23-44-34 {\n",
        "dimensions:\n",
        "\tensemble_member = 1 ;\n",
        "\ttime = 172080 ;\n",
        "\tbnds = 2 ;\n",
        "\tstring27 = 27 ;\n",
        "\tstring64 = 64 ;\n",
        "variables:\n",
        "\tfloat pr(ensemble_member, time) ;\n",
        "\t\tpr:standard_name = \"lwe_precipitation_rate\" ;\n",
        "\t\tpr:long_name = \"Precipitation rate\" ;\n",
        "\t\tpr:units = \"mm/hour\" ;\n",
        "\t\tpr:cell_methods = \"time: mean\" ;\n",
        "\t\tpr:grid_mapping = \"transverse_mercator\" ;\n",
        "\t\tpr:coordinates = \"ensemble_member_id latitude longitude month_number projection_x_coordinate projection_y_coordinate year yyyymmddhh\" ;\n",
        "\tint transverse_mercator ;\n",
        "\t\ttransverse_mercator:grid_mapping_name = \"transverse_mercator\" ;\n",
        "\t\ttransverse_mercator:longitude_of_prime_meridian = 0. ;\n",
        "\t\ttransverse_mercator:semi_major_axis = 6377563.396 ;\n",
        "\t\ttransverse_mercator:semi_minor_axis = 6356256.909 ;\n",
        "\t\ttransverse_mercator:longitude_of_central_meridian = -2. ;\n",
        "\t\ttransverse_mercator:latitude_of_projection_origin = 49. ;\n",
        "\t\ttransverse_mercator:false_easting = 400000. ;\n",
        "\t\ttransverse_mercator:false_northing = -100000. ;\n",
        "\t\ttransverse_mercator:scale_factor_at_central_meridian = 0.9996012717 ;\n",
        "\tint ensemble_member(ensemble_member) ;\n",
        "\t\tensemble_member:units = \"1\" ;\n",
        "\t\tensemble_member:long_name = \"ensemble_member\" ;\n",
        "\tdouble time(time) ;\n",
        "\t\ttime:axis = \"T\" ;\n",
        "\t\ttime:bounds = \"time_bnds\" ;\n",
        "\t\ttime:units = \"hours since 1970-01-01 00:00:00\" ;\n",
        "\t\ttime:standard_name = \"time\" ;\n",
        "\t\ttime:calendar = \"360_day\" ;\n",
        "\tdouble time_bnds(time, bnds) ;\n",
        "\tchar ensemble_member_id(ensemble_member, string27) ;\n",
        "\t\tensemble_member_id:units = \"1\" ;\n",
        "\t\tensemble_member_id:long_name = \"ensemble_member_id\" ;\n",
        "\tdouble latitude ;\n",
        "\t\tlatitude:units = \"degrees_north\" ;\n",
        "\t\tlatitude:standard_name = \"latitude\" ;\n",
        "\tdouble longitude ;\n",
        "\t\tlongitude:units = \"degrees_east\" ;\n",
        "\t\tlongitude:standard_name = \"longitude\" ;\n",
        "\tint month_number(time) ;\n",
        "\t\tmonth_number:units = \"1\" ;\n",
        "\t\tmonth_number:long_name = \"month_number\" ;\n",
        "\tdouble projection_x_coordinate ;\n",
        "\t\tprojection_x_coordinate:bounds = \"projection_x_coordinate_bnds\" ;\n",
        "\t\tprojection_x_coordinate:units = \"m\" ;\n",
        "\t\tprojection_x_coordinate:standard_name = \"projection_x_coordinate\" ;\n",
        "\tdouble projection_x_coordinate_bnds(bnds) ;\n",
        "\tdouble projection_y_coordinate ;\n",
        "\t\tprojection_y_coordinate:bounds = \"projection_y_coordinate_bnds\" ;\n",
        "\t\tprojection_y_coordinate:units = \"m\" ;\n",
        "\t\tprojection_y_coordinate:standard_name = \"projection_y_coordinate\" ;\n",
        "\tdouble projection_y_coordinate_bnds(bnds) ;\n",
        "\tint year(time) ;\n",
        "\t\tyear:units = \"1\" ;\n",
        "\t\tyear:long_name = \"year\" ;\n",
        "\tchar yyyymmddhh(time, string64) ;\n",
        "\t\tyyyymmddhh:units = \"1\" ;\n",
        "\t\tyyyymmddhh:long_name = \"yyyymmddhh\" ;\n",
        "\n",
        "// global attributes:\n",
        "\t\t:collection = \"land-cpm\" ;\n",
        "\t\t:contact = \"ukcpproject@metoffice.gov.uk\" ;\n",
        "\t\t:description = \"Precipitation rate\" ;\n",
        "\t\t:domain = \"uk\" ;\n",
        "\t\t:frequency = \"1hr\" ;\n",
        "\t\t:institution = \"Met Office Hadley Centre (MOHC), FitzRoy Road, Exeter, Devon, EX1 3PB, UK.\" ;\n",
        "\t\t:institution_id = \"MOHC\" ;\n",
        "\t\t:label_units = \"mm/hour\" ;\n",
        "\t\t:plot_label = \"Precipitation rate (mm/hour)\" ;\n",
        "\t\t:project = \"UKCP18\" ;\n",
        "\t\t:references = \"https://ukclimateprojections.metoffice.gov.uk\" ;\n",
        "\t\t:resolution = \"5km\" ;\n",
        "\t\t:scenario = \"rcp85\" ;\n",
        "\t\t:source = \"UKCP18 realisation from a set of 12 convection-permitting models (HadREM3-RA11M) driven by perturbed variants of the Met Office Unified Model Global Atmosphere GA7 model (HadREM3-GA705) at 12km resolution.  The HadREM3-GA705 models were driven by perturbed variants of the global HadGEM3-GC3.05\" ;\n",
        "\t\t:title = \"UKCP18 land projections - Regridded 2.2km convection-permitting climate model results on 5km British National Grid from Ordnance Survey (OSGB), Precipitation rate over the UK for the RCP8.5 scenario\" ;\n",
        "\t\t:version = \"v20210615\" ;\n",
        "\t\t:Conventions = \"CF-1.7\" ;\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "[back to contents](#Contents)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**大白话总结（拆快递 + 读说明书）**\n",
        "\n",
        "这一步虽然看起来都是代码，其实就是在做两件事：**拆快递** 和 **读说明书**。\n",
        "\n",
        "1. **拆快递（文件结构）：**\n",
        "    * 你下载的不仅仅是一个数据文件，而是一个**大礼包**。\n",
        "    * **`.nc` 文件**：这是真正的肉（数据）。\n",
        "    * **`.txt` 文件**：这是“发票”和“保修卡”。特别是 `request.txt`，如果你以后忘了这数据是怎么选的，看它就能复盘。\n",
        "    * **幕后功臣**：教授提到，这一个 `.nc` 文件其实是服务器在后台把几万个小文件**拼接**起来的。如果不靠 Web 界面，你自己写代码拼这些文件会累死。\n",
        "\n",
        "2. **读说明书（`ncdump` 包含的重要信息）：**\n",
        "    * 那一大坨代码其实是 NetCDF 文件的**头部信息 (Header)**，也就是数据的“身份证”。\n",
        "    * **关键信息 1：变量名 `pr`**。在代码里你要用 `data['pr']` 来读取降水，而不是用 `rain` 或 `precipitation`。\n",
        "    * **关键信息 2：单位 `mm/hour`**。这是毫米每小时。如果你要算一天的降水，记得要处理单位。\n",
        "    * **关键信息 3（Quiz 必考坑点）：`time:calendar = \"360_day\"`**。\n",
        "        * **注意看！** 这个模型用的**不是**地球的真实日历（365天/闰年366天）。\n",
        "        * 为了计算方便，气象模型里的一年**只有 360 天**（每个月都是 30 天）。\n",
        "        * 如果你用普通的 Python 日期库去处理它，绝对会报错。必须用我们之前提到的 `cftime` 库！\n",
        "\n",
        "3. **商业许可：**\n",
        "    * UKCP18 使用的是 **Open Government Licence**。\n",
        "    * 这意味着：**随便用**。不仅学术能用，你甚至可以拿这个数据开发一个 App 卖钱（商业用途），只要你注明数据来源是英国政府就行。\n",
        "\n",
        "> **一句话攻略：** 这一步用 `ncdump` 确认了三个核心事实：变量叫 `pr`，单位是 `mm/hour`，日历是奇葩的 `360_day`（处理时间时要千万小心）。"
      ],
      "metadata": {
        "id": "0YDYgpz_RwmV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffkkg_ermDm2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "esmvaltool",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 20:33:18) \n[Clang 11.1.0 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "70371e773136594d30d0e78906af2c7f432cd5459c4a58da662094ce1bdf37a0"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}